[
  {
    "objectID": "02_handling-spatial-data.html",
    "href": "02_handling-spatial-data.html",
    "title": "2  Handling of spatial data in R",
    "section": "",
    "text": "2.1 Introduction\nGeostatistical analysis primarily deals with point-referenced data. However, geostatistical modeling often requires more than just point data—raster and areal (polygon) data are frequently used to build essential covariates, enhance spatial understanding, or provide environmental context. For instance, population density, climate data, land use, and elevation, often key covariates in many spatial analyses, usually come in raster or polygon formats. Efficient handling of these different data types is critical for successful model-building. This chapter provides a comprehensive guide to the various stages of managing spatial data in R, using the sf and terra packages, to support a complete geostatistical workflow.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#accessing-covariates-for-disease-mapping",
    "href": "02_handling-spatial-data.html#accessing-covariates-for-disease-mapping",
    "title": "2  Handling of spatial data in R",
    "section": "2.4 Accessing covariates for disease mapping",
    "text": "2.4 Accessing covariates for disease mapping\nCovariates can play a crucial role in understanding and modeling the spatial variation in disease risk. In geostatistical analysis, incorporating environmental, demographic, and climatic variables might improve the predictive power of models or at least reduce the level of uncertainty in the predictions. These covariates can influence factors such as the spread of infectious diseases, the distribution of disease vectors, and the socio-economic conditions that impact health outcomes.\nA wide range of open-access spatial datasets provide covariates for disease mapping. These include population density, climate, land cover, and human infrastructure data, which often come in raster or polygon formats. Alongside these environmental covariates, administrative boundaries are also crucial for public health analysis, as they help organize and aggregate data at different levels (e.g., country, region, or district). For example, aggregating health outcomes or covariate data by administrative units allows researchers to identify geographic disparities and allocate resources accordingly. Fortunately, open-source platforms such as geoBoundaries provide easily accessible administrative boundary data for many countries at various levels of granularity. This data is available in formats compatible with geospatial analysis tools in R, making it easy to integrate into geostatistical workflows.\nTable 2.1 below summarizes key sources of covariates useful in disease mapping. Each dataset offers specific types of data, from satellite-derived environmental variables to gridded population estimates and administrative boundaries, which can be accessed through various R packages or APIs.\nMENTION RSPATIAL DATA WEBSITE?? https://rspatialdata.github.io/\n\n\n\nTable 2.1: Some key sources of covariates useful in disease mapping and R packages to retrieve them.\n\n\n\n\n\n\n\n\n\n\n\nSource\nData Type\nDescription\nR Package\n\n\n\n\nWorldClim\nClimate (temperature, rainfall)\nGlobal climate data\ngeodata\n\n\nMODIS\nRemote Sensing\nSatellite imagery (e.g., vegetation indices)\nMODIStsp\n\n\nOpenStreetMap (OSM)\nHuman settlements, roads\nGlobal geographic features\nosmdata\n\n\nGoogle Earth Engine\nSatellite imagery\nLarge-scale environmental data analysis\nrgee\n\n\nWorldPop\nPopulation data\nGridded population density estimates\nworldpop (via API)\n\n\n\n\n\n\n\n2.4.1 Example: Downloading administrative boundaries\nAdministrative boundaries provide an essential spatial structure for many types of geostatistical analyses, particularly in disease mapping where data is often aggregated by administrative units such as regions, districsts or provinces. The geoBoundaries datasets, which are accessible via the rgeoboundaries package, provides openly available administrative boundary data for nearly every country, allowing researchers to integrate these boundaries into their analyses seamlessly.\n# Load the rgeoboundaries package\nlibrary(rgeoboundaries)\n\n# Download administrative boundaries for Liberia (level 0: country)\nliberia_admin0 &lt;- gb_adm0(\"Liberia\")\n\n# Do the same for level 1: regions\nliberia_admin1 &lt;- gb_adm1(\"Liberia\")\n\n# Plot the administrative boundaries\nplot(liberia_admin0$geometry)\nplot(liberia_admin1$geometry)\n\n\n\n\n\n\n\n\n\n\n\n(a) Adimin level 0\n\n\n\n\n\n\n\n\n\n\n\n(b) Admin level 1\n\n\n\n\n\n\n\nFigure 2.1: Administrative boundaries for Liberia retrieved using the rgeoboundaries package.\n\n\n\n\n\n2.4.2 Example: Download population data\nPopulation density is a key covariate in geostatistical models for public health research. In this section, we demonstrate how to retrieve high-resolution population data for Liberia from WorldPop using the wpgpDownloadR package. This package provides easy access to the WorldPop datasets, which offers gridded population estimates at various spatial resolutions.\nBefore downloading the data, we will search for available datasets for Liberia. The function wpgpListCountryDatasets() helps in retrieving a list of all available datasets for a specified country.\n\nlibrary(wpgpDownloadR)\n\n# Search for datasets available for Liberia \n# usign the ISO3 country code\nlbr_datasets &lt;- wpgpListCountryDatasets(ISO3 = \"LBR\")\nlbr_datasets\n\n\n  \n\n\n\nWe can check the description column to see what datasets are available. Let's download the population data for Liberia for the year 2014 at a 100m resolution. The wpgpGetCountryDataset function will then download a raster dataset based on ISO3 code and covariate name.\n\nlbr_pop_url &lt;- wpgpGetCountryDataset(ISO3 = \"LBR\", covariate = \"ppp_2014\") \n\nThis will download a raster file locally in a temporary directory. The path to the downloaded file is contained in the lbr_pop_url variable and when we introduce the terra package in the next sections we will show how to upoload the population raster into R. It is also possible to specify the directory where we want the raster to be downloaded using the destDir argument.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#sources-of-covariates",
    "href": "02_handling-spatial-data.html#sources-of-covariates",
    "title": "2  Handling of spatial data in R",
    "section": "2.1 Sources of Covariates",
    "text": "2.1 Sources of Covariates\nThe table below summarizes key sources for covariates useful in disease mapping. These datasets range from population density to environmental and land cover data, and are available through various R packages or APIs.\n\n\n\n\n\n\n\n\n\nSource\nData Type\nDescription\nR Package/Method\n\n\n\n\nWorldClim\nClimate (temperature, rainfall)\nGlobal climate data\nraster::getData()\n\n\nMODIS\nRemote Sensing (Land use)\nSatellite imagery (e.g., vegetation indices)\nMODIStsp\n\n\nOpenStreetMap (OSM)\nHuman settlements, roads\nGlobal geographic features\nosmdata\n\n\nCopernicus Land Monitoring\nLand cover, water bodies\nEurope-wide environmental datasets\nraster / sf for shapefiles\n\n\nGoogle Earth Engine\nSatellite imagery\nLarge-scale environmental data analysis\nrgee\n\n\nWorldPop\nPopulation density\nGridded population density estimates\nworldpop (via API)\n\n\n\n\n2.1.1 Example: Downloading WorldClim Data\nWorldClim provides climate variables such as temperature and precipitation at various spatial resolutions. These covariates are valuable in disease risk models as they can influence factors like mosquito abundance and disease transmission.\n# Download bioclimatic variables for Africa\nlibrary(raster)\nclimate_data &lt;- getData('worldclim', var='bio', res=10, lon=20, lat=0)\nplot(climate_data[[1]])  # Plot annual mean temperature",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#importing-and-processing-spatial-data-in-r",
    "href": "02_handling-spatial-data.html#importing-and-processing-spatial-data-in-r",
    "title": "2  Handling of spatial data in R",
    "section": "2.1 Importing and processing spatial data in R",
    "text": "2.1 Importing and processing spatial data in R",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#visualizing-geostatistical-data",
    "href": "02_handling-spatial-data.html#visualizing-geostatistical-data",
    "title": "2  Handling of spatial data in R",
    "section": "2.2 Visualizing geostatistical data",
    "text": "2.2 Visualizing geostatistical data",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#section",
    "href": "02_handling-spatial-data.html#section",
    "title": "2  Handling of spatial data in R",
    "section": "2.6 ",
    "text": "2.6",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "04_model-validation.html",
    "href": "04_model-validation.html",
    "title": "4  Model validation",
    "section": "",
    "text": "4.1 How to simulate geostatistical data from a fitted model",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model validation</span>"
    ]
  },
  {
    "objectID": "04_model-validation.html#validating-the-calibration-of-the-model",
    "href": "04_model-validation.html#validating-the-calibration-of-the-model",
    "title": "4  Model validation",
    "section": "4.2 Validating the calibration of the model",
    "text": "4.2 Validating the calibration of the model",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model validation</span>"
    ]
  },
  {
    "objectID": "04_model-validation.html#validating-the-spatial-correlation-of-the-model",
    "href": "04_model-validation.html#validating-the-spatial-correlation-of-the-model",
    "title": "4  Model validation",
    "section": "4.3 Validating the spatial correlation of the model",
    "text": "4.3 Validating the spatial correlation of the model",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model validation</span>"
    ]
  },
  {
    "objectID": "05_geostatistical-prediction.html",
    "href": "05_geostatistical-prediction.html",
    "title": "5  Geostatistical prediction",
    "section": "",
    "text": "5.1 Pixel-level predictive targets",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Geostatistical prediction</span>"
    ]
  },
  {
    "objectID": "05_geostatistical-prediction.html#area-level-predictive-targets",
    "href": "05_geostatistical-prediction.html#area-level-predictive-targets",
    "title": "5  Geostatistical prediction",
    "section": "5.2 Area-level predictive targets",
    "text": "5.2 Area-level predictive targets",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Geostatistical prediction</span>"
    ]
  },
  {
    "objectID": "05_geostatistical-prediction.html#comparing-the-predictive-performance-of-geostatistical-models",
    "href": "05_geostatistical-prediction.html#comparing-the-predictive-performance-of-geostatistical-models",
    "title": "5  Geostatistical prediction",
    "section": "5.3 Comparing the predictive performance of geostatistical models",
    "text": "5.3 Comparing the predictive performance of geostatistical models",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Geostatistical prediction</span>"
    ]
  },
  {
    "objectID": "06_case-studies.html",
    "href": "06_case-studies.html",
    "title": "6  Case studies",
    "section": "",
    "text": "6.1 Mapping stunting risk in Ghan",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "06_case-studies.html#mapping-river-blindness-in-malawi",
    "href": "06_case-studies.html#mapping-river-blindness-in-malawi",
    "title": "6  Case studies",
    "section": "6.2 Mapping river blindness in Malawi",
    "text": "6.2 Mapping river blindness in Malawi",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "06_case-studies.html#mapping-mosquitoes-abundance-in-cameroon",
    "href": "06_case-studies.html#mapping-mosquitoes-abundance-in-cameroon",
    "title": "6  Case studies",
    "section": "6.3 Mapping mosquitoes abundance in Cameroon",
    "text": "6.3 Mapping mosquitoes abundance in Cameroon",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Case studies</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015.\n“Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical\nSoftware 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine\nLearning. Springer.\n\n\nBowman, A. W. 1997. Applied Smoothing Techniques for Data Analysis :\nThe Kernel Approach with s-Plus Illustrations. Oxford Statistical\nScience Series ; 18. Oxford : New York: Clarendon Press ; Oxford\nUniversity Press.\n\n\nBreslow, N. E., and D. G. Clayton. 1993. “Approximate Inference in\nGeneralized Linear Mixed Models.” Journal of the American\nStatistical Association 88: 9–25.\n\n\nChilès, J-P, and P. Delfiner. 2016. Geostatistics (Second\nEdition). Hoboken: Wiley.\n\n\nChristensen, OF, GO Roberts, and M Sköld. 2006. “Robust Markov\nChain Monte Carlo Methods for Spatial Generalized Linear Mixed\nModels.” Journal of Computational and Graphical\nStatistics 15 (1): 1–17.\n\n\nChristensen, Ole F. 2004. “Monte Carlo Maximum Likelihood in\nModel-Based Geostatistics.” Journal of Computational and\nGraphical Statistics 13 (3): 702–18.\n\n\nCressie, N. A. C. 1991. Statistics for Spatial Data. New York:\nWiley.\n\n\nCressie, Noel. 1985. “Fitting Variogram Models by Weighted Least\nSquares.” Mathematical Geology 17 (5): 563–86.\n\n\nDiggle, P. J., J. A. Tawn, and R. A. Moyeed. 1998. “Model-Based\nGeostatistics.” Journal of the Royal Statistical Society:\nSeries C (Applied Statistics) 47 (3): 299–350. https://doi.org/10.1111/1467-9876.00113.\n\n\nDiggle, Peter J. 2019. Model-Based Geostatistics for Global Public\nHealth : Methods and Applications. Chapman and Hall/CRC\nInterdisciplinary Statistics Ser. Milton: Chapman; Hall/CRC.\n\n\nDobson, A. J., and A. Barnett. 2008. An Introduction to Generalized\nLinear Models. Third. Chapman; Hall/CRC.\n\n\nFernández, J. A, A Rey, and A Carballeira. 2000. “An Extended\nStudy of Heavy Metal Deposition in Galicia (NW Spain) Based on Moss\nAnalysis.” Science of The Total Environment 254 (1):\n31–44. https://doi.org/10.1016/S0048-9697(00)00431-9.\n\n\nGelman, Andrew, and Donald B Rubin. 1992. “Inference from\nIterative Simulation Using Multiple Sequences.” Statistical\nScience 7 (4): 457–72.\n\n\nGeweke, John. 1992. “Evaluating the Accuracy of Sampling‐based\nApproaches to the Calculation of Posterior Moments.” Edited by\nJose M. Bernardo, James O. Berger, A. Philip Dawid, and Adrian F. M.\nSmith, 169–93.\n\n\nGeyer, Charles J. 1991. “Markov Chain Monte Carlo Maximum\nLikelihood.” Journal of Computational and Graphical\nStatistics 1 (4): 39–55.\n\n\nGeyer, Charles J. 1994. “Likelihood and Exponential\nFamilies.” Department of Statistics, University of\nMinnesota.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. The\nElements of Statistical Learning. Springer Series in Statistics.\nNew York, NY, USA: Springer New York Inc.\n\n\nHastings, W. K. 1970. “Monte Carlo Sampling Methods Using Markov\nChains and Their Applications.” Biometrika 57 (1):\n97–109.\n\n\nKatz, Elizabeth, and Bill & Melinda Gates Foundation. 2020.\n“Gender and Malaria Evidence Reivew.” Bill & Melinda\nGates Foundation. https://www.gatesgenderequalitytoolbox.org/wp-content/uploads/BMGF_Malaria-Review_FC.pdf.\n\n\nKrige, D. G. 1951. “A Statistical Approach to Some Basic Mine\nValuation Problems on the Witwatersrand.” Journal of the\nChemical, Metallurgical and Mining Society of South Africa 52:\n119–39.\n\n\nMatern, B. 2013. Spatial Variation. Lecture Notes in\nStatistics. Springer New York. https://books.google.co.uk/books?id=HrbSBwAAQBAJ.\n\n\nMatheron, G. 1963. “Principles of Geostatistics.”\nEconomic Geology 58: 1246–66.\n\n\nMetropolis, Nicholas, Arianna W. Rosenbluth, Marshall N. Rosenbluth,\nAugusta H. Teller, and Edward Teller. 1953. “Equation of State\nCalculations by Fast Computing Machines.” The Journal of\nChemical Physics 21 (6): 1087–92.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear\nModels.” Journal of the Royal Statistical Society A 135:\n370–84.\n\n\nPawitan, Yudi. 2001. In All Likelihood : Statistical Modelling and\nInference Using Likelihood. Oxford ; New York: Clarendon Press :\nOxford University Press.\n\n\nRipley, B. D. 1981. Spatial Statistics. New York: Wiley.\n\n\nRoberts, Gareth O., and Richard L. Tweedie. 1996. “Exponential\nConvergence of Langevin Distributions and Their Discrete\nApproximations.” Bernoulli 2 (4): 341–63.\n\n\nRoss, Sheldon. 2013. First Course in Probability, a. 9th ed.\nHarlow: Pearson Education UK.\n\n\nRossky, Peter J., J. D. Doll, and Harold L. Friedman. 1978.\n“Brownian Dynamics as Smart Monte Carlo Simulation.”\nThe Journal of Chemical Physics 69 (10): 4628–33.\n\n\nRue, H., S. Martino, and N. Chopin. 2009. “Approximate Bayesian\nInference for Latent Gaussian Models by Using Integrated Nested Laplace\nApproximations.” Journal of the Royal Statistical Society:\nSeries B (Statistical Methodology) 71 (2): 319–92. https://doi.org/10.1111/j.1467-9868.2008.00700.x.\n\n\nSmith, David L, Carlos A Guerra, Robert W Snow, and Simon I Hay. 2007.\n“Standardizing Estimates of the Plasmodium Falciparum Parasite\nRate.” Malaria Journal 6 (1): 131–31.\n\n\nStein, Michael L. 1999. Interpolation of Spatial Data Some Theory\nfor Kriging. 1st ed. 1999. Springer Series in Statistics. New York,\nNY: Springer New York : Imprint: Springer.\n\n\nStevenson, Gillian H. AND Gitonga, Jennifer C. AND Stresman. 2013.\n“Reliability of School Surveys in Estimating Geographic Variation\nin Malaria Transmission in the Western Kenyan Highlands.”\nPLOS ONE 8 (10). https://doi.org/10.1371/journal.pone.0077641.\n\n\nTene Fossog, Billy, Diego Ayala, Pelayo Acevedo, Pierre Kengne, Ignacio\nNgomo Abeso Mebuy, Boris Makanga, Julie Magnus, et al. 2015.\n“Habitat Segregation and Ecological Character Displacement in\nCryptic African Malaria Mosquitoes.” Evolutionary\nApplications 8 (4): 326–45. https://doi.org/10.1111/eva.12242.\n\n\nTobler, W. R. 1970. “A Computer Movie Simulating Urban Growth in\nthe Detroit Region.” Economic Geography 46: 234–40.\n\n\nWatson, G. S. 1971. “Trend -Surface Analysis.”\nMathematical Geology 3: 215–26.\n\n\n———. 1972. “Trend Surface Analysis and Spatial\nCorrelation.” Geological Society of America Special\nPaper 146: 39–46.\n\n\nWeisberg, Sanford. 2014. Applied Linear Regression. Fourth.\nHoboken NJ: Wiley. http://z.umn.edu/alr4ed.\n\n\nZhang, Hao. 2002. “On Estimation and Prediction for Spatial\nGeneralized Linear Mixed Models.” Biometrics 58 (1):\n129–36.\n\n\n———. 2004. “Inconsistent Estimation and Asymptotically Equal\nInterpolations in Model-Based Geostatistics.” Journal of the\nAmerican Statistical Association 99 (465): 250–61.\n\n\nZouré, Honorat GM, Mounkaila Noma, Afework H Tekle, Uche V Amazigo,\nPeter J Diggle, Emanuele Giorgi, and Jan HF Remme. 2014.\n“Geographic Distribution of Onchocerciasis in the 20 Participating\nCountries of the African Programme for Onchocerciasis Control: (2)\nPre-Control Endemicity Levels and Estimated Number Infected.”\nParasites & Vectors 7 (1): 326–26.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#introduction",
    "href": "02_handling-spatial-data.html#introduction",
    "title": "2  Handling of spatial data in R",
    "section": "",
    "text": "2.0.1 Spatial Data Handling in Geostatistical Analysis\nThroughout a geostatistical analysis, handling spatial data takes place at multiple key stages:\n\nRetrieving External Spatial Data Sources: Before starting the modeling process, one often needs to acquire spatial datasets from external sources to improve model accuracy. These could include satellite-derived environmental variables, population data from WorldPop, or climate data from WorldClim. Acquiring these datasets and ensuring they are in compatible formats and resolutions is an essential early step in spatial analysis.\nIn this chapter, we will review several open-access spatial data repositories and demonstrate how to access these covariates in R.\nImporting and Standardizing Spatial Data: Once external data is obtained, it must be imported into R. This involves handling various file formats such as shapefiles for vector data and GeoTIFFs for raster data. Moreover, different datasets might use different Coordinate Reference Systems (CRS), requiring CRS standardization to ensure that all datasets align correctly in the spatial framework. Failure to standardize CRS can result in spatial misalignments and incorrect results.\nThis chapter covers how to import spatial data into R using the sf and terra packages and how to handle CRS transformations to ensure data consistency.\nExtracting Covariate Information for Modeling: Covariate extraction is an essential step in geostatistical modeling. After importing spatial data, it is necessary to extract relevant covariate information both for the sampled locations (where we have observed data) and the prediction locations (where we wish to make predictions). This step involves linking raster or polygonal covariates—such as climate, population, or land cover data—to the geostatistical data points.\nIn this chapter, you will learn how to use spatial operations in R to extract covariates from polygon and raster layers and associate them with geostatistical data points for use in model fitting and predictions.\nPrediction and Creation of a Spatial Grid: For predictive geostatistical models, a regular grid of points is often created over the study region. Covariates must then be assigned to each grid point to make predictions across the entire region of interest. Creating predictive grids and linking them to the necessary covariates is key to generating continuous spatial predictions from geostatistical models.\nThis chapter will demonstrate how to create a predictive grid using sf and extract covariates for each grid point using terra, providing the foundation for model predictions.\nVisualizing Spatial Data: Visualization is crucial for exploring spatial data, interpreting model results, and communicating findings. Whether working with point-referenced data, polygons, or rasters, clear and effective visualization helps reveal patterns that inform the modeling process. Effective visualization can also help highlight covariate trends, spatial clusters, and uncertainty in predictions.\nThis chapter will guide you through visualizing various types of spatial data in R, using ggplot2, sf, and terra.\n\n\n\n2.0.2 Chapter Overview\nThis chapter introduces methods for handling spatial data at each stage of the geostatistical analysis workflow. You’ll learn how to:\n\nRetrieve covariates from external spatial data sources;\nImport and standardize spatial data;\nExtract covariates for geostatistical analysis;\nCreate predictive grids and link them with covariates;\nVisualize spatial data effectively to aid in analysis and presentation.\n\nBy mastering these steps, you’ll be equipped to handle the complexity of spatial data in geostatistical modeling, enhancing your analyses and improving your predictions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#spatial-data-handling-in-geostatistical-analysis",
    "href": "02_handling-spatial-data.html#spatial-data-handling-in-geostatistical-analysis",
    "title": "2  Handling of spatial data in R",
    "section": "2.2 Spatial data handling in geostatistical analysis",
    "text": "2.2 Spatial data handling in geostatistical analysis\nThroughout a geostatistical analysis, handling spatial data takes place at multiple key stages:\n\nRetrieving External Spatial Data Sources: Before starting the modeling process, one often needs to acquire spatial datasets from external sources to improve model accuracy. These could include satellite-derived environmental variables, population data from WorldPop, or climate data from WorldClim. Acquiring these datasets and ensuring they are in compatible formats and resolutions is an essential early step in spatial analysis.\nImporting and Standardizing Spatial Data: Once external data is obtained, it must be imported into R. This involves handling various file formats such as shapefiles or geopackages for vector data and GeoTIFFs for raster data. Moreover, different datasets might use different Coordinate Reference Systems (CRS), requiring CRS standardization to ensure that all datasets align correctly. Failure to standardize CRS can result in spatial misalignments and incorrect results.\nExtracting Covariate Information for Modeling: Covariate extraction is an essential step in geostatistical modeling. After importing spatial data, it is necessary to extract relevant covariate information both for the sampled locations (where we have observed data) and the prediction locations (where we wish to make predictions). This step involves linking raster or polygonal covariates—such as climate, population, or land cover data—to the geostatistical data points.\nPrediction and Creation of a Spatial Grid: For predictive geostatistical models, a regular grid of points is often created over the study region. Covariates must then be assigned to each grid point to make predictions across the entire region of interest. Creating predictive grids and linking them to the necessary covariates is key to generating continuous spatial predictions from geostatistical models.\nVisualizing Spatial Data: Visualization is crucial for exploring spatial data, interpreting model results, and communicating findings. Whether working with point-referenced data, polygons, or rasters, clear and effective visualization helps reveal patterns that inform the modeling process. Effective visualization can also help highlight covariate trends, spatial clusters, and uncertainty in predictions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#chapter-overview",
    "href": "02_handling-spatial-data.html#chapter-overview",
    "title": "2  Handling of spatial data in R",
    "section": "2.3 Chapter overview",
    "text": "2.3 Chapter overview\nThis chapter introduces methods for handling spatial data at each stage of the geostatistical analysis workflow. You’ll learn how to:\n\nRetrieve covariates from external spatial data sources;\nImport and standardize spatial data;\nExtract covariates for geostatistical analysis;\nCreate predictive grids and link them with covariates;\nVisualize spatial data effectively to aid in analysis and presentation.\n\nBy mastering these steps, you’ll be equipped to handle the complexity of spatial data in geostatistical modeling, enhancing your analyses and improving your predictions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "01_intro.html",
    "href": "01_intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Objectives of this book\nThe overall aim of this book is to provide you with the skills to perform a geostatistical analysis of a data-set using the R software environment. As you work your way through the book, you will learn to:\nAlthough the focus of this book is on public health, the statistical ideas, as well as the software used, can also be applied for the analysis of geostatistical data-sets arising from other scientific fields.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#objectives-of-this-book",
    "href": "01_intro.html#objectives-of-this-book",
    "title": "1  Introduction",
    "section": "",
    "text": "explore geostatistical data-sets using graphical procedures and summary statistics;\nformulate and fit geostatistical models using the maximum likelihood estimation method;\ncarry out prediction of health outcomes at different spatial scales;\nvisualize and interpret the results from geostatistical models;\nmodel the relationships between spatially referenced risk factors and the health outcome of interest;\nvalidate the assumptions of geostatistical models and assess their predictive performance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#pre-requisites-for-using-this-book",
    "href": "01_intro.html#pre-requisites-for-using-this-book",
    "title": "1  Introduction",
    "section": "1.2 Pre-requisites for using this book",
    "text": "1.2 Pre-requisites for using this book\nTo effectively understand and use the material presented in this book, it is expected that you should possess prior knowledge of basic probability theory, foundational topics in statistical modelling and R programming. Below we provide a more detailed explanation of the pre-requisites for each of these three fields.\n\n1.2.1 Topics in probability\nBasics probability theory is important to fully understand the content of this book. In particular, you should have knowledge of: the general definition and properties of continuous and discrete distribution; how the describe the properties of probability distributions through their mean, variance and skeweness; the concepts of stochastic dependence and correaltion; the distinction between marginal and conditional distributions; the basic properties of the Gaussian, Binomial and Poisson distributions; the definition and properties of the multivariate Gaussian distribution. The redear can find an extensive explanation and illustrations with examples of all these topics in Ross (2013).\n\n\n1.2.2 Topics in statistics\nLikelihood-based inference (whether frequensist or Bayesian) provides the theoretical bedrock for the estimation of almost any statistical model. In this book will focus on maximum likelihood estimation methods of inference. Extensive use of the notions of point and interval estimates obtained using the maximum likelihood estimation methods will be made through the book. Recommended readings include chapters 1, 2 and 4 of Pawitan (2001).\nGood prior knowledge of Generalized linear models (GLMs) is essential, as the geostatistical modelling framework builds on these as an extension. Before embarking on the use of this book, we thus encourage you to review the basic theory of GLMs and, in particular, how these are applied and interpreted. In this book, we will cover examples that will model continuously measured outcomes and counts. Hence, good understanding of linear regression modelling and modelling of counts data using Binomial and Poisson regression should be the main focus of the review. For comprehensive overview of GLMs and their implementation in R, we refer you to Dobson and Barnett (2008).\n\n\n1.2.3 Topics in R programming\nAlthough this book does not require to possess advanced skills in R programming, it is important you have good knowledge in the following topics: creation and manipulation of vectors and matrices; logical vectors; character vectors; handling of lists and data frame objects; reading data into R; graphical procedures. A very large amount of freely available material covering these topics can be found online. Our recommendation is to start from the manual “An introduction to R” of the Comprehensive R Archive Network available at this link, available at R manual.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#obtaining-and-running-the-r-packages",
    "href": "01_intro.html#obtaining-and-running-the-r-packages",
    "title": "1  Introduction",
    "section": "1.3 Obtaining and running the R packages",
    "text": "1.3 Obtaining and running the R packages\nIt is advised that you obtain the latest 64-bit version of R in order to run the R code of this book. To install R, go to the R website, where you can download the installer packages for Windows and Mac, and find instructions for Linux, using binary files.\n\nWindows\nMac\nLinux\n\nThe list of the R packages used in this book is provided in Table 1.1.\n\n\n\nTable 1.1: List of the R packages that will be used in the book with a description of their use in the data analysis. The packages marked by (E) are essential for the geostatistical analysis. Those instead marked by (R) are recommended and can be helpful to overcome issues as described under the column “Used for”.\n\n\n\n\n\n\n\n\n\nR packages\nUsed for\n\n\n\n\nRiskMap (E)\nEstimating of geostatistical models and spatial prediction\n\n\nsf (E)\nHandling of spatial data in R\n\n\nterra (E)\nHandling of raster files in R\n\n\nggplot2 (E)\nCreating maps and exploratory plots\n\n\ncrsuggest (R)\nGuessing a coordinate reference systems when unknown\n\n\n\n\n\n\nTo install packages in R for the first time, you can use the command install.packages in the R console, as shown below for the RiskMap package.\n\ninstall.packages(\"RiskMap\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#sec-examples-ch1",
    "href": "01_intro.html#sec-examples-ch1",
    "title": "1  Introduction",
    "section": "1.4 Data-sets used in the book",
    "text": "1.4 Data-sets used in the book\nThe geostatistical data-sets described in this section will be used throughout the book to illustrate the use of the R packages mentioned in the previous sections.\nEach of the examples data-sets can be loaded from the RiskMap package, using the command\n\ndata(NAME_OF_THE_DATASET)\n\nwhere in place of NAME_OF_THE_DATASET you should type of the name of one of the data-sets listed in Table 1.2.\n\n\n\nTable 1.2: List of data-sets available from the RiskMap package. Data-sets listed as “Example” are used throughout the book to illustrate the use of R functions. Data-sets listed as “Case study” are analysed in Chapter 6.\n\n\n\n\n\n\n\n\n\n\nNames of the data-set\nShort description\nUsed in this book as\n\n\n\n\ngalicia\nLead concentration m from moss samples collected in Galicia, Northern Spain\nExample\n\n\nliberia\nPrevalence data on river-blindness from Liberia\nExample\n\n\nmalkenya\nMalaria prevalence data from a community and school survey conducted in Western Kenya\nExample\n\n\nitaly_sim\nSimulated geostatistical data-set within the Italian national boundaries\nExample\n\n\nmalnutrition\nData on stunting among children in Ghana\nCase study\n\n\n\n\n\n\n\n1.4.1 Lead concentration in Galicia\n\n\n\n\n\n\n\n\nFigure 1.1: Data on the meausred lead concentration (in micrograms per gram dry weight) in moss samples collected in Galicia, North-West of Spain.\n\n\n\n\nLead is a heavy metal which, in high concentrations, can cause chronic damage to living organisms over a long period of time. For this reason its spread and source must be regularly monitored. To assess the extent of the contamination in an area, measurements of lead are often taken from plants. The data here considered (Figure 1.1) consist of 132 locations of moss samples collected in 2000, in and around Galicia, a region in the North-Western part of Spain. One of the objectives of this survey was to establish the spatial pattern of lead concentration in Galicia so as to better identify possible sources of contamination; fore more information, see Fernández, Rey, and Carballeira (2000).\nIn this case, geostatistical modelling can be used to predict the lead concentration across Galicia and allows to disentangle variation which is purely random, possibly due to measurement error, and genuine spatial variation, which is our main object of interest.\nThis data-set will be used in this book to show how to carry out the spatial analysis of continuously measured variables using linear geostatistical models.\n\n\n1.4.2 River-blindness in Liberia\n\n\n\n\n\n\n\n\nFigure 1.2: River-blindness data from a cross-sectional survey carried out in Liberia.\n\n\n\n\nIn low-resource settings, where disease registries are typically absent, cross-sectional surveys are an essential monitoring tool that enables the estimation of the disease burden in a population of interest. The data considered in this example (Figure 1.2) have been collected as part of an Africa-wide initiative called the Rapid Epidemiological Mapping of Onchocerchiasis (REMO) carried out in 2011 in 20 African countries (Zouré et al. 2014). The goal of REMO is to identify areas where river-blindness (or onchocerchiasis), a disease transmitted by black flies who breed along fast flowing rivers, is still a public health problem. In this context, it is especially of interest to identify communities with a prevalence above 20% and for treatment is urgently needed.\nIn this book, we will use data collected from Liberia to model nodule prevalence, which is based on a alternative and cheaper diagnostic technique for river-blindness. In the analysis of this data-set, we will illustrate how to formulate and fit Binomial geostatistical models, and how these can be used to predict prevalence within a region of interest.\n\n\n1.4.3 Malaria in the Western Kenyan Highlands\n\n\n\n\n\n\n\n\nFigure 1.3: Malaria prevalence data from a cross-sectional survey carried out in Nyanza Province, in the Western Highlands of Kenya.\n\n\n\n\nMalaria is one of deadliest diseases that affects populations living in tropical and subtropical countries. It is caused by a parasite of the genus Plasmodium which is transmitted through the infectious bite of female Anopheles mosquitoes. In the following chapters, we shall analyse a data-set from a cross-sectional community survey carried out in July 2010 in Nyanza Province, in the Western Highlands of Kenya (Stevenson 2013).\nWhat distinguishes this from the other examples data-sets is that the data contain both individual-level and household-level information. The outcome of interest is the result from a rapid diagnostic test for malaria which. In the book, we will illustrate how to account for the the hierarchical structure of the data and the binary nature of the outcome at each of the stages of the geostatistical analysis.\n\n\n1.4.4 Anopheles gambiae mosquitoes in Southern Cameroon\n\n\n\n\n\n\n\n\nFigure 1.4: Map of the collected number of Anopheles gambiae mosquitoes in an area of Southern Cameroon.\n\n\n\n\nIn studies of vector-borne and zoonotic diseases, understanding of the vector distribution can help to better guide the decision-making process for the implementation, monitoring and evaluation of control programmes. Anopheles gambiae mosquitoes are one of the main vectors for malaria transmission in sub-Saharan Africa. Their distribution over space is affected by several environmental and climatic factors, including temperature, humidity and vegetation.\nThe data-set on mosquitoes (Figure 1.4) that will use in the book consists of a sub-set taken from a large database (Tene Fossog et al. 2015). This was assembled in order to understand how the environment affects the distribution of different species of Anopheles mosquitoes in sub-Saharan Africa. This example data-set will be used to illustrate the application of Poisson geostatistical models for mapping mosquitoes abundance.\n\n\n1.4.5 Simulated-dataset\n\n\n\n\n\n\n\n\nFigure 1.5: Map of the locations of the simulated data-set generated over Italy for a continuous outcome.\n\n\n\n\nThis data-set was generated using a geostatistical model, with the addition of unstructured random effects at provincial and regional level. More details on how this data-set was generated will be provided in Section 3.2. Whilst this data-set does not have any scientific relevance like the other data-sets used in this book, it will serve us to illustrate some of the more advanced features of the package that enable the inclusion of random effects, in addition to the latent Gaussian process that is common to all geostatistical models. The skills you will aquire through the analysis of this data-set will be useful for the analysis of data-sets presented as case studies in Chapter 6.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#sec-geostat-models",
    "href": "01_intro.html#sec-geostat-models",
    "title": "1  Introduction",
    "section": "1.5 Geostatistical problems and geostatistical models",
    "text": "1.5 Geostatistical problems and geostatistical models\nWhat the examples of the previous section have in common is that, in each case, the goal of statistical analysis is to draw inferences on an unobserved spatially continuous surface using data collected from a finite set of locations. The lead concentration in Galicia, the prevalence for river-blindness in Liberia and the abundance of A. gambiae mosquitoes in Cameroon can all be represented as spatially continuous processes that originate from the combined effects of environmental factors. We denote this class of inferential problems as geostatistical problems for which a solution can be found through the development and application of suitable geostatistical models, which are the subject of this book.\nAs one can soon realize, geostatistical problems are not unique to global health but arise in many other fields of science, including economics, physics, biology, geology and others. It thus comes to no surprise that geostatistics was initially developed in the South African mining industry in the 1950s (Krige 1951). This was then further developed as a self-contained discipline by Georges Matheron and other researchers at Fontainebleau, in France (Matheron 1963; Chilès and Delfiner 2016). In Watson (1971) and Watson (1972) a first connection is drawn between geostatistics and the prediction of stochastic processes. However, it is only with Ripley (1981) and then Cressie (1991) that geostatistics is explicitly brought into a classical statistical framework for the analysis of spatially referenced data. Diggle, Tawn, and Moyeed (1998) coined the term model-based geostastics and introduced this as belonging to the general class of generalized linear mixed models (Breslow and Clayton 1993), while emphasizing the use of likelihood-based methods of inference. As in Diggle, Tawn, and Moyeed (1998), also in this book, we advocate the application of model-based geostistical models as a class of parametric statistical models on which inference can be carried out using either maximum likelihood estimation or Bayesian methods.\nMore precisely, our attention will be directed at the class of generalized linear geostatistical models, or GLGM. To formally specify this, we first define the random variables \\(S\\), a spatial stochastic process, and the random variable \\(Y= (Y_1, \\ldots, Y_n)\\) which correspond to the outcome observed at a set of locations \\(X = (x_1, \\ldots, x_n)\\). Let us use \\([A]\\) to denote “the distribution of the random variable \\(A\\)”. To formulate a GLGM, we should then specify the joint distribution of \\(S\\) and \\(Y\\), which we write as\n\\[\n[Y, S] = [S] [Y | S].\n\\tag{1.1}\\]\nOn the right-hand side of the equation above, we have factorized the joint distribution of \\(Y\\) and \\(S\\), as the product between the marginal distribution of \\(S\\) and the conditional distribution of \\(Y\\) given \\(S\\). Hence, the formulation of a GLGM can be break down into the tasks of formulating \\([S]\\) and \\([Y | S]\\).\nIn defining \\([S]\\), throughout the book, we shall assume that this is a zero-mean stationary and isotropic Gaussian process. In other words, these assumptions impose that the joint distribution of \\(S(X) = (S(x_1),\\ldots,S(x_n))\\), i.e. the process \\(S\\) at the sampled locations \\(x_1, \\ldots, x_n\\), is invariant with respect to rations and translations of the locations \\(X\\). In practical terms, the main implication of this is that, for any pair of locations \\(x_i\\) and \\(x_j\\) the correlation function \\(\\rho(\\cdot)\\) between \\(S(x_i)\\) and \\(S(x_j)\\) is purely a function of the Euclidean distance, \\(u_{ij}\\), between \\(x_i\\) and \\(x_j\\), i.e. \\[\n{\\rm cov}\\{S(x_i), S(x_j)\\} = \\sigma^2\\rho(u_{ij}),\n\\tag{1.2}\\]\nwhere \\(\\sigma^2\\) is the variance of \\(S(x)\\) for all \\(x\\). Below we provide more details on the type of covoriance functions that we consider in this book. Furthermore, the fact that we assume the process \\(S\\) to have mean zero is because this is process acts as a residual term in our modelling of \\(Y\\). This aspect will be reiterated several times in the following chapters, as it as important implications for the interpretation of the other components of a geostatistical model, as well understanding the results of the analysis.\nFinally, we model \\([Y | S]\\), i.e. the distribution of \\(Y\\) given \\(S\\), is modeled as a set of mutually independent distributions which belong the exponential family, as defined in classical generalized linear modelling framework (Nelder and Wedderburn 1972). It then follows that, we can write \\([Y | S]\\) as\n\\[\n[Y | S] = \\prod_{i=1}^n [Y_i | S(x_i)].\n\\tag{1.3}\\]\nThe final step then consists of specifying a distribution for \\([Y_i | S(x_i)]\\). Table 1.3 gives the range, mean and variance the three specifications for $[Y_i | S(x_i)]$$ which we will consider in this book. In Table 1.3, the canonical function, say \\(g(\\cdot)\\), denotes the natural transformation of the mean component \\(\\mu_i\\) that allows us to introduce both covariates and the spatial process \\(S(x_i)\\) into the model so as to explain the variation in \\(\\mu_i\\) as\n\\[\ng(\\mu_i) = d(x_i)^\\top \\beta + S(x_i).\n\\tag{1.4}\\]\nwhere \\(d(x_i)\\) is a vector of spatially referenced covariates with associated regression coefficients \\(\\beta\\). Finally, the quantity \\(m_i\\), which appears in the formulation of the Binomial and Poisson distributions, is an offset quantity and is used to account for the number of tests or the population size at a given location \\(x_i\\).\n\n\n\nTable 1.3: Type of outcomes \\(Y_{i}\\) considered in this book.\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nRange of \\(Y_i\\)\nMean of \\([Y_i | S(x_i)]\\)\nVariance of \\([Y_i | S(x_i)]\\)\nCanonical link\n\n\n\n\nGaussian\n\\((-\\infty, +\\infty)\\)\n\\(\\mu_i\\)\n\\(\\tau^2\\)\n\\(g(\\mu_i) = \\mu_i\\)\n\n\nBinomial\n\\(1,\\dots,m_i\\)\n\\(m_i\\mu_i\\)\n\\(m_i\\mu_i(1-\\mu_i)\\)\n\\(g(\\mu_i) = \\log\\{ \\mu_i/(1-\\mu_i) \\}\\)\n\n\nPoisson\n\\(1,2,\\ldots,\\infty\\)\n\\(m_i\\mu_i\\)\n\\(m_i\\mu_i\\)\n\\(g(\\mu_i) = \\log\\{ \\mu_i \\}\\)\n\n\n\n\n\n\nBased on the formulation in (1.4), we can see that \\(S(x_i)\\) quantifies residual spatial effects on \\(\\mu_i\\) that have not been accounted for by the covariates \\(d(x_i)\\). In an ideal scenario, the covariates \\(d(x_i)\\) should explain all the spatial variation without the need for \\(S(x_i)\\). Although this unrealistic, in practice we may be able to most of the variation in \\(\\mu_i\\) through \\(d(x_i)\\) and, hence, reduce \\(S(x_i)\\) to a negligible component. In Chapter 2, we will show how a thorough exploratory analysis can help to understand whether we have come close to that ideal scenario or, if instead, we need the use of GLGM to model the data.\nThe model described in (1.4) can be seen as the most basic GLGM that can be used for a geostatistical analysis. As we will see in the analysis of some of the examples and, in Chapter 6, for the case studies, extensions of this model will be required to accommodate the intrinsic non-spatial random variation of the data which is not captured by the covariates.\nThe types of problems that statistical models are applied to can be distinguished into three main categories: prediction problems; explanatory problems; problems of hypothesis testing. Most of the times, geostatistical problems tend to fall under the first category, where the goal is make predictive inferences on the process \\(S(x)\\) at location \\(x\\), which is usually outside of the set of sampled locations. However, as will illustrate in the later chapters, geostatistical models play an important also in the other two types of problems. In particular, we will show that spatial correlation can have a substantial impact on the point estimates and standard errors for \\(\\beta\\). Hence, if the goal of the analysis is explain the relationship between a covariate \\(d(x)\\) with the mean component \\(\\mu\\).\n\n1.5.1 The Matern family of correlation functions\nThroughout the book, we shall consider the Matern (2013) family of correlation functions to model the spatial correlation of the Gaussian process \\(S(x)\\). This defined as \\[\n\\rho(u;\\phi,\\kappa) =\\{2^{\\kappa-1} \\Gamma(\\kappa)\\}^{-1} (u/\\phi)^\\kappa K_\\kappa(u/\\phi),\n\\tag{1.5}\\] where \\(\\phi&gt;0\\) and \\(\\kappa&gt;0\\) are parameters and \\(K_\\kappa(\\cdot)\\) is the modified Bessel function of the third kind of order \\(\\kappa\\). The parameters \\(\\phi\\) and \\(\\kappa\\) regulate how fast the spatial correlation decays to zero for increasing distance and the smoothness of the process, respectively. A special case of Matern family of correlation functions, which is obtained when \\(\\kappa=0.5\\), will be of particular relevance to the application considered in this book. This is the expeonential correlation function which we write as \\[\n\\rho(u;\\phi) = \\exp\\{-u/\\phi\\}.\n\\tag{1.6}\\]\nAnother special case, which we dot consider in this book but has often been used in machine learning applications, is the Gaussian correlation function obtain as a limiting case for \\(\\kappa \\to +\\infty\\) the possible smoothest process arising from the Matern family.\nTo better understand how \\(\\phi\\) and \\(\\kappa\\) affect the spatial correlation and the pattern of the spatial of the spatial surface, we now consider some examples.\n\n\n\n\n\n\n\n\nFigure 1.6: Examples of stationary and isotropic Matern correlation functions. Panel (a) shows three different correlations functions that have the same smothness parameter of \\(\\kappa=0.5\\), while varying the scale parameter \\(\\phi\\) over \\({0.05, 0.1, 0.2}\\). In panel (b) the scale of the spatial correlation \\(\\phi\\) is chosen so that each of the three functions reaches 0.05 at distance 0.3 (as also shown by the horizontal and vertical black dashed segments).\n\n\n\n\n\nFigure 1.6 shows six different Matern correlation functions. In panel (a), we have kept \\(\\kappa\\) fixed to 0.5 and varied \\(\\phi\\) over the values 0.05, 0.1 and 0.2. As expected, for larger values of \\(\\phi\\) the correlation function has a slower decay to zero. Panels (a) to (c), in Figure 1.7, show three realizations of a Gaussian process from each of these correlation functions. The mean of the Gaussian process was set to zero and variance to 1. We can observe that spatial correlations with larger scales are associated with longer spatial trends, whilst smaller scales exhibit a patchier pattern. This is because, as \\(\\phi\\) takes values that are closer to zero, the spatial surface will tend to show a less structured pattern and will revert towards its zero mean more rapidly. In our examples in the book, we will often use the so called practical range to interpret our estimates for \\(\\phi\\). The practical range is defined as the distance at which spatial correlation reaches 0.05, hence one can interpret this as the distance beyond which observations can be considered approximately independent. In the case of the exponential correlation function, the practical range is \\(\\log(20) \\times \\phi \\approx 3 \\phi\\).\nFinally, let us consider the correlation functions, shown in the panel (b) of Figure 1.6. Here, we have varied \\(\\kappa\\) over the values 0.5, 1.5 and 2.5, whilst \\(\\phi\\) has been fixed in order to force all three correlation functions to reach 0.05 for distance 0.3. In this way, we can better observe the effect of different values \\(\\kappa\\) on the spatial surface for process that have approximately the same range for the spatial correlation. In Figure 1.7, we observe three realizations from these correlation functions. We observe that the differences between the different surface are determined by the small spatial scale behaviour; \\(\\kappa = 0.5\\) correspond to a rougher and less regular spatial pattern, whilst \\(\\kappa=2.5\\) shows a smoothest surface of the three processes considered. These properties of the spatial surface are related to the so called differentiability of the the Guassian process, which determines its local behaviour. If you are interested in delving these theoretical aspects, we suggest reading Chapter 2 of Stein (1999).\n\n\n\n\n\n\n\n\nFigure 1.7: Simulated spatial surface using the three correltion functions shown in Figure 1.6. Panels (a), (b) and (c) correspond to the correlation functions from panel (a) in Figure 1.6 and in order these are: \\(\\phi = 0.05\\) and \\(\\kappa=0.5\\); (b) \\(\\phi=0.1\\) and \\(\\kappa=0.5\\); (c) \\(\\phi=0.2\\) and \\(\\kappa=0.5\\). Panels (c), (d) and (e) correspond to the correlation functions from panel (b) in Figure 1.6 and in order these are: \\(\\phi = 0.1\\) and \\(\\kappa=0.5\\); (b) \\(\\phi=0.063\\) and \\(\\kappa=1.5\\); (c) \\(\\phi=0.051\\) and \\(\\kappa=2.5\\).\n\n\n\n\n\nThe flexibility provided by the Matern correlation function in capturing different forms of spatial correlations has made one of, if not the most widely used correlation function in model-based geostatistics (Stein 1999). For this reason, in this book we will consider the Matern correlation function. We will consider estimation issues related to the Matern correlation in Chapter 3.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01_intro.html#workflow-of-a-statistical-analysis-and-structure-of-the-book",
    "href": "01_intro.html#workflow-of-a-statistical-analysis-and-structure-of-the-book",
    "title": "1  Introduction",
    "section": "1.6 Workflow of a statistical analysis and structure of the book",
    "text": "1.6 Workflow of a statistical analysis and structure of the book\n\n\n\n\n\n\nFigure 1.8: Stages of a statistical analysis\n\n\n\nFigure 1.8 shows the different stages that will follow in carrying the geostatistical analysis of the examples introduced in Section 1.4. The exploratory analysis of the data is an essential first step that is used to understand the empirical associations between risk factors and the the health outcome of interest. In our case, this first stage is also used to justify the use of geostatistical models by questioning the underlying assumptions of standard generalized linear models. Based on the results obtained from the exploration of the data, we then formulate a suitable statistical model and estimate its parameters using likelihood based methods of inference. These also allows us to obtain uncertainty measures about the strength of associations of regression relationships and the other model parameters that define the shape of the spatial correlation in the data. Following the estimation of the model, we then proceed to validate its underlying assumptions using suitable diagnostics that assess whether the model can later be sufficiently trusted to represent the observed variation in the modelled outcome. At this stage, if the diagnostics checks yield results that indicate the incompatibility of the model with the data, we then back to the stage of model formulation and address the issues arisen from the validation stage. If instead, we do not find any evidence against the fitted model we can proceed to carry out sptatial prediction. At this stage, it is important to define suitable predictive targets that can help us to better answer the original research question and better assist the decision making process. The final step of visualization of uncertainty plays an important role in geostatistical analysis in order to convey the main findings of the study in an effective and easy-to-understand way for a wider audience which also consists of non-experts.\nIn the remainder of this book, each chapter focuses on a specific stage as shown in Figure 1.8. We treat visualization of uncertainty together with spatial prediction in Chapter 5.\nChapter 2 will provide an overview of how to handle saptial data in R, in particular raster and vector data (both points and polygons). The skills learned in this chapter will be applied throughout the book, and will especially be useful in Chapter 5 and Chapter 6 for generating predictive maps of the modelled outcome.\nChapter 3 focuses on the model building process and estimation of geostatistical models. This chapter will show how to carry out initial exploratory analyses of the data to inform the formulation of suitable geostatistical models and how these can be fitted using maximum likelihood estimation methods.\nChapter 4 illustrated the use of methods that can be used to validate the assumptions and calibration of statistical models.\nChapter 5 shows how geostatistical models can be used to carry out spatial prediction of a health outcome of interest both on a spatially continuous and spatially aggregated scales.\nFinally, Chapter 6 presents the application of all the methods illustrated in the previous chapters to three additional data-sets. This chapter offers a summary of the content of book by putting together all the stages in the geostatistical analyses for each of the three case studies, and illustrates additional functionalities of the RiskMap R package not covered in the previous chapters.\n\n\n\n\nBreslow, N. E., and D. G. Clayton. 1993. “Approximate Inference in Generalized Linear Mixed Models.” Journal of the American Statistical Association 88: 9–25.\n\n\nChilès, J-P, and P. Delfiner. 2016. Geostatistics (Second Edition). Hoboken: Wiley.\n\n\nCressie, N. A. C. 1991. Statistics for Spatial Data. New York: Wiley.\n\n\nDiggle, P. J., J. A. Tawn, and R. A. Moyeed. 1998. “Model-Based Geostatistics.” Journal of the Royal Statistical Society: Series C (Applied Statistics) 47 (3): 299–350. https://doi.org/10.1111/1467-9876.00113.\n\n\nDobson, A. J., and A. Barnett. 2008. An Introduction to Generalized Linear Models. Third. Chapman; Hall/CRC.\n\n\nFernández, J. A, A Rey, and A Carballeira. 2000. “An Extended Study of Heavy Metal Deposition in Galicia (NW Spain) Based on Moss Analysis.” Science of The Total Environment 254 (1): 31–44. https://doi.org/10.1016/S0048-9697(00)00431-9.\n\n\nKrige, D. G. 1951. “A Statistical Approach to Some Basic Mine Valuation Problems on the Witwatersrand.” Journal of the Chemical, Metallurgical and Mining Society of South Africa 52: 119–39.\n\n\nMatern, B. 2013. Spatial Variation. Lecture Notes in Statistics. Springer New York. https://books.google.co.uk/books?id=HrbSBwAAQBAJ.\n\n\nMatheron, G. 1963. “Principles of Geostatistics.” Economic Geology 58: 1246–66.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society A 135: 370–84.\n\n\nPawitan, Yudi. 2001. In All Likelihood : Statistical Modelling and Inference Using Likelihood. Oxford ; New York: Clarendon Press : Oxford University Press.\n\n\nRipley, B. D. 1981. Spatial Statistics. New York: Wiley.\n\n\nRoss, Sheldon. 2013. First Course in Probability, a. 9th ed. Harlow: Pearson Education UK.\n\n\nStein, Michael L. 1999. Interpolation of Spatial Data Some Theory for Kriging. 1st ed. 1999. Springer Series in Statistics. New York, NY: Springer New York : Imprint: Springer.\n\n\nStevenson, Gillian H. AND Gitonga, Jennifer C. AND Stresman. 2013. “Reliability of School Surveys in Estimating Geographic Variation in Malaria Transmission in the Western Kenyan Highlands.” PLOS ONE 8 (10). https://doi.org/10.1371/journal.pone.0077641.\n\n\nTene Fossog, Billy, Diego Ayala, Pelayo Acevedo, Pierre Kengne, Ignacio Ngomo Abeso Mebuy, Boris Makanga, Julie Magnus, et al. 2015. “Habitat Segregation and Ecological Character Displacement in Cryptic African Malaria Mosquitoes.” Evolutionary Applications 8 (4): 326–45. https://doi.org/10.1111/eva.12242.\n\n\nWatson, G. S. 1971. “Trend -Surface Analysis.” Mathematical Geology 3: 215–26.\n\n\n———. 1972. “Trend Surface Analysis and Spatial Correlation.” Geological Society of America Special Paper 146: 39–46.\n\n\nZouré, Honorat GM, Mounkaila Noma, Afework H Tekle, Uche V Amazigo, Peter J Diggle, Emanuele Giorgi, and Jan HF Remme. 2014. “Geographic Distribution of Onchocerciasis in the 20 Participating Countries of the African Programme for Onchocerciasis Control: (2) Pre-Control Endemicity Levels and Estimated Number Infected.” Parasites & Vectors 7 (1): 326–26.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#importing-and-standardizing-ppatial-data",
    "href": "02_handling-spatial-data.html#importing-and-standardizing-ppatial-data",
    "title": "2  Handling of spatial data in R",
    "section": "2.5 Importing and standardizing ppatial data",
    "text": "2.5 Importing and standardizing ppatial data\nIn geostatistical analysis, importing and standardizing spatial data is a critical step to ensure that data from different sources align and can be used effectively. Spatial data, whether it's vector data (points, lines, polygons) or raster data (grids), can come in various formats and may use different Coordinate Reference Systems (CRS). To perform accurate spatial analyses, it's essential to import data correctly and ensure consistency in terms of projection and format. This section will cover how to import vector and raster data into R, explain the concept of CRS and sensure that different datasets align properly for subsequent geostatistical analysis.\n\n2.5.1 Importing vector data\nVector data are typically stored in formats such as shapefiles (.shp) or geopackages (.gpkg). The sf (simple features) package in R is the most common tool for handling vector data.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#importing-and-standardizing-spatial-data",
    "href": "02_handling-spatial-data.html#importing-and-standardizing-spatial-data",
    "title": "2  Handling of spatial data in R",
    "section": "2.5 Importing and standardizing spatial data",
    "text": "2.5 Importing and standardizing spatial data\nIn geostatistical analysis, importing and standardizing spatial data is a critical step to ensure that data from different sources align and can be used effectively. Spatial data, whether it's vector data (points, lines, polygons) or raster data (grids), can come in various formats and may use different Coordinate Reference Systems (CRS). To perform accurate spatial analyses, it's essential to import data correctly and ensure consistency in terms of projection and format. This section will cover how to import vector and raster data into R, explain the concept of CRS and sensure that different datasets align properly for subsequent geostatistical analysis.\n\n2.5.1 Importing vector data\nVector data are typically stored in formats such as shapefiles (.shp) or geopackages (.gpkg). The sf (simple features) package in R is the most common tool for handling vector data.\n\n# Load the sf package\nlibrary(sf)\n\n# Import a shapefile (e.g., administrative boundaries)\nadmin &lt;- st_read(\"path_to_your_shapefile/Admin_Boundaries.shp\")\n\n# Inspect the data\nprint(admin)\nplot(admin)  # Basic plot of the shapefile\n\nThe st_read() function reads various spatial data formats, automatically recognizing file types.\n\n\n2.5.2 Importing raster data\nRaster data consists of a grid of cells, where each cell holds a value representing a spatial attribute such as elevation or temperature. The terra package in R is designed to work with raster data and has superseded the older raster package due to better performance and greater functionality. Let’s see now how to import a GeoTIFF file using terra. We can upload the population raster for Liberia that we have downloaded in Section 2.4.2.\n\n# Load the terra package\nlibrary(terra)\n\n# Import a raster file \nlbr_pop_100 &lt;- rast(lbr_pop_url)\n\n# Inspect the raster data\nprint(lbr_pop_100)\n\nclass       : SpatRaster \ndimensions  : 5040, 4945, 1  (nrow, ncol, nlyr)\nresolution  : 0.0008333333, 0.0008333333  (x, y)\nextent      : -11.48625, -7.365417, 4.352084, 8.552084  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : lbr_ppp_2014.tif \nname        : lbr_ppp_2014 \nmin value   :  0.006426936 \nmax value   : 92.716874581 \n\n# Basic plot of the raster\nplot(lbr_pop_100)\n\n\n\n\n\n\n\nFigure 2.2: Liberia population count for 2014 at 100m resolution\n\n\n\n\n\nThe output of print() provides a detailed summary of the raster’s properties. The rast() function reads raster files in formats like GeoTIFF, ASCII grid, or other common raster formats and then we used plot() as a quick way to visualize raster data.\n\n\n2.5.3 Understanding coordinate reference systems (CRS)\nWhen working with spatial data, especially from different sources, one of the most critical tasks is ensuring that all datasets share the same Coordinate Reference System (CRS). A CRS defines how the two-dimensional map data corresponds to locations on the three-dimensional Earth. If different layers (e.g., raster and vector data) have different CRSs, they may not align correctly when plotted or analyzed together, leading to inaccurate analyses or visualizations.\nCRSs can either be:\n\nGeographic: These use latitude and longitude coordinates to represent locations on the Earth’s surface. The most common example is WGS84 (EPSG:4326), the default CRS used by GPS and global datasets.\nProjected: These convert the Earth’s curved surface to a flat map and preserve certain properties like area, distance, or direction. Examples include Universal Transverse Mercator (UTM) or Albers Equal Area projections.\n\nIn many cases, using a projected rather than a geographic CRS s preferred, especially when any summary statistic or parameter is distance-related. In a geographic CRS, distances between two points are calculated using angular coordinates (degrees), which do not translate easily into linear units like meters or kilometers. This makes interpreting distances challenging, as the length of a degree of latitude differs from the length of a degree of longitude. In contrast, a projected CRS uses a linear coordinate system (usually meters), ensuring that distances are accurately represented on a flat surface. This is important when computing spatial variograms, covariance functions, or some parameters in geostatistical models, where distance between sampling locations is a key factor. In this context, we assume that the default distance metric is Euclidean distance. However, an important exception to our recommendation arises when analyzing data across large, global-scale regions. In such cases, it is more appropriate to use a geographic CRS along with spherical distances, as these better reflect the curved nature of the Earth’s surface.\n\n\n2.5.4 EPSG codes\nAn EPSG code is a unique identifier that defines a CRS. These codes, managed by the European Petroleum Survey Group (EPSG), are widely used in geographic information systems (GIS) to simplify the use of specific projections, ensuring that spatial data is correctly aligned and interpreted. Each EPSG code corresponds to a unique CRS or map projection, making it easier to standardize and manage spatial data from different sources.\nSome key and often used EPSG codes are:\n\nEPSG:4326: This code represents WGS84, the most commonly used geographic CRS, which uses latitude and longitude to describe locations on the Earth’s surface. It is the default CRS for global datasets and GPS systems.\nEPSG:326XX: These codes represent the UTM (Universal Transverse Mercator) projection, which divides the world into zones. Each zone is optimized to preserve local distances and areas. For example (e.g. EPSG:32629: UTM Zone 29N, covering parts of Western Africa, including Liberia)\nEPSG:3857: This code is for the Web Mercator projection, which is widely used for web mapping services, including Google Maps, OpenStreetMap, and Bing Maps. This projected CRS uses meters as the unit of distance and is optimized for visualizing maps on a 2D plane, though it distorts area and distance, especially at high latitudes. It is well-suited for interactive online mapping but not ideal for precise distance-based geostatistical analyses.\n\n\n\n2.5.5 Convert a data frame to an sf object\nIn geospatial analysis, data is often provided in tabular formats like CSV files that contain spatial coordinates (e.g., latitude and longitude). To use these data effectively in R, it is necessary to convert the data frame into an sf object, which is the standard format for working with spatial data in R. Here we show how to achieve this, we can use the Liberia data available in the RiskMap package as it is a data frame.\n\n# Load the RiskMap package\nlibrary(RiskMap)\n\n# Load the Liberia data set\ndata(\"liberia\")\n\n# Convert the data frame to an sf object\nliberia_sf &lt;- st_as_sf(liberia, \n                       coords = c(\"long\", \"lat\"), \n                       crs = 4326)\n\n# Inspect the new sf object\nliberia_sf\n\n\n  \n\n\nplot(liberia_sf)\n\n\n\n\n\n\n\n\nThe st_as_sf() function converts the data frame into an sf object. The coords argument specifies which columns contain the spatial coordinates and the crs argument assigns the CRS that in this case we know being WGS84 (EPSG:4326). The sf object can now be used for operations such as spatial joins, distance calculations, and mapping with other spatial layers. Note that the columns containing the spatial coordinates have been replaced by a geometry column, which now stores this information. If you would like to retain the original coordinate columns in the output, you can set the remove argument to FALSE when converting the data frame to an sf object.\n\n\n2.5.6 Working with CRSs in R\nWhen working with data from multiple sources, such as environmental layers, population data, or administrative boundaries, ensuring that all datasets share the same CRS is essential for accurate spatial analysis. This section covers the core tasks involved in managing CRSs in R: checking the CRS of spatial data to ensure datasets are compatible and Reprojecting spatial data into a common CRS when necessary.\n\n2.5.6.1 Checking the CRS of Spatial Data\nBefore performing any spatial operation, it's crucial to check the CRS of your spatial datasets. Knowing whether your data uses geographic coordinates (e.g., WGS84) or a projected coordinate system (e.g., UTM) helps ensure that they are aligned and ready for analysis. Both sf and terra provide functions to retrieve and inspect the CRS, ensuring datasets are spatially aligned before analysis. The st_crs() function retrieves the CRS information for vector data.\n\n# Check the CRS of the vector data\nst_crs(liberia_sf)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nThe same can be achieved for raster data with the crs() function from the terra package.\n\n# Check the CRS of the raster data\ncrs(lbr_pop_100, proj = TRUE, describe = TRUE)\n\n\n  \n\n\n\n\n\n2.5.6.2 Reprojecting spatial data to a common CRS\nIf your datasets have different CRSs or if you want to change CRS (e.g. from geographical to projected) you will need to reproject one or more datasets so they can be spatially aligned. This ensures that they can be overlaid and analyzed together. For vector data, st_transform() reprojects the data into a specified CRS. This example transforms the liberia point data from WGS84 into UTM. To know what’s the correct UTM zone and hence EPSG code for Liberia we can use the get_epsg_utm function from the RiskMap package.\n\n# MOVE THIS INTO RISKMAP\n\nget_epsg_utm &lt;- function(sf_object) {\n  # Ensure the input is an sf object\n  if (!inherits(sf_object, \"sf\")) {\n    stop(\"Input must be an sf object.\")\n  }\n  \n  # Check if the CRS is WGS 84 (EPSG: 4326)\n  crs &lt;- st_crs(sf_object)\n  if (is.null(crs) || crs$epsg != 4326) {\n    stop(\"Input sf object must have a CRS of WGS84 (EPSG: 4326).\")\n  }\n  \n  # Get the coordinates of the centroid of the sf object\n  centroid &lt;- st_centroid(st_union(sf_object)) \n  coords &lt;- st_coordinates(centroid)  \n  \n  # Function to calculate UTM zone based on longitude\n  get_utm_zone &lt;- function(lon) {\n    return((floor((lon + 180) / 6) %% 60) + 1)\n  }\n  \n  # Calculate UTM zone from longitude\n  utm_zone &lt;- get_utm_zone(coords[1])\n  \n  # Determine the EPSG code based on the latitude (north/south)\n  epsg_code &lt;- if (coords[2] &gt;= 0) {\n    32600 + utm_zone   # Northern Hemisphere\n  } else {\n    32700 + utm_zone   # Southern Hemisphere\n  }\n  \n  # Return the calculated EPSG code\n  return(epsg_code)\n}\n\n# Obtain EPSG code for UTM for Liberia\nget_epsg_utm(liberia_sf)\n\n[1] 32629\n\n# Reproject the vector data \nliberia_sf_utm &lt;- st_transform(liberia_sf, crs = get_epsg_utm(liberia_sf))\n\nReprojecting raster data is more complex than reprojecting vector data due to the continuous nature of raster grids. The process involves recalculating cell values to fit a new grid based on the new CRS, which can lead to challenges like resampling, distortion, and data loss. When reprojecting a raster, the grid must adjust to the new CRS, often requiring resampling of cell values. The method you choose depends on the data type: nearest neighbor is best for categorical data like land use while bilinear or cubic interpolation is good for continuous data like temperature, where smooth transitions are needed.\nThe function project() from the terra package can be used to reproject a raster.\n\nlbr_pop_100_utm &lt;- project(lbr_pop_100, \n                           crs(liberia_sf_utm),\n                           method = \"bilinear\")\n\nReprojecting a raster may alter its resolution. For example, reprojecting from geographic (degrees) to projected (meters) CRS can result in a mismatch between the original and new cell sizes. Moreover, distortion can occur when converting between projections, especially at high latitudes. Some cells may be stretched or compressed, leading to potential loss of information or edge artifacts. These distortions arise because the Earth is not flat, and projecting the curved surface of the Earth onto a flat plane (or vice versa) leads to trade-offs. For example, the Mercator projection preserves angles and shapes but distorts area, particularly near the poles.\nFor these reasons, it’s often better to reproject vectors rather than rasters when both data types are used together and avoid as much as possible to change the CRS of raster. One way to achieve this is to work with WGS84 when performing all spatial operations like extraction of covariates from rasters and then transform only the point data to a projected CRS before fitting the model.\n\n\n\n2.5.7 Common Issues and Considerations\n#NOT SURE ABOUT THIS, MAYBE EXPAND OR REMOVE\n\nCRS Mismatch: One of the most common issues in spatial analysis is the mismatch of CRSs between datasets. Always check and reproject datasets if necessary before performing spatial operations like overlays or extractions.\nDatum Transformations: In some cases, especially when working with datasets from different regions, a simple projection transformation may not be enough. You might need to handle datum shifts (e.g., from NAD83 to WGS84). R can handle these transformations, but it's important to be aware of them.\nChoosing the Right CRS: The choice of CRS depends on your study area and the type of analysis. For global studies, WGS84 is often used. For regional studies, UTM or other local projections may be more appropriate to preserve accuracy in distances and areas.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#extracting-covariate-data",
    "href": "02_handling-spatial-data.html#extracting-covariate-data",
    "title": "2  Handling of spatial data in R",
    "section": "2.6 Extracting covariate data",
    "text": "2.6 Extracting covariate data\nIn geostatistical models, the inclusion of relevant covariates (environmental, demographic, or climatic) can potentially enhances predictive accuracy. Covariate data often come from raster or polygon sources, and extracting these values for point locations is essential to link spatial context to point-referenced data. These covariates could include variables like temperature, elevation, land cover, or population density, which influence the spatial distribution of diseases. In this section, we will cover how to extract covariates at point locations from both polygon layers raster layers.\n\n2.6.0.1 Extracting covariates from polygon layers\nPolygon layers contain discrete spatial entities, such as administrative boundaries or land use areas, with associated attributes. Extracting covariates from polygon layers involves associating point data with the attributes of the polygon in which they fall. Here is an example with…..NEED SOME DATA FOR THIS\n\n# Perform a spatial join to transfer polygon attributes to the points\npoints_with_admin &lt;- st_join(liberia_sf, liberia_admin1[\"shapeName\"])\n\n# View the results, points now include covariates from the polygon layer\nhead(points_with_admin)\n\n\n  \n\n\n\nThis spatial join operation adds attributes (in this case the admin1 names) from the polygons to the points. These attributes can now be used in geostatistical models to explain spatial variation in disease risk based on the regions they fall into.\n\n\n2.6.1 Extracting covariates from raster layers\nRaster data provides continuous spatial information, such as elevation, climate data, or population density. Covariate values from raster layers can be extracted for specific points using the extract() function from the terra package. Each point will receive the value of the raster cell it overlaps.\n\n# Extract raster values at the point locations\ncovariate_values &lt;- extract(lbr_pop_100, liberia_sf)\n\n# Combine the extracted values with the point data\nliberia_sf$pop_total &lt;- covariate_values[, 2]\n\n# View the updated dataset\nhead(liberia_sf)\n\n\n  \n\n\n\nIn this example, the extract() function assigns the raster value from the population density layer to each point in the dataset. This allows the point data to include population density as a covariate in the analysis.\nInstead of extracting values for exact point locations, it can sometimes be useful to aggregate covariate values within a defined area around each point. This is often done by creating a buffer around each point and calculating summary statistics (e.g., mean, sum) of the raster values within that buffer. For instance, you might want to calculate the average population density or temperature within a 2 km radius around each point to smooth out fine-scale variation.\n\n# Create buffers around each point (e.g., 2 km radius)\nbuffered_points &lt;- st_buffer(liberia_sf_utm, dist = 1000)  \n\n# Plot the buffers for visualization\nplot(st_geometry(buffered_points), col = \"blue\", border = \"black\")\n\n\n\n\n\n\n\n# Extract raster values within the buffer areas and calculate the \n# mean or sum. Note that since we used the utm data to work on the\n# meter scale we need to convert them back to WGS84\nmean_pop_density &lt;- extract(lbr_pop_100, \n                            st_transform(buffered_points, crs = 4326),\n                            fun = mean, na.rm = TRUE)\n\n# Add the averaged values to the points dataset\nliberia_sf$pop_mean2km &lt;- mean_pop_density[,2]\n\n# View the updated dataset\nhead(liberia_sf)\n\n\n  \n\n\n\nYou can modify the fun argument to calculate other summary statistics, such as the sum, min or max of the raster values within the buffer. This approach is particularly useful when the phenomenon being modeled (e.g., disease transmission) is influenced by broader spatial factors around the observation point, rather than just the value at the exact point location.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#creating-a-predictive-grid",
    "href": "02_handling-spatial-data.html#creating-a-predictive-grid",
    "title": "2  Handling of spatial data in R",
    "section": "2.7 Creating a predictive grid",
    "text": "2.7 Creating a predictive grid\nA predictive grid is a regularly spaced set of points or cells that spans the study region. This grid serves as the basis for predictions made by your model. The density of the grid (i.e., the distance between grid points) affects both the resolution of the prediction and the computational cost. For point-based predictions, we can generate a grid of points over a polygon (e.g., administrative boundary) using the sf package and the st_make_grid function.\n\n# First we convert the Liberia boundaries to the UTM CRS\n# because we want our grid in meters\nliberia_admin0_utm &lt;- liberia_admin0 |&gt; \n  st_transform(crs = get_epsg_utm(liberia_sf))\n\n# Generate prediction grid at 5km resolution\npred_locations &lt;- st_make_grid(liberia_admin0_utm, \n                               cellsize = 5000, \n                               what = \"centers\")\n\n# Exclude locations that fall outside the study area\npred_locations &lt;- st_intersection(pred_locations, liberia_admin0_utm)\n\n# Visualize the result\nplot(liberia_admin0_utm$geometry, col = \"white\")\nplot(pred_locations, cex = .01, col = \"red\", pch = 19, add = T)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  },
  {
    "objectID": "02_handling-spatial-data.html#visualizing-spatial-data",
    "href": "02_handling-spatial-data.html#visualizing-spatial-data",
    "title": "2  Handling of spatial data in R",
    "section": "2.8 Visualizing spatial data",
    "text": "2.8 Visualizing spatial data\nVisualization is a key part of spatial data analysis, as it allows you to explore and communicate spatial patterns and relationships effectively. R provides a lot of functionalities to visualize spatial data and create very beautiful maps. Until now we have used basic plotting functions. Here we introduce the ggplot2 package that allows to combine different types of geographic data in a map. The ggplot2 package in R provides a flexible and powerful framework for creating both simple and complex visualizations, including maps of point data, polygons, and rasters. With the help of extensions like geom_sf() and geom_raster(), ggplot2 makes it easy to visualize spatial data, whether you’re working with point locations, polygons, or continuous raster data.\n\n2.8.1 Visualizing point data\nPoint data often represents the locations of observations (e.g., disease cases, sampling sites). ggplot2 allows you to plot these points and optionally color them by a covariate (e.g., disease prevalence or population density). Here is an example that uses the Liberia data.\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a new variable with prevalence in the dataset\nliberia_sf$prevalence &lt;- liberia$npos / liberia$ntest\n\n# Plot only the locations\nggplot(data = liberia_sf) +\n  geom_sf(col = \"black\") +  \n  theme_minimal() +\n  labs(title = \"Survey locations\")\n# Color the points according to prevalence\nggplot(data = liberia_sf) +\n  geom_sf(aes(color = prevalence)) +  \n  scale_color_viridis_c(labels = scales::label_percent()) +  \n  theme_minimal() +\n  labs(title = \"Onchocerciasis in Liberia\",\n       color = \"Prevalence (%)\")\n\n\n\n\n\n\n\n\n\n\nHere geom_sf() is used to plot the spatial points. aes(color = prevalence) specifies that the points should be colored based on the prevalence covariate, providing a visual representation of spatial variation in disease risk. The scale_color_viridis_c() function applies the Viridis color scale, which is well-suited for continuous data and is friendly for those with color blindness. The labels = scales::label_percent() argument ensures that the color scale's labels are displayed as percentages (e.g., 5, 10%) rather than raw decimal values. To make the plot visually clean and minimal, theme_minimal() is applied, stripping away unnecessary background elements and keeping the focus on the data. Finally, the labs() defines the plot title and the color legend label.\n\n\n2.8.2 Visualizing polygon data\nPolygon data typically represents administrative boundaries, land use, or other regional divisions. We can still use geom_sf() to create maps of polygons, optionally filling them by a covariate.\n# Plot Liberia admin 1 level boundaries \nggplot(data = liberia_admin1) +\n  geom_sf() + \n  theme_minimal() +\n  labs()\n# We compute the area of each polygon\nliberia_admin1$area &lt;- as.numeric(st_area(liberia_admin1) / 1000 ^ 2)\n\n# Color the polygons according tis new variable\nggplot(data = liberia_admin1) +\n  geom_sf(aes(fill = area), color = \"black\") + \n  scale_fill_distiller(direction = -1) +\n  theme_minimal() +\n  labs(fill = \"Area km^2\")\n\n\n\n\n\n\n\n\n\n\nIn this code, aes(fill = area) is used to fill each polygon with colors corresponding to its area. The color = \"black\" argument outlines the polygons in black, and you could set fill = NA to make the polygons transparent while still displaying the borders. The scale_fill_distiller(direction = -1) function applies a color gradient from ColorBrewer, with the direction = -1 argument reversing the gradient (e.g., darker colors for larger areas).\n\n\n2.8.3 Visualizing raster data\nIn ggplot2, you can visualize raster data by converting it into a data frame of coordinates and values. You can convert raster data into a format that ggplot2 can handle by using the as.data.frame() function from terra.\n\n# Convert the raster to a data frame for ggplot2\nraster_df &lt;- as.data.frame(lbr_pop_100, xy = TRUE)\n\n# Plot raster using ggplot2\nggplot(data = raster_df) +\n  geom_raster(aes(x = x, y = y, fill = lbr_ppp_2014)) + \n  scale_fill_viridis_c() +                      \n  theme_minimal() +\n  labs(title = \"Population Density\",\n       fill = \"Density\")\n\n\n\n\n\n\n\n\nIn this example as.data.frame() converts the raster into a data frame with x and y coordinates and their corresponding raster values and geom_raster() is used to plot the raster cells, coloring them based on the population density.\n\n\n2.8.4 Combining Multiple Spatial Data Types\nIn many cases, it's useful to combine different spatial data types (points, polygons, and rasters) in a single visualization. ggplot2 allows you to overlay these layers, providing a more comprehensive view of your spatial data.\n\n# Combine points, polygons, and raster data in one plot\ncombined_map &lt;- ggplot() +\n  geom_raster(data = raster_df, aes(x = x, y = y, fill = lbr_ppp_2014)) + \n  geom_sf(data = liberia_admin1, fill = NA, color = \"grey\") +    \n  geom_sf(data = liberia_sf, shape = 21, col = \"black\", fill = \"white\") +\n  scale_fill_viridis_c() +  \n  theme_minimal() +\n  labs(title = \"Combined Spatial Data: Points, Polygons, and Raster\",\n       fill = \"Population Density\",\n       x = \"\", y = \"\")\n\ncombined_map\n\n\n\n\n\n\n\n\nTo enhance spatial visualizations in ggplot2, adding a scale bar and a north arrow improves map readability and professionalism. The ggspatial package offers tools to easily integrate these elements into your maps. Below is an example that demonstrates how to use ggspatial to add a scale bar and north arrow to a map that includes raster, polygon, and point data.\n\n# Load necessary libraries\nlibrary(ggspatial)\n\n# Add scale bar and north arrow\ncombined_map +\n  annotation_scale(location = \"bl\", width_hint = 0.5) +  \n  annotation_north_arrow(location = \"tr\", which_north = \"true\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Handling of spatial data in R</span>"
    ]
  }
]