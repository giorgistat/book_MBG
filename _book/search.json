[
  {
    "objectID": "03_model-fitting.html#list-of-the-main-functions-used-in-the-chapter",
    "href": "03_model-fitting.html#list-of-the-main-functions-used-in-the-chapter",
    "title": "3  Model formulation and parameter estimation",
    "section": "List of the main functions used in the chapter",
    "text": "List of the main functions used in the chapter\n\n\n\n\n\n\n\n\nFunction\nR Package\nUsed for\n\n\n\n\nlmer\nlme4\nFitting linear mixed models\n\n\nglmer\nlme4\nFitting generalized linear mixed models\n\n\nglgm\nRiskMap\nFitting generalized linear mixed models\n\n\ns_variogram\nRiskMap\nComputing the empirical variogram and carrying out permutation test for spatial independence"
  },
  {
    "objectID": "03_model-fitting.html#exploratory-analysis",
    "href": "03_model-fitting.html#exploratory-analysis",
    "title": "3  Model formulation and parameter estimation",
    "section": "3.1 Exploratory analysis",
    "text": "3.1 Exploratory analysis\nAs illustrated in Figure 1.8, exploratory analysis is the first step that should be carried out in a statistical analysis. This stage is essential to inform how covariates should be introduced in the model and, in our case, whether the variation unexplained by those covariates exhibits spatial correlation.\nIn the exploratory analysis of count data, we will also look at how overdispersion, which is a necessary, though not sufficient, condition for residual spatial correlation.\n\n3.1.1 Exploring associations with risk factors using count data\nAssessment of the association between the health outcome of interest and non-categorical (i.e. continuous) risk factors can be carried using graphical tools, such scatter plots. The graphical inspection of the empirical association between the outcome and the covariates is especially useful to identify non-linear patterns in the relationship which should then be accounted for in the model formulation.\nIn this section, we look more closely at the case when the observed outcome is a count which requires a different treatment from continuously measured outcomes, which are generally covered by most statistics textbooks (see, for example, Chapter 1 of Weisberg (2014)).\n\n3.1.1.1 When the outcome is an aggregated count\nLet us first consider the example of the river-blindness data in Liberia (Section 1.4.2), and examine the association between prevalence and elevation. We first generate a plot of the prevalence against the measured elevation at each of the sample locations\n\nliberia$prev &lt;- liberia$npos/liberia$ntest\n\nggplot(liberia, aes(x = elevation, y = prev)) + geom_point() +\n  labs(x=\"Elevation (meters)\",y=\"Prevalence\")\n\n\n\n\nFigure 3.1: Scatter plot of the empirical prevalence for river-blindess against elevation, measured in meters.\n\n\n\n\nThe plot shown in Figure 3.1 shows that, as elevation increases from 0 to around 150 meters, prevalence rapidly increases to around 0.25 and, for larger values in elevation than 150 meters, the relationship levels off. This begs the question of how we can account for this in a regression model. To answer this question rigorously, however, the plot in Figure 3.1 cannot be used. This is because, when the modelled outcome is a bounded Binomial count, regression relationships are specified on the logit-transformed prevalence (log-odds) scale; see Table 1.3 in Section Section 1.5 . To explore regression relationships in the case of prevalence data, it is convenient to use the so-called empirical logit in place of the empirical prevalence. The empirical logit is defined as\n\\[\nl_{i} = \\log\\left\\{\\frac{y_i + 1/2}{n_i - y_i + 1/2}\\right\\}\n\\tag{3.1}\\]\nwhere \\(y_i\\) are the number of individuals who tested positive for riverblindness and \\(n_i\\) is the total number of people tested at a location. The reason for using the empirical logit, rather than the standard logit transformation applied directly to the empirical prevalence, is that it allows to generate finite values for empirical prevalence values of 0 and 1, for which the standard logit transformation is not defined.\n\n# The empirical logit\nliberia$elogit &lt;- log((liberia$npos+0.5)/\n                      (liberia$ntest-liberia$npos+0.5))\n\nggplot(liberia, aes(x = elevation, y = elogit)) + geom_point() +\n  \n  # Adding a smoothing spline\n  labs(x=\"Elevation (meters)\",y=\"Empirical logit\") +\n  stat_smooth(method = \"gam\", formula = y ~ s(x),se=FALSE)+\n  \n  # Adding linear regression fit with log-transformed elevation\n  stat_smooth(method = \"lm\", formula = y ~ log(x),\n              col=\"green\",lty=\"dashed\",se=FALSE) +\n\n  # Adding linear regression fit with change point in 150 meters\n  stat_smooth(method = \"lm\", formula = y ~ x + pmax(x-150, 0),\n              col=\"red\",lty=\"dashed\",se=FALSE) \n  \n\n\n\n\nFigure 3.2: Scatter plot of the empirical prevalence for river-blindess against elevation, measured in meters.\n\n\n\n\nFigure 3.2 shows the scatter plot of the empirical logit against elevation. In this plot, we have also added three lines though the stat_smooth from the ggplot2 package. Using this function, we first pass the term gam to method to add a penalized smoothing spline (Hastie, Tibshirani, and Friedman 2001), represented by the blue solid line. The smoothing spline allows us to better discern how the type of relationship and how to best capture it using a standard regression approach. As er can see from Figure 3.2, the smoothing spline corroborates our initial observation of a positive relationship up to about 150 meters, followed by a plateau.\nTo capture this non-linear relationship, we can use the two following approaches. The first is based on a simple log-transformation of elevation and is represented in Figure 3.2 by the green line. If were to express this relationship using a standard Binomial regression model, this would take the form \\[\n\\log\\left\\{\\frac{p(x_i)}{1-p(x_i)}\\right\\} = \\beta_0 + \\beta_1 \\log\\{e(x_i)\\}\n\\tag{3.2}\\] where \\(p(x_i)\\) and \\(e(x_i)\\) are the river-blindness prevalence and elevation at sampled location \\(x_i\\), respectively.\nAlternatively, the non-linear effect of elevation on prevalence could be captured using a linear spline. Put in simple terms, we want to fit a linear regression model that allows for a change in slope above 150 meters. Formally, this is expressed in a Binomial regression model as \\[\n\\log\\left\\{\\frac{p(x_i)}{1-p(x_i)}\\right\\} = \\beta_0 + \\beta_1 e(x_i) + \\beta_{2} \\max\\{e(x_i)-150, 0\\}.\n\\tag{3.3}\\] Based on the equation above, the effect of elevation below 150 meters is quantified by the parameter \\(\\beta_1\\). Above 150 meters, instead, the effect of elevation becomes \\(\\beta_1 + \\beta_2\\). Note that the function pmax (and not the standard base function max) should be used in R when the computation of the maximum between a scalar value and each of the components of a numeric vector is required.\nBefore proceeding further, it is important to explain the differences between the use of the logarithmic transformation (Equation 3.2) and the linear spline (Equation 3.3). We observe that both curves provide a similar fit to the data, with larger differences observed for larger values in elevation, where the log-transformed elevation models yields larger values for the predicted prevalence. This also suggests that if we were to extrapolate the predictions beyond 600 meters in elevation the implied pattern by the model with the log-transformed elevation would predict an increasingly larger elevation, which is unrealistic, since the fly that transmits the diseases cannot breed at those altitudes. The linear spline model instead would generate predictions that would be very similar to those observed between 150 and 600 meters. From this point view, the linear spline model would thus have more scientific validity than the other model. However, which of the two approaches should be chosen to model the effect of elevation is a question that closely depends on the research question to be addressed.\nIf the interest of the study was in better understanding the association between elevation and prevalence, the linear spline model does not only provide a more credible explanation but also its regression parameters can be more easily interpreted. In fact, for a unit increase in elevation, the multiplicative change in the odds for river-blindness is \\(\\exp\\{\\beta_1\\}\\), if elevation is below 150 meters, and \\(\\exp\\{\\beta_1+\\beta_2\\}\\), if elevation is above 150 meters. When instead we use the log-transformed elevation, the interpretation of \\(\\beta_1\\) in Equation 3.2 is slightly more complicated, as it is based on the multiplicative increase in elevation by the same amount given by the base of the algorithm, which is about \\(e \\approx 2.718\\)1. To avoid this, one could rescale the regression coefficient as, for example, \\(\\beta_1/\\log_{2}(e)\\) which would be interpreted as the multiplicative change in the odds for river-blindness for a doubling in elevation. However, a doubling in elevation is less meaningful when considering larger values of elevation.\nWhen the goal of statistical analysis is instead in developing a predictive model for the outcome of interest, the explanatory power and interpretability of the model may be of less concern. For this reason, the model with the log-transformed model could be preferred over the model with the linear spline, if it shown to yield more predictive power. We will come back to this point again in Chapter 5, where will show how to assess and compare the predictive performance of different geostatistical models.\nThe other type of aggregated count data that we consider are unbounded counts. The Anopheles mosquitoes data-set (Section 1.4.4) is an example of this, since there is no upper limit to the number of mosquitoes that can be trapped at a location. Let us consider the covariate represented by elevation. In this case, the simplest model that can be used to analyse the data is a Poisson regression, where the linear predictor is defined on the log of the mean number of mosquitoes (Table 1.3). Hence, exploratory plots for the association with covariates should be generated using the log transformed counts of mosquitoes. In this instance, to avoid taking the log of zero, we can add 1 to the reported counts, if required. The variable of the An.gambiae in the anopheles data-set does not contain any 0, hence we simply apply the log tranformation without adding 1.\n\nanopheles$log_counts &lt;- log(anopheles$An.gambiae)\nggplot(anopheles, aes(x = elevation, y = log_counts)) + geom_point() +\n  \n  # Adding a smoothing spline\n  labs(x=\"Elevation (meters)\",y=\"Log number of An. gambiae mosquitoes\") +\n  stat_smooth(method = \"lm\", formula = y ~ x, se=FALSE)\n\n\n\n\nFigure 3.3: Scatter plot of the log tranformed number of Anopheles gambiae mosquitoes against elevation, measured in meters. The blue line is generated using the least squares fit.\n\n\n\n\nThe scatter plot of Figure 3.3 shows that there is a negative, though weak, association, with the average number of mosquitoes decreasing for increasing elevation. In this instance, the assumption of a linear relationship with elevation would be a reasonable choice.\n\n\n3.1.1.2 When the outcome is an invidual-level binary indicator\nWe now consider the malaria data from Kenya (Section 1.4.3) where the main outcome is the result from a rapid diagnostic test (RDT) for malaria from individuals within households. In this case, because the outcome only takes two values, 1 for a positive RDT test result and 0 otherwise, the direct application of the empirical logit from Equation 3.1 would not help us to generate informative scatter plots. Throughout the book, we will consider the data from the community survey only, hence we work with a subset of the data which we shall name malkenya_comm\n\nmalkenya_comm &lt;- malkenya[malkenya$Survey==\"community\", ]\n\nTo show how this issue can be overcome, let us consider the variables age and gender. To generate a plot that can help us understand between the relationship with malaria prevalence and the two risk factors, we proceed as follows.\n\n# Grouping of ages into classes defined through \"breaks\"\nmalkenya_comm$Age_class &lt;- cut(malkenya_comm$Age, \n                            breaks = c(0, 5, 10, 15, 30, 40, 50, 100),\n                            include.lowest = TRUE)\n\nUsing the cut function, we first split age (in years) into classes through the argument breaks. The classification of age into \\([0,5]\\), \\((5, 10]\\) and \\((10, 15]\\) is common in many malaria epidemiology studies, as children are one of the groups at highest risk malaria. The choice of the other classes of age reflects instead the need to balance the number of observations falling in each of the classes.\n\n# Computation of the empirical logit by age groups and gender\nage_class_data &lt;- aggregate(RDT ~ Age_class + Gender, \n                                    data = malkenya_comm, \n                                    FUN = function(y) \n                                    log((sum(y)+0.5)/(length(y)-sum(y)+0.5)))\n\nWe then compute the empirical logit, using the total number of cases within age group and by gender. For a given age group and gender, which we denote as \\(\\mathcal{C}\\), the empirical logit in Equation 3.1, now takes the form \\[\nl_{\\mathcal{C}} = \\log\\left\\{\\frac{\\sum_{i \\in \\mathcal{C}} y_{i} + 0.5}{|\\mathcal{C}|- \\sum_{i \\in \\mathcal{C}} y_{i} + 0.5} \\right\\}\n\\tag{3.4}\\] where \\(y_i\\) are the individual binary outcomes and \\(i\\in \\mathcal{C}\\) is used to indicate that the sum is carried out over all the individuals who belong the class \\(\\mathcal{C}\\), identified by a specific age group and gender. Finally, \\(|\\mathcal{C}|\\) is the number of individuals who fall within \\(\\mathcal{C}\\). In the code above, the empirical logit in Equation 3.4 is computed using the aggregate function. An inspection of the object age_class_data, a data frame, shows that the empirical is found in the column named RDT.\n\n# Computation of the average age within each age group \nage_class_data$age_mean_point &lt;- aggregate(Age ~ Age_class + Gender, \n                                 data = malkenya_comm, \n                                 FUN = mean)$Age\n\n\n# Number of individuals within each age group, by gender\nage_class_data$n_obs &lt;-  aggregate(Age ~ Age_class + Gender, \n                         data = malkenya_comm, \n                         FUN = length)$Age\n\nIn order to generate the scatter-plot, we compute the average age within each age group by gender, and use these as our values for the x-axis. Note that since we only need to obtain the average age from this output, we use $Age to extract this only and allocate to the column age_mean_point. Finally, we also compute the number of observations within each of classes and place this in n_obs.\n\nggplot(age_class_data, aes(x = age_mean_point, y = RDT, \n                           size = n_obs, \n                           colour = Gender)) + \n  geom_point() + \n  labs(x=\"Age (years)\",y=\"Empirical logit\")  \n\n\n\n\nFigure 3.4: Plot of the empirical logit against age, for males and females. The size of each solid point is rendered proportional to the number of individuals within age group, as indicated in the legend.\n\n\n\n\nThe resulting plot in Figure 3.4 shows the empirical logit against age by gender, with the size of each of the points proportional to the number of observations falling within each class. The observed patterns are explained by the fact that young children, especially those under the age of five, are particularly vulnerable to severe malaria infections. This is primarily due to their immature immune systems and lack of acquired immunity. As individuals grow older, they generally develop partial immunity to malaria through repeated exposure to the disease. This acquired immunity can provide some level of protection against severe malaria. At the same time, gender roles and activities can influence exposure to malaria-carrying mosquitoes. For example, men may spend more time outdoors for work or other activities, increasing their exposure to mosquito bites and thus their risk of infection. In addition, there are also biological factors to consider. Hormonal and genetic differences between males and females may also contribute to variations in immune responses to malaria infection. The interaction between age and gender is complex and may vary depending on the specific context and population being studied. A 2020 report from the Bill & Melinda Gates foundation provides a detailed overview of this and other aspects related to gender and malaria (Katz and Bill & Melinda Gates Foundation 2020).\nTo account for age in a model for malaria prevalence, several approaches are possible, some of which have been developed using biological models (Smith et al. 2007). To model the patterns observed in Figure 3.4, we can follow the same approach used in the previous section to model the relationship between elevation and river-blindness prevalence. First, let us consider age without the effect of gender. Let \\(p_{j}(x_i)\\) denote the probability of a positive RDT for the \\(j\\)-th individual living in a household at location \\(x_i\\). Assuming that malaria risk reaches its peak at 15 years of age, we can capture the non-linear relationship using a linear spline with two knots, one at 15 years and a second one at 40 years. This is expressed as \\[\n\\begin{aligned}\n\\log\\left\\{\\frac{p_{j}(x_i)}{1-p_j(x_i)}\\right\\} = \\beta_{0} + \\beta_{1}a_{ij}+\\beta_{2} \\times\\max\\{a_{ij}-15, 0\\} + \\beta_{3}\\max\\{a_{ij}-40, 0\\}\n\\end{aligned}\n\\tag{3.5}\\] where \\(a_{ij}\\) is the age, in years, for the \\(j\\)-th individual at household \\(i\\). Based on this model the effect of age on RDT prevalence is \\(\\beta_{1}\\), for \\(a_{ij} &lt; 15\\), \\(\\beta_{1}+\\beta_{2}\\), for \\(15 &lt; a_{ij} &lt; 40\\), and \\(\\beta_{1}+\\beta_{2}+\\beta_{3}\\) for \\(a_{ij} &gt; 40\\).\nFigure 3.4 indicates that age may interact with gender, meaning that the effect of gender on RDT prevalence changes across age, with larger differences observed between males and females for ages above 20 years. To assess such differences using a standard Binomial regression model, the linear predictor for RDT prevalence can be formulated as \\[\n\\begin{aligned}\n\\log\\left\\{\\frac{p_{j}(x_i)}{1-p_j(x_i)}\\right\\} = \\beta_{0} + (\\beta_{1} + \\beta_{1}^*g_{ij})\\times a_{ij}+(\\beta_{2} + \\beta_{2}^*g_{ij})\\times\\max\\{a_{ij}-15, 0\\} + \\\\\n(\\beta_{3} + \\beta_{3}^*g_{ij}) \\times \\max\\{a_{ij}-40, 0\\}\n\\end{aligned}\n\\tag{3.6}\\] where \\(g_{ij}\\) is the indicator for gender, with 1 corresponding to male and 0 to female. The coefficients \\(\\beta_{1}^*\\), \\(\\beta_{2}^*\\) and \\(\\beta_{3}^*\\) thus quantify the differences in risk between the two genders for ages below 15 years, betwee 15 and 40 years, and above 40 years, respectively. If all of those coefficients were 0, the model in Equation 3.5 would be recovered.\n\nglm_age_gender_interaction &lt;- glm(RDT ~ Age + Gender:Age + \n                                  pmax(Age-15, 0) + Gender:pmax(Age-15, 0) + \n                                  pmax(Age-40, 0) + Gender:pmax(Age-40, 0),\n                              data = malkenya_comm, family = binomial)\n\nsummary(glm_age_gender_interaction)\n## \n## Call:\n## glm(formula = RDT ~ Age + Gender:Age + pmax(Age - 15, 0) + Gender:pmax(Age - \n##     15, 0) + pmax(Age - 40, 0) + Gender:pmax(Age - 40, 0), family = binomial, \n##     data = malkenya_comm)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -0.7681  -0.7051  -0.4940  -0.2734   2.7294  \n## \n## Coefficients:\n##                              Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)                  -1.05835    0.10245 -10.331  &lt; 2e-16 ***\n## Age                          -0.03384    0.01310  -2.584  0.00978 ** \n## pmax(Age - 15, 0)            -0.03975    0.02356  -1.687  0.09162 .  \n## pmax(Age - 40, 0)             0.09170    0.02482   3.695  0.00022 ***\n## Age:GenderMale                0.01428    0.01221   1.170  0.24202    \n## GenderMale:pmax(Age - 15, 0) -0.03625    0.03145  -1.153  0.24908    \n## GenderMale:pmax(Age - 40, 0)  0.02451    0.04320   0.567  0.57052    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 2875.8  on 3351  degrees of freedom\n## Residual deviance: 2673.8  on 3345  degrees of freedom\n## AIC: 2687.8\n## \n## Number of Fisher Scoring iterations: 5\n\nThe code above shows how to fit the model specified in Equation 3.6. The terms Age, pmax(Age-15, 0) and pmax(Age-40, 0) respectively correspond to \\(\\beta_{1}\\), \\(\\beta_{2}\\) and \\(\\beta_{3}\\), whilst the Gender:Age, Gender:pmax(Age-15, 0) and Gender:pmax(Age-40, 0) to \\(\\beta_{1}^*\\), \\(\\beta_{2}^*\\) and \\(\\beta_{3}^*\\), respectively. In the summary of the fitted model, we observe that the interaction coefficients are non-statistically significant. However, removing the interaction based on the fact that each of the coefficients have each p-values larger than the conventional level of 5% would be wrong. Instead we should carry out the likelihood ratio test, as shown below.\n\nglm_age_gender_no_interaction &lt;- glm(RDT ~ Age +  pmax(Age-15, 0) + pmax(Age-40, 0),\n                              data = malkenya_comm, family = binomial)\n\nanova(glm_age_gender_no_interaction, glm_age_gender_interaction, test = \"Chisq\")\n## Analysis of Deviance Table\n## \n## Model 1: RDT ~ Age + pmax(Age - 15, 0) + pmax(Age - 40, 0)\n## Model 2: RDT ~ Age + Gender:Age + pmax(Age - 15, 0) + Gender:pmax(Age - \n##     15, 0) + pmax(Age - 40, 0) + Gender:pmax(Age - 40, 0)\n##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n## 1      3348     2675.6                     \n## 2      3345     2673.8  3   1.8051   0.6138\n\nTo carry out the likelihood ratio test to assess the null hypothesis that \\(\\beta_{1}^*=\\beta_{2}^*=\\beta_{3}^*=0\\), we first fit the simplified nested model under this null hypothesis. The likelihood ratio test can then be carried out using the anova command as shown. The p-value indicates that the we do not find evidence against the null hypothesis, hence in our analysis of the data we might favour the simplified model that does not assumes an interaction between the two genders.\nThe approach just illustrated, can also be applied to explore the association with other continuous variables that are a property of the household and not of the individual. Let us, for example, consider the variable elevation from the malkenya data-set.\n\nmalkenya_comm$elevation_class &lt;- cut(malkenya_comm$elevation,\n                            breaks = quantile(malkenya_comm$elevation, seq(0, 1, by = 0.1)),\n                            include.lowest = TRUE)\n\nFollowing the same approach used for age, we first split elevation into classes. To define these, we use the deciles of the empirical distribution of elevation which we calculate using the quantile function above. In this way we also ensure that the number of observations falling within each class of elevation is approximately the same.\n\n# Computation of the empirical logit by classes of elevation\nelev_class_data &lt;- aggregate(RDT ~ elevation_class, \n                                    data = malkenya_comm, \n                                    FUN = function(y) \n                                    log((sum(y)+0.5)/(length(y)-sum(y)+0.5)))\n\n\n# Computation of the average elevation within each class of elevation\nelev_class_data$elevation_mean &lt;- aggregate(elevation ~ elevation_class, \n                                    data = malkenya_comm, \n                                    FUN = mean)$elevation\n\nWe then compute the empirical logit and the average elevation for each class of elevation. The empirical logit is computed as already defined in Equation 3.4, where now the definition of \\(\\mathcal{C}\\) is given by a specific decile used to split the distribution of elevation.\n\nggplot(elev_class_data, aes(x = elevation_mean, y = RDT), \n                           size = n_obs) + \n  geom_point() + \n  labs(x=\"Elevation (meters)\",y=\"Empirical logit\")  \n\n\n\n\nFigure 3.5: Plot of the empirical logit against elevation measured in meters.\n\n\n\n\nThe resulting plot in Figure 3.5 shows an approximately linear relationship with decreasing values of the empirical logit for increasing elevation. This is expected because the cooler environment at higher altitudes is less favourable to the development of the overall mosquito life cycle.\nAn alternative approach to generate a scatter plot for assessing the association between elevation and the empirical logit would be to aggregate the data at household level, rather than using classes of elevation. However, this approach does not work as the one illustrated above when only one individual is sampled for each location. In the case of the malkenya data, the great majority of the locations only include one individual making this second approach less useful than the one illustrated.\nOther more sophisticated approaches for the exploration of the associations between covariates and binary outcomes are available. For example, the use of the empirical logit could be avoided by using non-parametric regression methods for Binomial outcomes (Bowman 1997), also implemented in sm package in R. Our view is that a careful exploratory analysis based on simpler methods, as those illustrated above, can be equally effective to inform the module formulation.\n\n\n\n3.1.2 Exploring overdispersion in count data\nOne of the main advantages in the use of covariates is the ability to attribute part of the variation of the outcome to a set of measured variables and, hence, reduce the uncertainty of our inferences. However, it almost always the case that the finite number of covariates at our disposal is not enough to fully explain the variation in the outcome. In other words, the existence of unmeasured covariates that are related to the modelled outcome give rise to the so called residual variation. In a standard linear regression model the extent to which we are able to account for important covariates is directly linked to the size of the variance of the residuals. In the case of count data, instead, this link is less well defined and one of the main consequences of the omission of covariates, which we address in this chapter, is overdispersion.\nOverdispersion occurs when the variability of the data is larger than that implied by the generalized linear model (GLM) fitted to them. For example, if we consider the Binomial distribution, the presence of overdispersion implies that \\(V(Y_i) &gt; n_i \\mu_{i}(1-\\mu_i)\\), where we recall that \\(n_i\\) is the Binomial denominator and \\(mu_i\\) is the probability of “success” for each of the \\(n_i\\) Bernoulli trials; for a Poisson distribution with \\(E(Y_i) = \\mu_i\\), instead, overdispersion implies that \\(V(Y_i) &gt; \\mu_{i}\\).\nAssessment of the overdispersion for count data can be carried out in different ways depending on the goal of the statistical analysis. Since the focus of this book is to illustrate how to formulate and apply geostatistical models, the most natural approach to assess overdispersion is through the use of generalized linear mixed models (GLMMs). The class of GLMMs that we consider in this and the next section are obtained by replacing the spatial Gaussian process \\(S(x_i)\\) in introduced in Equation 1.4 with a set of mutually independent random effects, which we denote as \\(Z_i\\), and thus write \\[\ng(\\mu_i) = d(x_i)^\\top \\beta + Z_i.\n\\tag{3.7}\\] The model above accounts for the overdispersion in the data through \\(Z_i\\) whose variance can be interpreted as an indicator of the amount of overdispersion. To show this, we carry out a small simulation as follows. For simplicity, we consider the Binomial mixed model with an intercept only, hence \\[\n\\log\\left\\{\\frac{\\mu_i}{1-\\mu_i}\\right\\} = \\beta_0 + Z_i\n\\tag{3.8}\\] and assume that the \\(Z_i\\) follow a set of mutually independent Gaussian variables with mean 0 and variance \\(\\tau^2\\). In our simulation we vary \\(\\beta_0\\) over the set \\(\\{-3, -2, -1, 0, 1, 2, 3\\}\\) and set \\(\\tau^2=0.1\\) and the binomial denominators to \\(n_i = 100\\). For a given value of \\(\\beta_0\\), we then proceed through the following iterative steps.\n\nSimulate 10,000 values for \\(Z_i\\) from a Gaussian distribution with mean 0 and variance \\(\\tau^2\\).\nCompute the probabilities \\(\\mu_i\\) based on Equation 3.8.\nSimulate 10,000 values from a Binomial model with probability of success \\(\\mu_i\\) and denominator \\(n_i\\).\nCompute the empirical variance of the counts \\(y_i\\) simulated in the previous step.\nChange the value of \\(\\beta_0\\) and repeat the previous steps, for all the values of \\(\\beta_0\\).\n\nThe code below shows the implementation of the above steps in R.\n\n# Number of simulations\nn_sim &lt;- 10000\n\n# Variance of the Z_i\ntau2 &lt;- 0.1\n\n# Binomial denominator \nbin_denom &lt;- 100\n\n# Intercept values\nbeta0 &lt;- c(-3, -2, -1, 0, 1, 2, 3)\n\n# Vector where we store the computed variance from\n# the simulated counts from the Binomial mixed model\nvar_data &lt;- rep(NA, length(beta0))\n\n\nfor(j in 1:length(beta0)) {\n  # Simulation of the random effects Z_i\n  Z_i_sim &lt;- rnorm(n_sim, sd = sqrt(tau2))\n\n  # Linear predictor of the Binomial mixed model\n  lp &lt;- beta0[j]  + Z_i_sim\n  \n  # Probabilities of the Binomial distribution conditional on Z_i\n  prob_sim &lt;- exp(lp)/(1+exp(lp))\n  \n  # Simulation of the counts from the Binomial mixed model\n  y_i_sim &lt;- rbinom(n_sim, size = bin_denom, prob = prob_sim)\n  \n  # Empirical variance from the simulated counts\n  var_data[j] &lt;- var(y_i_sim)\n}\n\n# Probabilities from the standard Binomial model (Z_i = 0)\nprobs_binomial &lt;- exp(beta0)/(1+exp(beta0))\n\n# Variance from the standard Binomial model\nvar_bimomial &lt;- bin_denom*probs_binomial*(1-probs_binomial)\n\n\nmatplot(beta0, cbind(var_data, var_bimomial), type = \"b\", pch = 20,\n        lty = \"solid\", ylab = \"Variance\", xlab = expression(beta[0]))\nlegend(-3, 80, c(\"Binomial mixed model\", \"Standard Binomial model\"),\n       col=1:2, lty = \"solid\", cex = 0.75)\n\n\n\n\nFigure 3.6: Plot of the variances of the standard Binomial model and the Binomial mixed model (see Equation 3.8) against \\(\\beta_0\\)\n\n\n\n\nFigure 3.6 shows the results of the simulation. In this figure, the red line corresponds to the variance of a standard Binomial model, obtained by setting \\(Z_i=0\\) and computed as \\(n_i \\mu_i (1-\\mu_i)\\) with \\(\\mu_i = \\exp\\{\\beta_0\\}/(1+\\exp\\{\\beta_0\\})\\). As expected, this plot shows that the variance of the simulated counts from the mixed model in Equation 3.8 exhibit a larger variance than would be expected under the standard Binomial model. It also indicates that the chosen value for the variance of \\(Z_i\\) of \\(\\tau^2 = 0.1\\) corresponds to a significant amount of dispersion. One way to relate \\(\\tau^2\\) to the amount of overdispersion is by considering that, following from the properties of a univariate Gaussian distribution, a priori the \\(Z_i\\) will take values between \\(-1.96 \\sqrt{\\tau^2}\\) and \\(+1.96 \\sqrt{\\tau^2}\\) with approximately 95\\(\\%\\) probability. That implies that \\(\\exp\\{Z_i\\}\\), which expresses the effect of the random effects on the odds ratios, will be with 95\\(\\%\\) probability between \\(\\exp\\{-1.96 \\sqrt{\\tau^2}\\}\\) and \\(\\exp\\{+1.96 \\sqrt{\\tau^2}\\}\\). By replacing \\(\\tau^2\\) with the chosen values for the simulation, those two becomes about 0.54 and 1.86, meaning that with the \\(Z_i\\) with \\(95\\%\\) probability will have a multiplicative effect on the odds ratios between \\(0.54\\) and \\(1.86\\).\nWe encourage you to do Exercise 1 and Exercise 2 at the end of this chapter, to further explore how generalized linear mixed models can be used as a tool to account for overdispersion.\n\n3.1.2.1 Maximum likelihood estimation of generalized linear mixed models for count data\nWe now illustrate how to fit a generalize linear mixed, using the anopheles data-set as an example. We consider two models: an intercept-only model and one that uses elevation as a covariate. Let \\(\\mu(x_i)\\) be the number of mosquitoes captured at a location \\(x_i\\); then the linear predictor with elevation as a covariate takes the form \\[\n\\log\\{\\mu_i\\} = \\beta_{0} + \\beta_{1} d(x_i) + Z_i\n\\tag{3.9}\\] where \\(d(x_i)\\) indicates the elevation in meters at location \\(x_i\\) and the \\(Z_i\\) are independent and identically distributed Gaussian variables with mean 0 and variance \\(\\tau^2\\). The model with an intercept only is simply obtained by setting \\(\\beta_1 = 0\\).\nWe carry out the estimation in R using the glmer function from the lme4 package (see Bates et al. (2015) for a detailed tutorial). The glmer function implements the maximum likelihood estimation for generalized linear mixed models. The code below shows how the glmer is used to carry out this step for the model in Equation 3.9 and the one withuot covariates.\n\n# Create the ID of the location\nanopheles$ID_loc &lt;- 1:nrow(anopheles)\n\n# Poisson mixed model with elevation\nfit_glmer_elev &lt;- glmer(An.gambiae ~ scale(elevation) + (1|ID_loc), family = poisson, \n                        data = anopheles, nAGQ = 25)\n\n# Poisson mixed model with intercept only\nfit_glmer_int &lt;- glmer(An.gambiae ~ (1|ID_loc), family = poisson, \n                        data = anopheles, nAGQ = 25)\n\nTo fit the model with glmer, we first must create a variable in our data-set that allows us to identify the location associated with each count. In this case, since every row corresponds to a different location, we simply use the row number to identify the locations and save this in the ID_loc variable. The random effects \\(Z_i\\) are then included in the model by adding (1 | ID_loc) in the formula of the glmer function.\nWhen introducing the variable elevation, we standardize the variable so that its mean is 0 and its variance is 1. This is done to aid the convergence of the algorithm used to fit the model and it is generally considered good practice, especially when many variables with different scales are used as covariates. However, we emphasize that standardizing a variable does not affect the fit of the model to the data. This is because the model with the standardized variable is a reparametrization of the model with the unstandardized variable. In other words, a model that uses standardized covariates only attaches a different interpretation to its regression coefficients while maintaining the same goodness of fit of the model with that uses the covariates on their original scale.\nThe argument nAGQ is used to define the precision of the approximation of the maximum likelihood estimation algorithm. By default nAGQ = 1, which corresponds to the Laplace approximation. Values for nAGQ larger than 1 are used to define the number of points of the adaptive Gaussian-Hermite quadrature. The general principle is that the larger nAGQ the better, but at the expense of an increased computing time. Based on the guidelines and help pages of the lme4 package, it is stated that a reasonable value for nAGQ is 25. For more technical details on this aspect, we refer the reader to Bates et al. (2015).\nWe can now look at the summary of the fitted models to the mosquitoes data-set.\n\n### Summary of the model with elevation\nsummary(fit_glmer_elev)\n## Generalized linear mixed model fit by maximum likelihood (Adaptive\n##   Gauss-Hermite Quadrature, nAGQ = 25) [glmerMod]\n##  Family: poisson  ( log )\n## Formula: An.gambiae ~ scale(elevation) + (1 | ID_loc)\n##    Data: anopheles\n## \n##      AIC      BIC   logLik deviance df.resid \n##    291.8    300.1   -142.9    285.8      113 \n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -0.89574 -0.42469 -0.09483  0.29445  0.53352 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  ID_loc (Intercept) 0.7146   0.8453  \n## Number of obs: 116, groups:  ID_loc, 116\n## \n## Fixed effects:\n##                  Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)       1.53042    0.09365  16.342   &lt;2e-16 ***\n## scale(elevation) -0.19794    0.08950  -2.212    0.027 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Correlation of Fixed Effects:\n##             (Intr)\n## scale(lvtn) 0.036\n\n### Summary of the model with the intercept only\nsummary(fit_glmer_int)\n## Generalized linear mixed model fit by maximum likelihood (Adaptive\n##   Gauss-Hermite Quadrature, nAGQ = 25) [glmerMod]\n##  Family: poisson  ( log )\n## Formula: An.gambiae ~ (1 | ID_loc)\n##    Data: anopheles\n## \n##      AIC      BIC   logLik deviance df.resid \n##    294.6    300.1   -145.3    290.6      114 \n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -0.73816 -0.42718 -0.06941  0.26564  0.45022 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  ID_loc (Intercept) 0.761    0.8724  \n## Number of obs: 116, groups:  ID_loc, 116\n## \n## Fixed effects:\n##             Estimate Std. Error z value Pr(&gt;|z|)    \n## (Intercept)  1.52849    0.09584   15.95   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFrom the summary of the model that uses elevation, we observe the that the estimated regression coefficient \\(\\beta_{1}\\) is statistically significant different from 0. The interpretation of the estimated regression coefficient is the following: for an increase of about 100 meters in elevation, all other things being equal, the average number of mosquitoes decreases by about \\(100\\% \\times [1-\\exp\\{-0.19794\\}] \\approx 18\\%\\). Note that when using a standardized variable, a unit increase for this corresponds to an increase in the original unstandardized variable equal to its standard deviation, which for the elevation variable is about 100 meters.\nFrom the summaries of the two models, under Random effects:, we obtain the estimates associates with the random effects introduced in the model. In this case, since we only have introduced \\(Z_i\\), this part of summary provides the maximum likelihood estimate for \\(\\tau^2\\), the variance of \\(Z_i\\), which found on the line where ID_loc is printed. We then observe that the estimates for \\(\\tau^2\\) for the intercept-only model is 0.761, whilst for the model with elevation this is 0.7146. Note that the figures reported under Std.Dev. are simply the square root of the value reported under Variance. As expected, the introduction of elevation contributes to the explanation of the residual variation captured by \\(Z_i\\), though by a very small amount. The estimated values of \\(\\tau^2\\) thus suggest that there is extra-Binomial variation in the data that is not account for by elevation.\nIn the next section, we will illustrate how to assess the presence of residual correlation for continuous measurements and overdispersed count data.\n\n\n\n3.1.3 Exploring residual spatial correlation\nIn its most basic form, the concept of spatial correlation can be succinctly encapsulated by Tobler (1970) first law of geography, which posits that “everything is interconnected, but objects in close proximity exhibit stronger relationships than those situated farther apart.” After we have identified the key variables to introduce as covariates in the model (Section 3.1.1) and, in the case of count data, assessed the presence of overdispersion (Section 3.1.2), our final exploratory step consists of assessing whether the residuals of the non spatial model show evidence of spatial correlation. Hence, in geostatistical modelling, the interest is not in the spatial correlation of the data, but rather on understanding whether the variation in the outcome unexplained by the covariates exhibits spatial correlation. We call this residual spatial correlation, to emphasize that spatial correlation is a concept relative to the covariates that we have introduced in the model.\nIn the context of geostatistical analysis, the tool that is generally used to assess the residual spatial correlation is the the so called empirical variogram. Before looking at the mathematical definition of the empirical variogram, let us consider a generalized linear mixed model as expressed in Equation 3.7. Our goal is then to question the assumption of independently distributed random effects \\(Z_i\\) by asking whether the \\(Z_i\\) show evidence of spatial correlation. Let \\(Z_i\\) and \\(Z_j\\) be two random effects that are associate with two different locations \\(x_i\\) and \\(x_j\\), respectively, and let us take the squared difference between the two \\[\nV_{ij} = (Z_i - Z_j)^2.\n\\tag{3.10}\\] How does the spatial correlation affect the value of \\(V_{ij}\\)? To answer this question, we can refer to the aforementioned Tobler’s law of geography. When \\(x_i\\) and \\(x_j\\) will be closer to each other, then \\(Z_i\\) and \\(Z_j\\) will also tend to be more similar to each other, thus making \\(V_{ij}\\) smaller, on average. On the contrary, when \\(x_i\\) and \\(x_j\\) will be further apart, then \\(V_{ij}\\) will become larger, on average. We can then construct the empirical variogram by considering all possible pairs of locations \\(x_i\\) and \\(x_j\\), for which we then compute \\(V_{ij}\\) and plot this against the distance between \\(x_i\\) and \\(x_j\\), which we denote as \\(u_{ij}\\). If there is spatial correlation in the random effects \\(Z_i\\), then this will manifest as an average increase in the \\(V_{ij}\\) as \\(u_{ij}\\) increases. However, there are still two issues that we have to address before we can generate and plot the empirical variogram.\nThe first issue is that we do not observe \\(Z_i\\) as, by definition, this is a latent variable. Hence, we require an estimate for \\(Z_i\\) which we can then feed into \\(V_{ij}\\). To emphasize this point, from now on, we shall replace Equation 3.10 with \\[\n\\hat{V}_{ij} = (\\hat{Z}_{i} - \\hat{Z}_j)^2.\n\\tag{3.11}\\] Several options are available for estimating \\(Z_{i}\\). Our choice is to use the model of the predictive distribution of \\(Z_i\\), that is the distribution of \\(Z_{i}\\) conditioned to the data \\(y_i\\). This estimator for \\(Z_i\\) is also readily available from the output of the lmer and glmer functions of the lme4 package, as we will illustrate later in our example in this section.\nThe second issue is that if simply plot \\(\\hat{V}_{ij}\\) against the distances \\(u_{ij}\\) (also knwon as cloud variogram), due to the high noiseness in the \\(\\hat{V}_{ij}\\), it may be quite difficult to assess the presence of an increasing trend in the \\(\\hat{V}_{ij}\\) and thus detect spatial correlation. Hence, it is general practice to group the distances \\(u_{ij}\\) into classes, say \\(\\mathcal{U}\\), and then take average of all the \\(\\hat{V}_{ij}\\) that fall within \\(\\mathcal{U}\\).\nWe can now write the formal definition of the empirical variogram as \\[\n\\hat{V}(\\mathcal{U}) = \\frac{1}{2 |\\mathcal{U}|} \\sum_{(i, j): (u_i, u_j) \\in \\mathcal{U}} \\hat{V}_{ij}\n\\tag{3.12}\\] where \\(|\\mathcal{U}|\\) denotes the number of pairs of locations that fall within the distance class \\(\\mathcal{U}\\). The rationale behind dividing by 2 in \\(1/2 |\\mathcal{U}|\\) from the above equation, will be elucidated in Section 3.2, and there is no need for us to delve into this matter at this juncture. When creating the empirical variogram plot, we select the midpoint values of the distance classes \\(\\mathcal{U}\\) to represent the x-axis values.\nBefore we can evaluate residual spatial correlation, there remains one crucial concern: relying solely on a visual inspection of the empirical variogram is susceptible to human subjectivity. Furthermore, it is worth noting that even a seemingly upward trend observed in the empirical variogram might be merely a product of random fluctuations, rather than a reliable indication of actual residual spatial correlation. To address these concerns and enhance the objectivity of the use of the empirical variogram, one approach would involve comparing the observed empirical variogram pattern with those generated in the absence of spatial correlation. Following this principle, we then use a permutation test that allows us to generate empirical variograms under the assumption of absence of spatial correlation through the following iterative steps.\n\nPermute the order of the locations in the data-set while keeping everything else fixed.\nCompute the empirical variogram \\(\\hat{V}(\\mathcal{U})\\) for the permuted data-set.\nRepeat 1 and 2 a large number of times, say 10,000.\nUse the resulting 10,000 empirical varigorams to compute 95\\(\\%\\) confidence intervals, by taking the 0.025 and 0.975 quantiles of these for each distance class \\(\\hat{V}(\\mathcal{U})\\).\nIf the observed empirical variogram falls fully within the envelope generated in the previous point, we then conclude that the data do not exhibit residual spatial correlation. If, instead, the observed empirical variogram partly falls outside the envelope we conclude that the data do exhibit residual spatial correlation.\n\nWe now show an application of all the concepts introduced in this section to the Liberia data on river-blindness.\n\n3.1.3.1 Example: assessing spatial correlation for the Liberia data\nWe consider the Binomial mixed model that uses the log-transformed elevation as a covariate to model river blindness prevalence, hence \\[\n\\log\\left\\{\\frac{p(x_i)}{1-p(x_i)}\\right\\} = \\beta_{0} + \\beta_{1}\\log\\{e(x_i)\\} + Z_i\n\\tag{3.13}\\] where \\(e(x_i)\\) is the elevation in meters at location \\(x_i\\) and the \\(Z_i\\) are i.i.d. Gaussian variables with mean 0 and variance \\(\\tau^2\\). We first fit the model above using the glmer function.\n\n# Convert the data-set into an sf object\nliberia &lt;- st_as_sf(liberia, coords = c(\"lat\", \"long\"), crs = 4326)\n\n\n# Create the ID of the location\nliberia$ID_loc &lt;- 1:nrow(liberia)\n\n# Binomial mixed model with log-elevation\nfit_glmer_lib &lt;- glmer(cbind(npos, ntest) ~ log(elevation) + (1|ID_loc), family = binomial,\n                        data = liberia, nAGQ = 25)\n\nsummary(fit_glmer_lib)\n\nGeneralized linear mixed model fit by maximum likelihood (Adaptive\n  Gauss-Hermite Quadrature, nAGQ = 25) [glmerMod]\n Family: binomial  ( logit )\nFormula: cbind(npos, ntest) ~ log(elevation) + (1 | ID_loc)\n   Data: liberia\n\n     AIC      BIC   logLik deviance df.resid \n   127.9    135.4    -61.0    121.9       87 \n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.46033 -0.63341 -0.07633  0.61995  3.12732 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n ID_loc (Intercept) 0.003097 0.05565 \nNumber of obs: 90, groups:  ID_loc, 90\n\nFixed effects:\n               Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    -2.96292    0.21184 -13.987  &lt; 2e-16 ***\nlog(elevation)  0.26143    0.04071   6.422 1.35e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr)\nlog(elevtn) -0.981\n\n\nFrom the output, we observe that the estimate for \\(\\tau^2\\) is about 0.003, indicating a moderate level of overdispersion in the data.\n\nliberia$Z_hat &lt;- ranef(fit_glmer_lib)$ID_loc[,1]\n\nThrouhg the function ranef when extract the estimates of the random effects \\(Z_i\\) and save these in the data set. We then use the function s_variogram from the RiskMap package to compute the empirical varigoram for the estimated \\(\\hat{Z}_i\\).\n\nliberia_variog &lt;- s_variogram(data = liberia,\n                              variable = \"Z_hat\",\n                              bins = c(15, 30, 40, 80, 120,\n                                       160, 200, 250, 300, 350),\n                              scale_to_km = TRUE,\n                              n_permutation = 10000)\n\nThrough the argument bins we can specify the the classes of distance, previously denoted by \\(\\mathcal{U}\\); check the help page of s_variogram to see how this is defined by default. The value passed to bins in the code above correspond to define the following classes of distance \\(\\mathcal{U}\\): \\([15, 30]\\), \\((30, 40]\\) and so forth, with the last class being \\([350, +\\infty)\\), i.e. all pairs of locations whose distances are above 350km. The argument n_permutation allows the user to specify the number of permutations that are performed the generate the envelope for absence of spatial correlation previously described.\n\ndist_summaries(data = liberia,\n               scale_to_km = TRUE)\n## $min\n## [1] 3.34536\n## \n## $max\n## [1] 533.0733\n## \n## $mean\n## [1] 206.7424\n## \n## $median\n## [1] 192.6496\n\nThe dist_summaries function within the RiskMap package can be used for gauging the extent of the area covered by your dataset, aiding in the selection of appropriate values to be passed to the bins argument. In the provided output above, we can observe that for the Liberia dataset, the minimum and maximum distances span approximately 3km and 533km, respectively. While there is not a one-size-fits-all recommendation for setting bins, two fundamental principles should inform your decision-making. Firstly, it is advisable to avoid choosing overly large distance intervals, as the uncertainty associated with the empirical variogram tends to increase with distance due to fewer available pairs of observations for estimation. Secondly, especially when spatial correlation is not strong, it is crucial to carefully explore the behavior of the variogram at smaller distances. Consequently, it is generally advisable to experiment with different bins configurations and observe how they impact the pattern of the empirical variogram.\n\nplot_s_variogram(liberia_variog,\n                 plot_envelope = TRUE)\n\n\n\n\nFigure 3.7: Plot of the empirical variogram (solid line) computed using the estimated random effects from the model in Equation 3.13. The blue shaded area is the 95% confidence level envelope generated using the permutation procedure described in Section 3.1.3.\n\n\n\n\nFinally, the plot_s_variogram function enables us to visualize the empirical variogram and, through the plot_envelope argument, include the envelope generated by the permutation procedure. As illustrated in Figure 3.7, we observe that the empirical variogram falls outside the envelope at relatively short distances, typically below 30km. However, for distances exceeding 30km, the behavior of the empirical variogram does not significantly differ from variograms generated under the assumption of spatial independence. In summary, we interpret the evidence presented in Figure 3.7 as indicative of residual spatial correlation within the data. Nevertheless, it is essential to exercise caution when attempting to ascertain the scale of the spatial correlation using the empirical variogram. As we will emphasize throughout this book, the empirical variogram’s sensitivity to the choice of bins values renders it an unreliable tool for drawing statistical inferences. In other words, we advocate employing the empirical variogram primarily to assess the presence of residual correlation.\n\n\n3.1.3.2 Exploring residual spatial correlation with linear Guassian models\nWhen using a linear model to assess spatial correlation, it is important to distinguish two cases: 1) when the data contain only one location per location; 2) when more than one observation per location is available. We now consider each of these two scenarios separately.\n\n3.1.3.2.1 One observation per location\nTo illustrate the use of the variogram under this scenario, we shall use the Gailicia data on lead concentration in moss samples. The simplest possible model for the data is a standard linear model without covariates which assumes independence among the observations, hence \\[\nY_i = \\beta_0 + U_i\n\\tag{3.14}\\] where the \\(U_i\\) are i.i.d. Gaussian variables with mean zero and variance \\(\\omega^2\\). At this stage, our goal is then to assess whether the assumption of independence for the \\(Z_i\\) is supported by the data or whether there is evidence of spatial correlation. However, the measurement error of the device may be present as part of the natural random variation in \\(Z_i\\) which might mask the detection of spatial correlation using the residuals \\(Z_i\\) challenging, especially is the measurement error dominates the spatial variation of the data. One would be tempted to introduce an additional, location-specific random effect, say \\(Z_i\\) with mean zero and variance \\(\\tau^2\\) and fit the model \\[\nY_i = \\beta_0 + Z_i + U_i,\n\\tag{3.15}\\] where we interpret \\(U_i\\) as random variation due to the measurement device and \\(Z_i\\) as a random effect accounting for unmeasured covariates that contribute to the variation between locations in lead concentration. However, the model in Equation 3.15 is not identifiable because we cannot disentangle the separate contributions of \\(Z_i\\) and \\(U_i\\) from the variation of the data, unless: a) we know the precision of the measurement device, \\(\\omega\\) (recall that \\(\\omega^2\\) is the variance of \\(U_i\\)); b) or, if we do not know \\(\\omega\\), we can then separate the two sources of variation only if we have multiple observations per location (the scenario which we shall consider in the next section).\nFor the Galicia data, for we do not know the measurement device precision and we only have one observation per location. However, this does not prevent us from using the variogram based on the residuals from Equation 3.14, while keeping in mind the limitations and uncertainty that are inherent to this exploratory tool as remarked at the end of the last paragraph.\n\n# Fitting of the linear model and extraction of the residuals  \nlm_fit &lt;- lm(log(lead) ~ 1, data = galicia)\ngalicia$residuals &lt;- lm_fit$residuals\n\n# Convert the galicia data frame into an \"sf\" object\ngalicia_sf &lt;- st_as_sf(galicia, coords = c(\"x\", \"y\"), crs = 32629)\n\n# Compute the variogram, using the residuals from the linear model fit,\n# and the 95% confidence level envelope for spatial independence\ngalicia_variog &lt;- s_variogram(galicia_sf, variable = \"residuals\",\n                              scale_to_km = TRUE,\n                              bins = seq(10, 140, length = 15),\n                              n_permutation = 10000)\n\n# Plotting the results\nplot_s_variogram(galicia_variog, plot_envelope = TRUE)\n\n\n\n\nFigure 3.8: Plot of the empirical variogram (solid line) computed using the estimated residuals from the model in Equation 3.14. The blue shaded area is the 95% confidence level envelope generated using the permutation procedure described in Section 3.1.3.\n\n\n\n\nIn the code above, we first fit the linear model in Equation 3.14 and then extract the residuals from this. Note that for this simple model, the residuals of the model are obtained by simply centering the outcome to zero by subtracting its mean. However, for more complex linear models that use covariates the computation of the residuals is more involved and can be carried out through a simple extension of the code above by specifying an appropriate formula in the lm function.\nFigure 3.8 shows the empirical variogram and the 95% envelope for spatial independence. This clearly shows that the measurement of lead concentration are spatially correlated.\n\n\n3.1.3.2.2 More than one observation per location\nFor the case of more than one observation per location, we shall consider the italy_sim data-set. This data-set contains 10 observations per location, for a total of 200 locations. The variable ID_loc is a numeric indicator that can be used to identify the location each observation belong to. For this data-set, we use the population density, named pop_dens, as a log-transformed covariate; we leave you as an exercise to assess that is a reasonable modelling choice.\nTo specify a non-spatial mixed model for the outcome, we then use two subscripts: \\(i\\) to identify a given location; \\(j\\) to identify the \\(j\\)-th observation for a given location \\(i\\). We denote as \\(Z_i\\) the location-specific random effect ans as \\(U_{ij}\\) the random variation due to the measurement error inherent to each observation. Hence, we write \\[\nY_{ij} = \\beta_0 + \\beta_1 \\log\\{d(x_i)\\} + Z_i + U_{ij},\n\\tag{3.16}\\] where \\(d(x_i)\\) is the population density at location \\(x_i\\); as before, we use \\(\\tau^2\\) and \\(\\omega^2\\) to denote the variances of \\(Z_i\\) and \\(U_{ij}\\), respectively. To fit this model to the data we use the lmer function from the lme4 package.\n\n# Fitting a linear mixed model to the italy_sim data-set\n# See main text for model specification\nlmer_fit &lt;- lmer(y ~ log(pop_dens) + (1|ID_loc), data = italy_sim)\n\nsummary(lmer_fit)\n## Linear mixed model fit by REML ['lmerMod']\n## Formula: y ~ log(pop_dens) + (1 | ID_loc)\n##    Data: italy_sim\n## \n## REML criterion at convergence: 3390.9\n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -2.85209 -0.64198 -0.01832  0.66014  3.01870 \n## \n## Random effects:\n##  Groups   Name        Variance Std.Dev.\n##  ID_loc   (Intercept) 2.5006   1.5813  \n##  Residual             0.1959   0.4426  \n## Number of obs: 2000, groups:  ID_loc, 200\n## \n## Fixed effects:\n##               Estimate Std. Error t value\n## (Intercept)   -0.40448    0.57613  -0.702\n## log(pop_dens)  1.33798    0.07643  17.506\n## \n## Correlation of Fixed Effects:\n##             (Intr)\n## log(pp_dns) -0.981\n\n# Incorporating the estimated random effects into the data\nitaly_sim$rand_eff &lt;-ranef(lmer_fit)$ID_loc[italy_sim$ID_loc,1]\n\n# Converting the italy_sim data frame into an \"sf\" object \nitaly_sim_sf &lt;- st_as_sf(italy_sim, coords=c(\"x1\", \"x2\"), crs = 32634)\n\n# Compute the variogram, using the random effects from the linear mixed model fit,\n# and the 95% confidence level envelope for spatial independence\nitaly_sim_variog &lt;- s_variogram(italy_sim_sf, variable = \"rand_eff\", \n                                scale_to_km = TRUE,\n                                n_permutation = 200)\n\n# Plotting the results\nplot_s_variogram(italy_sim_variog, plot_envelope = TRUE)\n\n\n\n\nFigure 3.9: Plot of the empirical variogram (solid line) computed using the estimated random effects from the model in Equation 3.16. The blue shaded area is the 95% confidence level envelope generated using the permutation procedure described in Section 3.1.3.\n\n\n\n\nIn the code above, the introduction of the \\(Z_i\\) random effect is specified in the lmer function though (1|ID_loc) in the formula; recall that ID_loc is the numerical indicator that identifies each location. From the summary of the model, we obtain both the estimates of the regression coefficients \\(\\beta_{0}\\) and \\(\\beta_{1}\\), as well for the variances \\(\\tau^2\\) listed under Random effects:. The estimate of \\(\\sigma^2\\) is found on the line of printed output starting with ID_loc, whilst that for $^2 is next to Residual.\nThe application of the empirical variogram, whose results are shown in Figure 3.9, indicate the presence of residual correlation. This is because we observe that the solid line representing the empirical variogram falls outside of the 95% envelope for spatial independence."
  },
  {
    "objectID": "03_model-fitting.html#sec-linear-model",
    "href": "03_model-fitting.html#sec-linear-model",
    "title": "3  Model formulation and parameter estimation",
    "section": "3.2 The linear geostatistical model",
    "text": "3.2 The linear geostatistical model\nIn this section, we consider spatially referenced outcomes \\(Y_i\\) that are continuous. We first consider the simpler case of a single measurement \\(Y_i\\) per location \\(x_i\\). Recalling the class of generalized linear models introduced in Section 1.5, the linear predictor takes the form \\[\n\\mu_{i} = d(x_i)^\\top \\beta + S(x_i).\n\\tag{3.17}\\] Hence, in this case, we interpret \\(\\beta\\) as the effect on \\(\\mu_i\\) for a unit increase in \\(d(x_i)\\). Let \\(U_i\\) denote i.i.d. random variables representing the measurement error associated with \\(Y_i\\), each having mean zero and variance \\(\\omega^2\\). Thanks to the linear properties of Gaussian random variables, we can also express the linear model in a compact expression, as \\[\nY_i = \\mu_i + U_i = d(x_i)^\\top \\beta + S(x_i) + U_i.\n\\tag{3.18}\\] To fully specify a geostatistical model for our dataset, we must address two critical aspects.\n\nDefining the relationship between each covariate and the mean value \\(\\mu_i\\).\nSelecting an appropriate correlation function for \\(S(x)\\).\n\nAs illustrated in the previous sections, the initial step of exploratory analysis allows us to handle the first aspect, where we determine the regression relationship between covariates and the mean value \\(\\mu_i\\). However, based on existing methods of exploratory analysis, it is difficult to understand what is a suitable correlation function at this stage. A commonly recommended starting point is the Matern correlation function (see Equation 1.5), which offers considerable flexibility in capturing a wide range of correlation structures, under the assumption stationarity and isotropy. As we shall illustrate in the next example, even estimating a Matern correlation function is a task that poses many inferential challenges due to the poor identifiability, especially, of its smoothness parameter \\(\\kappa\\).\n\n3.2.1 Evaluating the inclusion of the measurement error term \\(U_i\\) and the specification of the smoothness parameter \\(\\kappa\\)\nIn this section we analyse the Galicia data using a linear geostatistical geostatitsical model for the log-transformed lead concentration, which we denote as \\(Y_i\\). Since we do not use covariates, we then write the model as \\[\nY_i = \\mu + S(x_i) + U_i\n\\tag{3.19}\\] where were \\(S(x)\\) is a Matern process with variance \\(\\sigma^2\\), scale parameter \\(\\phi\\) and and smothness parameter \\(\\kappa\\); the \\(U_i\\) correspond to the measurement error term and denote with \\(\\omega^2\\) their variance.\nWe carry out the parameter estimation of the model using the glgpm function from the RiskMap package. This function implements maximum likelihood estimation for generalized linear mixed models using a Matern correlation function, while fixing the smoothness parameter \\(\\kappa\\) at prespecified value by the user. The object passed to the argument data in glpm can either be a data.frame object or an sf object. Below we illustrate the use of glgpm while distinguishing between these two cases.\n\n# Parameter estimation when the argument passed to `data` is a data-frame\nfit_galicia &lt;- \nglgpm(log(lead) ~ gp(x, y, kappa = 1.5), data=galicia, family = \"gaussian\",\n      crs = 32629, scale_to_km = TRUE, messages = FALSE)\n\nThe code above shows the use of glgpm by passing galicia as a data.frame object to data. The specification of the Guassian process \\(S(x)\\) is done through the addition of the term gp() in the formula. The function gp allows you to specify the columns of the coordinates in the data, in this case x and y, the smoothness parameter through the argument kappa, set to 1.5 in this example. In the help page of gp, you can see that by default the nugget term (denoted in this book by the random variable \\(Z_i\\)) is excluded from the model by default; to include and estimate the variance parameter of the nugget, you should set nugget=NULL in the gp() function. However, doing so for a linear model that only has one observation per location will generate error message as this is a non-identifiable model for the same reasons given in Section 3.1.3.2.2. However, if the measurement error variance is known this can be fixed by the user using the argument fix_var_me and the inclusion of the nugget term is then possible (to better understand this point, try Exercise 7 at the end of this chapter).\nThe argument crs is used to specify the coordinate reference system (CRS) of the data. For the Galicia data, as well as for every other data-set used in this book, the CRS is reported in the help page description of the data-set. If crs is not specified, the function will assume that the coordinates are in longitude/latitude format and will use these without applying any transformation. Finally, the argument scale_to_km, is used to specify whether the distances between locations should be scaled to kilometers or maintained in meter; this argument will not affect the scale, and thus the interpretation of the spatial correlation parameter \\(\\phi\\).\n\n# Parameter estimation when the argument passed to `data` is an sf object\ngalicia_sf &lt;- st_as_sf(galicia, coords = c(\"x\", \"y\"), crs = 32629)\n\nfit_galicia_sf &lt;- \nglgpm(log(lead) ~ gp(kappa = 1.5), data=galicia_sf, family = \"gaussian\",\n      scale_to_km = TRUE, messages = FALSE)\n\nThe code above shows the alternative approach to estimate the model, when the argument passed to data is an sf object. In this case, the data-set galicia is converted into an sf object before the use of the glgpm function using st_as_sf. When when then fit the linear geostatistical model with galicia_sf, the only differences with the previous chunk of code that used galicia instead, is that the coordinates names in gp() and the crs argument in glgpm do not need to be specified as they are both directly obtained from galicia_sf.\n\nsummary(fit_galicia)\n## Linear geostatsitical model \n## 'Lower limit' and 'Upper limit' are the limits of the 95% confidence level intervals \n## \n##  Regression coefficients \n##             Estimate Lower limit Upper limit   StdErr z.value   p.value    \n## (Intercept) 0.707418    0.552762    0.862075 0.078908  8.9651 &lt; 2.2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Offset included into the linear predictor \n## \n##                           Estimate Lower limit Upper limit\n## Measuremment error var. 0.0154636   0.0051797      0.0462\n## \n##  Spatial Guassian process \n## Matern covariance parameters (kappa=1.5) \n##                      Estimate Lower limit Upper limit\n## Spatial process var.  0.17127     0.13303      0.2205\n## Spatial corr. scale   9.02085     7.59521     10.7141\n## Variance of the nugget effect fixed at 0 \n## \n##  Log-likelihood: 69.03029\n## \n##  AIC: -138.0606\n## \n\nWe can then inspect the point and interval estimates for the model through the summary function of the model as shown in the code chunk above. This outputs is presented in three sections: in the first section, we have the results for the regression coefficients; in the second section, we have the estimate for the variance of the measurement error component, \\(\\omega^2\\) whose point estimate is about 0.015; in the final section, we have the estimates for the parameters of the spatial covariance function, \\(\\sigma^2\\) and \\(\\phi\\) which are about 0.171 and 9.021 (km), respectively. The message Variance of the nugget effect fixed at 0 indicates that the nugget has not been included in the mode; try Exercise 7 to see how this summary changes in the presence of the nugget term.\nAt this point, you may be wondering, why we have used 1.5 for the value of \\(\\kappa\\) and whether there is a statistical approach to find the most suitable value for this. We show such an approach in the code below. However, before examining the code, we would like to point an important aspect that relates to how the value of \\(\\kappa\\) can affect the estimate of the measurement error variance \\(\\omega^2\\). As we have shown, in Section 1.5.1, values of \\(\\kappa\\) that are closer to zero will give a rougher and less regular surface for \\(S(x)\\). In the case of a single observation, when \\(\\kappa\\) is closer to zero, it will thus become increasingly difficult to estimate \\(\\omega^2\\) since the likelihood function may attribute most of the noisiness to \\(S(x)\\) and rely less on \\(U_i\\) to explain the unstructured random variation found in the data. As a consequence of this, we can expect that for value of \\(\\kappa\\) closer to zero, the estimates of \\(\\omega^2\\) will also be smaller and, on the contrary, when \\(\\kappa\\) is larger, the estimates of \\(\\omega^2\\) will also increase. The results generated in the below clearly illustrate this point.\n\n\n# Number of the values chosen for kappa\nn_kappa &lt;- 10\n\n# Set of values for kappa\nkappa_values &lt;- seq(0.5, 3.5, length = n_kappa)\n\n# Vector that will store the values of the likelihood function \n# evaluated at the maximum likelihood estimate\nllik_values &lt;- rep(NA, length = n_kappa)\n\n# Vector that will store the maximum likelihood estimates\n# of the variance of the measurement error\nsigma2_me_hat &lt;- rep(NA, length = n_kappa)\n\n# List that will contain all the geostatistical models fitted for \n# the different values of kappa specified in kappa_values\nfit_galicia_list &lt;- list()\n\nfor(i in 1:n_kappa) {\n  fit_galicia_list[[i]] &lt;- glgpm(log(lead) ~ gp(x, y, kappa = kappa_values[i]), \n                                 data=galicia, family = \"gaussian\",\n                                 crs = 32629, scale_to_km = TRUE, messages = FALSE)\n  llik_values[i] &lt;-  fit_galicia_list[[i]]$log.lik\n  sigma2_me_hat[i] &lt;-  coef(fit_galicia_list[[i]])[\"sigma2_me\"]\n}\n\n\n\n\n\n\nFigure 3.10: (a) plot of the maximum likelihood estimate for the variance of the measurement error \\(U_i\\), denoted in the text as \\(\\omega^2\\), against the chosen fixed value for \\(\\kappa\\); (b) profile likeihood for \\(\\kappa\\) with the horizontal dashed line corresponding to the (approximate) threshold for constructing a 95% confidence interval based on a \\(\\chi^2\\) with one degree of freedom.\n\n\n\n\nBy examining the results shown in panel (a) of Figure 3.10, we observe that, as expected, smaller values for \\(\\kappa\\) leads to smaller point estimates for \\(\\omega^2\\) and viceversa. This begs the question, what should be our chosen value for \\(\\kappa\\)?\nTo answer this question, a natural approach is to estimate the model for different values of \\(\\kappa\\) and see which one give the best fit to the data, according to the likelihood function. In panel (b) of Figure 3.10, we show the results of this approach where we considered 10 values for \\(\\kappa\\) within the range 0.5 to 3.5 (see code above). In this plot, we have also added a horizontal line to help us to approximate a range of the most plausible value for \\(\\kappa\\). More precisely, the horizontal dashed line is computed by taking the maximum observed values of the likelihoods computed for the different values of \\(\\kappa\\), say \\(\\hat{M}\\) and we subtract the quantile 0.95 of a \\(\\chi^2\\) distribution with 1 degree of freedom. In R this horizontal line is obtained as\n\nmax(llik_values)-pchisq(0.95, df = 2)/2\n\nwhere llik_values is as defined in the previous chunk of code. Note that this approach is essentially constructing the profile likelihood for \\(\\kappa\\) which one could use to derive a confidence interval for \\(\\kappa\\) with a finer segmentation for kappa_values. However, our current objective is not to derive the confidence interval for \\(\\kappa\\), but rather to gain a broad understanding of the \\(\\kappa\\) values supported by the dataset. The values of \\(\\kappa\\) that corresponds to likelihood values above the horizontal line are approximately between 0.75 and 2.75. Hence, selecting \\(\\kappa=1.5\\) seems to be a reasonable one in this case. Now, you may be pondering: are there value other than \\(\\kappa=1.5\\) that could fit the data even better? Our answer is that it is not worth the effort to try estimate \\(\\kappa\\) more precisely because it is empirically very difficult and, under some scenarios, even impossible. Estimating \\(\\kappa\\) poses a well-documented challenge in geostatistics (Zhang (2004)), which justifies our adoption of a pragmatic approach that sets it at a predefined value. This issue is also further exacerbated when analyzing count data, which tend to be less informative about the correlation structure than continuously measured data.\n\n\n3.2.2 Modelling hierarchical geostatistical data using the re() function\nWe now consider the analysis of geostatistical data with a hierarchical structure and show how to formulate and fit a geostatistical model that accounts for the effects of the different layers of the data. For this purpose, we use the italy_sim where each of the sampled locations can be grouped according to two administrative subdivisions of Italy, regions (Admin level 2) and provinces (Admin level 3), as shown in Figure 3.11.\n\nlibrary(rgeoboundaries)\nlibrary(mapview)\nitaly_regions &lt;- geoboundaries(country = \"italy\", adm_lvl = \"adm2\")\n\nitaly_provinces &lt;- geoboundaries(country = \"italy\", adm_lvl = \"adm3\")\n\npar(mfrow = c(1,2))\n\n# Map of the data with the region boundaries\nmap_regions &lt;- ggplot() + \ngeom_sf(data = italy_sim_sf, pch = 4, color = \"red\") + \ngeom_sf(data = italy_regions, fill = NA) + \ntheme_void() +\nlabs(title = \"Regions\") +\ntheme(plot.title = element_text(hjust = 1/2))\n\n# Map of the data with the province boundaries\nmap_provinces &lt;- ggplot() + \ngeom_sf(data = italy_sim_sf, pch = 4, color = \"red\") + \ngeom_sf(data = italy_provinces, fill = NA) + \ntheme_void() +\nlabs(title = \"Provinces\") +\ntheme(plot.title = element_text(hjust = 1/2))\n\nlibrary(gridExtra)\ngrid.arrange(map_regions, map_provinces, ncol = 2)\n\n\n\n\nFigure 3.11: The plot shows the location of the data-set italy_sim (red crosses), with the boundaries of the regions (left) and provinces (right) of Italy.\n\n\n\n\nLet \\(V_h\\), for \\(h=1,\\ldots,n_V\\) and \\(Z_k\\), for \\(k=1,\\ldots,n_{Z}\\), be a set of mutually independent Gaussian variables with zero means and variances \\(\\sigma^2_{V}\\) and \\(\\sigma^2_{Z}\\), respectively. Finally let \\(\\mathcal{A}_h\\), for \\(h=1,\\ldots,n_V\\) and \\(\\mathcal{B}_k\\), for \\(k=1,\\ldots,n_{Z}\\), denote the areas encompassed by the boundaries of the region and provinces, respectively. Since we have 10 repeated observations at each locations, we shall use \\(Y_{ij}\\) to denote the \\(j\\)-th outcome (found in the data under the column y) at the \\(i\\)-th location \\(x_i\\). The model from which we generated data \\(y_{ij}\\) is \\[\nY_{ij} = \\beta_0 + \\overbrace{\\beta_1 d(x_i) + S(x_i)}^\\text{Location-level effect} + \\underbrace{\\sum_{h=1}^{n_{V}} I(x_i \\in \\mathcal{A}_h) \\times V_h}_\\text{Region effect} +\\\\ \\overbrace{\\sum_{k=1}^{n_{Z}} I(x_i \\in \\mathcal{B}_k) \\times Z_k}^\\text{Province effect} +  \\underbrace{U_{ij}}_\\text{Measurement error},\n\\tag{3.20}\\] for \\(j=1,\\ldots 10\\) and \\(i=1,\\ldots,200\\), where, \\(d(x_i)\\) is the log-transformed population density, \\(I(x \\in \\mathcal{R})\\) is an indicator function that takes value 1 if the location \\(x\\) falls within the boundaries of the area denoted by \\(\\mathcal{R}\\) and 0 otherwise. The covariance function chosen in the generation of the data was an exponential correlation function hence \\({\\rm cov}\\{S(x), S(x') = \\sigma^2 \\exp\\{-||x-x'||/\\phi\\}\\).\nThe inclusion of the random effects \\(V_h\\), for the region, and \\(Z_k\\), for the province, can be included by using the re() function into the formula passed to glgpm as shown below.\n\n\n                        # Location-level effect\nitaly_fit &lt;- glgpm( y ~ log(pop_dens) + gp(kappa = 0.5, nugget = 0) +\n                        # Region and province effects\n                        re(region, province),\n         data = italy_sim_sf, scale_to_km = TRUE,\n         family = \"gaussian\")\n##   0:     441.00731: -0.404481  1.33798  0.00000  4.62406  0.00000  0.00000  0.00000\n##   1:    -130.05497: -0.408825  1.35003 -0.0387110  4.66059 -0.910409 -0.00265292 0.0107965\n##   2:    -268.13436: -0.865126  1.39525 -0.0305269  5.92133 -1.96079 -0.666581 0.487537\n##   3:    -329.24560: -1.44091  1.38139 -0.546306  4.98616 -1.67936 -1.15202 0.383821\n##   4:    -331.20861: -1.38498  1.37256 0.165463  5.61366 -1.63033 -2.11435 0.371263\n##   5:    -331.28418: -0.874796  1.37322 -0.271990  5.18071 -1.62887 -0.970704 0.371407\n##   6:    -331.42968: -1.04087  1.37294 -0.119652  5.33634 -1.62883 -1.33321 0.375031\n##   7:    -331.43375: -1.07312  1.37288 -0.0878211  5.36725 -1.62883 -1.42011 0.376251\n##   8:    -331.43375: -1.07445  1.37288 -0.0866254  5.36840 -1.62883 -1.42489 0.376323\n##   9:    -331.43375: -1.07445  1.37288 -0.0866254  5.36840 -1.62883 -1.42489 0.376323\n\nsummary(italy_fit)\n## Linear geostatsitical model \n## 'Lower limit' and 'Upper limit' are the limits of the 95% confidence level intervals \n## \n##  Regression coefficients \n##                Estimate Lower limit Upper limit    StdErr z.value p.value    \n## (Intercept)   -1.074451   -2.031690   -0.117212  0.488396    -2.2 0.02781 *  \n## log(pop_dens)  1.372877    1.352018    1.393735  0.010642   129.0 &lt; 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## Offset included into the linear predictor \n## \n##                          Estimate Lower limit Upper limit\n## Measuremment error var.  0.19616     0.19361      0.1987\n## \n##  Spatial Guassian process \n## Matern covariance parameters (kappa=0.5) \n##                       Estimate Lower limit Upper limit\n## Spatial process var.   0.91702     0.59470       1.414\n## Spatial corr. scale  214.51980   153.60751     299.587\n## Variance of the nugget effect fixed at 0 \n## \n##  Unstructured random effects \n##                             Estimate Lower limit Upper limit\n## region (random eff. var.)    0.24053     0.14553      0.3976\n## province (random eff. var.)  1.45692     1.37851      1.5398\n## \n##  Log-likelihood: 331.4338\n## \n##  AIC: -662.8675\n## \n\nIn the code above, the factor variables region and province found in italy_sim are passed to re() and these are estimated as unstructured random effects as defined by Equation 3.20. From the summary of model, we then obtain the parameter and interval estimates as reported in Table 3.1.\n\n\nTable 3.1: Maximum likelihood estimates and, lower and upper limits of the 95% confidence interval for the parameters of the model in Equation 3.16.\n\n\nParameter\nPoint estimate\nLower limit\nUpper limit\n\n\n\n\n\\(\\beta_0\\)\n-1.074\n-2.032\n-0.117\n\n\n\\(\\beta_1\\)\n1.373\n1.352\n1.394\n\n\n\\(\\sigma^2\\)\n0.917\n0.595\n1.414\n\n\n\\(\\phi\\)\n214.520\n153.608\n299.587\n\n\n\\(\\sigma^2_{V}\\)\n0.241\n0.146\n0.398\n\n\n\\(\\sigma^2_{Z}\\)\n1.457\n1.379\n1.540\n\n\n\\(\\omega^2\\)\n0.196\n0.194\n0.199\n\n\n\n\nThe function to_table from the RiskMap package can be used to obtain the Latex or HTML code directly from a fit of the model. Here is an example.\n\nto_table(italy_fit, digits = 3)\n## % latex table generated in R 4.1.2 by xtable 1.8-4 package\n## % Tue Jun 25 11:25:54 2024\n## \\begin{table}[ht]\n## \\centering\n## \\begin{tabular}{rrrr}\n##   \\hline\n##  & Estimate & Lower limit & Upper limit \\\\ \n##   \\hline\n## (Intercept) & -1.074 & -2.032 & -0.117 \\\\ \n##   log(pop\\_dens) & 1.373 & 1.352 & 1.394 \\\\ \n##   Spatial process var. & 0.917 & 0.595 & 1.414 \\\\ \n##   Spatial corr. scale & 214.520 & 153.608 & 299.587 \\\\ \n##   region (random eff. var.) & 0.241 & 0.146 & 0.398 \\\\ \n##   province (random eff. var.) & 1.457 & 1.379 & 1.540 \\\\ \n##   Measuremment error var. & 0.196 & 0.194 & 0.199 \\\\ \n##    \\hline\n## \\end{tabular}\n## \\end{table}"
  },
  {
    "objectID": "03_model-fitting.html#generalized-linear-geostatistical-models",
    "href": "03_model-fitting.html#generalized-linear-geostatistical-models",
    "title": "3  Model formulation and parameter estimation",
    "section": "3.3 Generalized linear geostatistical models",
    "text": "3.3 Generalized linear geostatistical models\nWe now consider the modelling of count outcomes, focusing our attention to the Binomial and Poisson geostatistical models.\nThe use of Binomial geostatistical model is appropriate whenever the reported counts have a known upper bound. For example, when analyzing aggregated disease counts from cross-sectional survey data, the total number of people sampled at a location defines the largest number of possible reported disease cases. Recalling the model specification of Table 1.3, we specify the Binomial geostatistical model by stating that the reported disease counts \\(Y_i\\) out of \\(n_i\\) (the maximum value that \\(Y_i\\) can take) at location \\(x_i\\), conditionally on the spatial random effects \\(S(x_i)\\), follow a Binomial distribution with probability \\(p(x_i)\\) and logit-linear predictor \\[\n\\log\\left\\{\\frac{p(x_i)}{1-p(x_i)}\\right\\} = d(x_i)^\\top \\beta + S(x_i).\n\\tag{3.21}\\] In this model, \\(p(x_i)\\) is commonly understood as representing the prevalence of the disease under consideration, defined as the proportion of individuals with the disease at a specific moment. While \\(p(x_i)\\) is not strictly a proportion, its interpretation as disease prevalence is justified by the fact that if we were to sample a sufficiently large population at location \\(x_i\\), the proportion of individuals with the disease would converge closely to the value of \\(p(x_i)\\).\nA Binomial geostatistical model is also used when analyzing data at individual level where the outcome is binary and usually indicates the positive or negative result of a test for a disease under investigation. In this analysis, a mix of individual-level variables and location-level variables could be introduced to model the probability of a positive test. To formally write the model, we now use \\(Y_{ij}\\) to indicate the binary outcome taking value 1, if the test was positive, and 0, if negative, for the \\(j\\)-th individual sampled at location \\(x_i\\). The linear predictor now takes the form \\[\n\\log\\left\\{\\frac{p_{ij}}{1-p_{ij}}\\right\\} = e_{ij}^\\top \\gamma + d(x_{i})^\\top \\beta + S(x_i).\n\\tag{3.22}\\] In the model above we have distinguished between covariates \\(e_{ij}\\), that express the effect on \\(p_{ij}\\) due to individual traits (such as age and gender), and covariates \\(d(x_i)\\), that instead quantify the effect on \\(p_{ij}\\) resulting from the environmental attributes of location \\(x_i\\) (such as elevation, temperature, and precipitation). In this model \\(p_{ij}\\) – the probability of a positive test – once again can be interpreted as the disease prevalence for a population characterized by the individual traits \\(e_{ij}\\) and residing at location \\(x_i\\).\nFinally, when the counts arise from a population whose size is unknown or when the occurrence of a disease is very small relative to the size of the target population, one should consider the use of a Poisson geostaistical model. We then use \\(Y_i\\) to denote the reported counts at location \\(x_i\\) and assume that this, conditionally on a Gaussian process \\(S(x_i)\\), follows a Poisson distribution with mean \\(\\lambda(x_i)\\) and log-linear predictor \\[\n\\log\\left\\{\\lambda(x_i)\\right\\} = d(x_i)^\\top \\beta + S(x_i).\n\\tag{3.23}\\] If for example, \\(Y_i\\) corresponds to the number of new cases reported at a health facility at location \\(x_i\\) over a certain, the \\(\\lambda(x_i)\\) is interpreted as the average number of new cases reported over that period. If the information on the size of the population at risk for the diseases were available, this can be incorporated into the model so as to enable the interpretation of \\(\\lambda(x_i)\\) as the disease incidence, defined as the total number of new cases reported over a given period of time divided by the population at risk. If we denote the latter as \\(m_i\\), then we would specify the mean of \\(Y_i\\) as \\(m_i\\lambda(x_i)\\) an model \\(\\lambda(x_i)\\) as already specified in Equation 3.23. Another prevalent application of the Poisson geostatistical model in epidemiology involves mapping the abundance of disease vectors. Here, \\(\\lambda(x_i)\\) corresponds to the mean number of vectors per unit area. Also, note that in this last scenario \\(m_i\\) cannot be defined as it is the goal of the analysis to infer the vector population size at location \\(x_i\\).\nIn all the three models, namely Equation 3.21, Equation 3.22 and Equation 3.23, one might consider the inclusion of an additional random effect \\(Z_i\\), previously used for the linear geostatistical model, to account for small spatial variation or overdispersion that stems from the omission of covariates that may not necessarily exhibit spatial structure, such as behavioral or genetic traits. We shall further elaborate and provide guidance on this point in the following examples of this section.\n\n3.3.1 Example: riverblindness in Liberia\nIn section Section 3.1.1.1, we carried out the exploratory analysis of the riverblindness prevalence data to assess how the covariate of elevation could be introduced into the model. In Section 3.1.3.1, we then tested for residual spatial correlation, when the log-transformed elevation is used as a covariate, using the empirical variogram. Based on these results, we now formulate a fit a Binomial geostatistical model to data. To achieve this, we extend the model presented in Equation 3.13 through the inclusion of a spatial Gaussian process, \\(S(x)\\), hence we write \\[\n\\log\\left\\{\\frac{p(x_i)}{1-p(x_i)}\\right\\} = \\beta_{0} + \\beta_{1}\\log\\{e(x_i)\\} + S(x_i) + Z_i.\n\\tag{3.24}\\] We shall assume that \\(S(x_i)\\) is a stationary isotropic Gaussian process with an exponential correlation function function \\(\\rho(u) = \\exp\\{-u/\\phi\\}\\), with scale parameter \\(\\phi\\).\nWe use the Monte Carlo maximum likelihood (MCML) method to carry out the parameter estimation (see Section 3.4.4), which is implemented in the glgpm function.\n\nset.seed(1)\nfit_liberia &lt;- \nglgpm(npos ~ log(elevation) + gp(long, lat, nugget = NULL),\n      distr_offset = ntest, data = liberia,\n      crs = 4326,\n      convert_to_crs = 32629,\n      family = \"binomial\", messages = FALSE)\n\nsummary(fit_liberia)\n\nBinomial geostatistical linear model \n'Lower limit' and 'Upper limit' are the limits of the 95% confidence level intervals \n\n Regression coefficients \n                Estimate Lower limit Upper limit    StdErr z.value   p.value\n(Intercept)    -2.948777   -3.376292   -2.521263  0.218123 -13.519 &lt; 2.2e-16\nlog(elevation)  0.298443    0.257020    0.339867  0.021135  14.121 &lt; 2.2e-16\n                  \n(Intercept)    ***\nlog(elevation) ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nOffset included into the linear predictor \n\n Spatial Guassian process \nMatern covariance parameters (kappa=0.5) \n                        Estimate Lower limit Upper limit\nSpatial process var.    0.246161    0.191336      0.3167\nSpatial corr. scale    60.068879   43.265064     83.3992\nVariance of the nugget  0.101348    0.071311      0.1440\n\n Log-likelihood: 39.78642\n\n\nWhen using the glgpm function to fit a Binomial model, we need to specify the Binomial denominators, the number of tested at a location in this case, through the argument distr_offset. Other arguments of glgpm that are used for carrying out MCML are control_mcmc, that defines the setting parameters of the Monte Carlo Markov Chain algorithm, and par0, which represents our best guess for the unknown parameters of the Binomial geostatistical model. For more information on how the default values of these two arguments, we refer the reader the to the help page of the glgpm function. To understand how to change the default parameters in order to improve the MCML estimation, we advise you to read Section 3.4.4. Also, remember that to enable the estimation of the nugget effect, \\(Z_i\\), you should set nugget=NULL in the gp function.\nIn the above summary of the model, the interpretation of the regression coefficients is the same as in a standard logistic regression model. Hence, by taking the exponential of the estimate for the regression coefficient \\(\\beta_1\\), we could then express the effect for a unit increase in the covariate – in this case the log-transformed elevation – on the odds of disease prevalence. Finally, the interpretation of estimates of the covariance parameters is not affected by the type of distribution used to model the main outcome."
  },
  {
    "objectID": "03_model-fitting.html#theory",
    "href": "03_model-fitting.html#theory",
    "title": "3  Model formulation and parameter estimation",
    "section": "3.4 Theory",
    "text": "3.4 Theory\n\n3.4.1 The likelihood function of a non-spatial generalized linear mixed model\nIn this section, we provide more technical details on the derivation of the likelihood function for the mixed model specified in Equation 3.7. While not essential for understanding the application and interpretation of geostatistical models, this section provides useful insights into computationally challenges inherent to the models presented in this book. It is also important to point out that in this section we consider the case when the outcome \\(Y_i\\) is a count.\nLet \\(\\theta = (\\beta, \\tau^2)\\) denote the vectore of unkown parameters, where \\(\\beta\\) is the vector of regression coefficients and \\(\\tau^2\\) is the variance of the random effects \\(Z = (Z_1, \\ldots, Z_n)\\). The likelihood function for a set of observations \\(Y = (Y_1, \\ldots, Y_n)\\), for \\(i=1,\\ldots,n\\), under the model in Equation 3.7, is obtained by deriving the marginal distribution of \\(Y_i\\), hence \\[\nL(\\theta) = [Y] = \\int_{R^n} \\: [Z] [Y \\:|\\: Z] \\: d Z\n\\] Since the elements \\(Z_i\\) in \\(Z\\) are mutually independent, we can write \\[\n[Z] = \\prod_{i=1}^n [Z_i]\n\\] and, using the conditional independence among the \\(Y\\) given \\(Z\\), we obtain \\[\n[Y \\:|\\: Z] = \\prod_{i=1}^n [Y_i | Z_i].\n\\] From these, it then follows that \\[\nL(\\theta) = \\int_{R^n} \\: \\prod_{i=1}^n [Z_i] \\prod_{i=1}^n [Y_i | Z_i] \\: d Z = \\prod_{i=1}^n \\int_{R} \\: [Z_i] [Y_i \\:|\\: Z_i] \\: d Z_i.\n\\tag{3.25}\\] This shows that under the model in Equation 3.7, whilst accounting for overdispersion, this model does not account for any source of correlation between the observations. This is because the likelihood function can be expressed, as shown in the equation above, as the product of \\(n\\) likelihoods associated to each of the observations.\nAnother important observation is that the integrals that express the likelihood in Equation 3.25, cannot be solved analytically and need to be approximated. Several solution are available that either based on maximum likelihood estimation or Bayesian methods of inference. The function glmer used in Section 3.1.3.2 provides both the Laplace and Gaussian quadrature approximations options. The former consists of finding the peak of the integrand - i.e. the function that is being integrate - and approximating that with a Gaussian distributioncentered at that peak, using the curvature of the original distribution to determine the width of the Gaussian distribution. The Gaussian quadrature is another numerical integration technique used to approximate the integral of a function. It involves choosing specific points (nodes) and weights within the integration interval to approximate the integral. By strategically selecting these points and weights based on a Gaussian distribution, Gaussian quadrature can provide highly accurate results compared to other numerical integration methods, such as the Laplace approximation. For a comprehensive overview of these and other methods of approximations of intractable integrals, we find that Bishop (2006) is one of the most accessible textbooks on this topic.\n\n\n3.4.2 The likelihood function of a genearlized linear geostatistical model\nTo derive the likelihood function of a generalized linear geostatistical model (GLGM), we consider the special case of a single observation per location without loss of generality. Let \\(Y\\) and \\(Z\\) denote the same vectors of random variables introduced in the previous section, with components \\(Y_i\\) and \\(Z_i\\) corresponding to the set of locations \\(X = (x_1, \\ldots, x_n)\\). Additionally, we define \\(S(X) = (S(x_1), \\ldots, S(x_n))\\) to represent the spatial Gaussian process at the sampled locations \\(X\\). In this case the vector of unknown parameters is \\(\\theta = (\\beta, \\sigma^2, \\phi, \\tau^2)\\) and, as done in the previous section, we can obtain the likelihood function though the marinal distribution of the data, hence we write \\[\nL(\\theta) = [Y] = \\int_{R^n} \\int_{R^n} \\: [Z] [S] [Y \\:|\\: Z, S] \\: d Z d S\n\\tag{3.26}\\] To simplify the explanation and notation in this section and the following ones, we will first rewrite the likelihood function in terms of the random effect ( W_i = S(x_i) + Z_i ). We will denote this in its vector form as ( W = (W_1, , W_n) ). \\[\nL(\\theta) = [Y] = \\int_{R^n} \\: [W] [Y \\:|\\: W] \\: d W\n\\tag{3.27}\\] In this model formulation, we assume that \\(Y\\) conditionally on \\(W\\) is a set of mutually independent outcomes, hence \\[\n[Y | W]  = \\prod_{i=1}^n [Y_i | W_i].\n\\] However, unlike in the previous section, the distribution of \\([W]\\) is a Multivariate Gaussian distribution with correlated components, hence we cannot split the integral into separate univariate ones as before. More specifically, \\(W\\) follows a Multivariate Gaussian distribution with mean vector \\(0\\) and covariance matrix \\(\\Omega\\), whose \\((i, j)\\)-th entry is \\[\n[\\Omega]_{ij} =\n\\begin{cases}\n\\sigma^2 \\rho(u_{ij}) & \\mbox{if }i\\neq j \\\\\n\\sigma^2 + \\tau^2 &  \\mbox{if }i = j\n\\end{cases}\n\\]\n\n\n3.4.3 Monte Carlo Markov Chain: sampling from the predictive distribution of the spatial Gaussian process\n\n\n3.4.4 Monte Carlo maximum likelihood"
  },
  {
    "objectID": "03_model-fitting.html#exercises",
    "href": "03_model-fitting.html#exercises",
    "title": "3  Model formulation and parameter estimation",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nConsider the Binomial mixed model with linear predictor as defined in Equation 3.7. By editing the code for the simulation shown in Section 3.1.2, generate a graph as in Figure 3.6 under the two following scenarios: \\(i\\)) \\(\\tau^2 = 0.2\\) and \\(n_i=100\\); \\(ii\\)) \\(\\tau^2 = 0.1\\) and \\(n_i = 1\\). How does the variance of \\(Y_i\\) change under \\(i\\)) and \\(ii\\)) in comparison to Figure 3.6? How do you explain the differences?\nSimilarly to the previous exercise, consider a Poisson mixed model with linear predictor \\[\n\\log\\left\\{\\mu_i\\right\\} = \\beta_0 + Z_i,\n\\] where \\(Z_i\\) are a set of mutually independent Gaussian variables with mean 0 and variance \\(\\tau^2\\). Using the code shown in Section 3.1.2, carry out a simulation study to compute the variance of \\(Y_i\\) and generate a graph similar to Figure 3.6 to compare the variance of the Poisson mixed model with that of a standard Poisson model. Generate the graph for different values of \\(\\tau^2\\) and summarize your findings. NOTE: In this simulation the offset \\(n_i\\) can be set to 1.\nCreate an R function that computes the cloud variogram. As explained in Section 3.1.3, the cloud variogram is obtained by plotting \\(\\hat{V}_{ij}\\) (see Equation 3.11) against the distances \\(u_{ij}\\). The function should take as input a data-set with three columns: the variable for which the cloud variogram is to be computed; and two columns corresponding to the location of the data. Then, use this function to create the cloud variogram for the model for river-blindness in Equation 3.13. How does this compare to empirical variogram that takes the averages within predefined distance classes, as shown in Figure 3.7?\nFit a Binomial mixed model to the Liberia data-set on river-blindness without any covariates, i.e. \\[\n\\log\\left\\{\\frac{p(x_i)}{1-p(x_i)}\\right\\} = \\beta_0 + Z_i.\n\\] Making use of the R code presented in Section 3.1.3.1, use the function s_variogram to generate the empirical variogram for this model and compare this to the empirical variogram of Figure 3.7. What differences do you observe?\nFit a Poisson mixed model to the anopheles mosquito data using elevation as a covariate, i.e. \\[\n\\log\\{\\mu(x_i)\\} = \\beta_{0} + \\beta_{1} d(x_i) + Z_i\n\\] where \\(d(x_i)\\) is the measured elevation at location \\(x_i\\). How strong is the overdispersion in the data? After fitting the model, extract the estimates of the random effects \\(Z_i\\) and compute the empirical variogram with the 95% envelope for spatial independence. Repeat this for different classes of distances by changing the input passed to bins in the s_variogram function. How do the different specifications of bins affect the results?\nConsider a linear model for the Galicia data as in Section 3.2.1 but now also introduce a nugget term, hence \\[\nY_i = \\mu + S(x_i) + Z_i + U_i\n\\tag{3.28}\\] where \\(Z_i\\) are i.i.d. Gaussian variables with mean 0 and variance \\(\\tau^2\\). Fit the model by fixing the variance of \\(U_i\\), \\(\\omega^2\\) to 0.01 using the argument fix_var_me in glgpm. Using this model, reproduce the graph as shown in Figure 3.10. Repeat this, but now fix \\(\\omega^2\\) to 0.02. How do the curves change and why?\n\n\n\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Springer.\n\n\nBowman, A. W. 1997. Applied Smoothing Techniques for Data Analysis : The Kernel Approach with s-Plus Illustrations. Oxford Statistical Science Series ; 18. Oxford : New York: Clarendon Press ; Oxford University Press.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. The Elements of Statistical Learning. Springer Series in Statistics. New York, NY, USA: Springer New York Inc.\n\n\nKatz, Elizabeth, and Bill & Melinda Gates Foundation. 2020. “Gender and Malaria Evidence Reivew.” Bill & Melinda Gates Foundation. https://www.gatesgenderequalitytoolbox.org/wp-content/uploads/BMGF_Malaria-Review_FC.pdf.\n\n\nSmith, David L, Carlos A Guerra, Robert W Snow, and Simon I Hay. 2007. “Standardizing Estimates of the Plasmodium Falciparum Parasite Rate.” Malaria Journal 6 (1): 131–31.\n\n\nTobler, W. R. 1970. “A Computer Movie Simulating Urban Growth in the Detroit Region.” Economic Geography 46: 234–40.\n\n\nWeisberg, Sanford. 2014. Applied Linear Regression. Fourth. Hoboken NJ: Wiley. http://z.umn.edu/alr4ed.\n\n\nZhang, Hao. 2004. “Inconsistent Estimation and Asymptotically Equal Interpolations in Model-Based Geostatistics.” Journal of the American Statistical Association 99 (465): 250–61."
  },
  {
    "objectID": "03_model-fitting.html#footnotes",
    "href": "03_model-fitting.html#footnotes",
    "title": "3  Model formulation and parameter estimation",
    "section": "",
    "text": "The letter \\(e\\) stands for the so called Euler’s number and represents the base of the natural logarithm. In the book, we write \\(\\log(\\cdot)\\) to mean the “natural logarithm of \\(\\cdot\\)”.↩︎"
  },
  {
    "objectID": "01_intro.html#objectives-of-this-book",
    "href": "01_intro.html#objectives-of-this-book",
    "title": "1  Introduction",
    "section": "1.1 Objectives of this book",
    "text": "1.1 Objectives of this book\nThe overall aim of this book is to provide you with the skills to perform a geostatistical analysis of a data-set using the R software environment. As you work your way through the book, you will learn to:\n\nexplore geostatistical data-sets using graphical procedures and summary statistics;\nformulate and fit geostatistical models using the maximum likelihood estimation method;\ncarry out prediction of health outcomes at different spatial scales;\nvisualize and interpret the results from geostatistical models;\nmodel the relationships between spatially referenced risk factors and the health outcome of interest;\nvalidate the assumptions of geostatistical models and assess their predictive performance.\n\nAlthough the focus of this book is on public health, the statistical ideas, as well as the software used, can also be applied for the analysis of geostatistical data-sets arising from other scientific fields."
  },
  {
    "objectID": "01_intro.html#pre-requisites-for-using-this-book",
    "href": "01_intro.html#pre-requisites-for-using-this-book",
    "title": "1  Introduction",
    "section": "1.2 Pre-requisites for using this book",
    "text": "1.2 Pre-requisites for using this book\nTo effectively understand and use the material presented in this book, it is expected that you should possess prior knowledge of basic probability theory, foundational topics in statistical modelling and R programming. Below we provide a more detailed explanation of the pre-requisites for each of these three fields.\n\n1.2.1 Topics in probability\nBasics probability theory is important to fully understand the content of this book. In particular, you should have knowledge of: the general definition and properties of continuous and discrete distribution; how the describe the properties of probability distributions through their mean, variance and skeweness; the concepts of stochastic dependence and correaltion; the distinction between marginal and conditional distributions; the basic properties of the Gaussian, Binomial and Poisson distributions; the definition and properties of the multivariate Gaussian distribution. The redear can find an extensive explanation and illustrations with examples of all these topics in Ross (2013).\n\n\n1.2.2 Topics in statistics\nLikelihood-based inference (whether frequensist or Bayesian) provides the theoretical bedrock for the estimation of almost any statistical model. In this book will focus on maximum likelihood estimation methods of inference. Extensive use of the notions of point and interval estimates obtained using the maximum likelihood estimation methods will be made through the book. Recommended readings include chapters 1, 2 and 4 of Pawitan (2001).\nGood prior knowledge of Generalized linear models (GLMs) is essential, as the geostatistical modelling framework builds on these as an extension. Before embarking on the use of this book, we thus encourage you to review the basic theory of GLMs and, in particular, how these are applied and interpreted. In this book, we will cover examples that will model continuously measured outcomes and counts. Hence, good understanding of linear regression modelling and modelling of counts data using Binomial and Poisson regression should be the main focus of the review. For comprehensive overview of GLMs and their implementation in R, we refer you to Dobson and Barnett (2008).\n\n\n1.2.3 Topics in R programming\nAlthough this book does not require to possess advanced skills in R programming, it is important you have good knowledge in the following topics: creation and manipulation of vectors and matrices; logical vectors; character vectors; handling of lists and data frame objects; reading data into R; graphical procedures. A very large amount of freely available material covering these topics can be found online. Our recommendation is to start from the manual “An introduction to R” of the Comprehensive R Archive Network available at this link, available at R manual."
  },
  {
    "objectID": "01_intro.html#obtaining-and-running-the-r-packages",
    "href": "01_intro.html#obtaining-and-running-the-r-packages",
    "title": "1  Introduction",
    "section": "1.3 Obtaining and running the R packages",
    "text": "1.3 Obtaining and running the R packages\nIt is advised that you obtain the latest 64-bit version of R in order to run the R code of this book. To install R, go to the R website, where you can download the installer packages for Windows and Mac, and find instructions for Linux, using binary files.\n\nWindows\nMac\nLinux\n\nThe list of the R packages used in this book is provided in Table 1.1.\n\n\nTable 1.1: List of the R packages that will be used in the book with a description of their use in the data analysis. The packages marked by (E) are essential for the geostatistical analysis. Those instead marked by (R) are recommended and can be helpful to overcome issues as described under the column “Used for”.\n\n\n\n\n\n\nR packages\nUsed for\n\n\n\n\nRiskMap (E)\nEstimating of geostatistical models and spatial prediction\n\n\nsf (E)\nHandling of spatial data in R\n\n\nterra (E)\nHandling of raster files in R\n\n\nggplot2 (E)\nCreating maps and exploratory plots\n\n\ncrsuggest (R)\nGuessing a coordinate reference systems when unknown\n\n\n\n\nTo install packages in R for the first time, you can use the command install.packages in the R console, as shown below for the RiskMap package.\n\ninstall.packages(\"RiskMap\")"
  },
  {
    "objectID": "01_intro.html#sec-examples-ch1",
    "href": "01_intro.html#sec-examples-ch1",
    "title": "1  Introduction",
    "section": "1.4 Data-sets used in the book",
    "text": "1.4 Data-sets used in the book\nThe geostatistical data-sets described in this section will be used throughout the book to illustrate the use of the R packages mentioned in the previous sections.\nEach of the examples data-sets can be loaded from the RiskMap package, using the command\n\ndata(NAME_OF_THE_DATASET)\n\nwhere in place of NAME_OF_THE_DATASET you should type of the name of one of the data-sets listed in Table 1.2.\n\n\nTable 1.2: List of data-sets available from the RiskMap package. Data-sets listed as “Example” are used throughout the book to illustrate the use of R functions. Data-sets listed as “Case study” are analysed in Chapter 6.\n\n\n\n\n\n\n\nNames of the data-set\nShort description\nUsed in this book as\n\n\n\n\ngalicia\nLead concentration m from moss samples collected in Galicia, Northern Spain\nExample\n\n\nliberia\nPrevalence data on river-blindness from Liberia\nExample\n\n\nmalkenya\nMalaria prevalence data from a community and school survey conducted in Western Kenya\nExample\n\n\nitaly_sim\nSimulated geostatistical data-set within the Italian national boundaries\nExample\n\n\nmalnutrition\nData on stunting among children in Ghana\nCase study\n\n\n\n\n\n1.4.1 Lead concentration in Galicia\n\n\n\n\n\n\nFigure 1.1: Data on the meausred lead concentration (in micrograms per gram dry weight) in moss samples collected in Galicia, North-West of Spain.\n\n\n\nLead is a heavy metal which, in high concentrations, can cause chronic damage to living organisms over a long period of time. For this reason its spread and source must be regularly monitored. To assess the extent of the contamination in an area, measurements of lead are often taken from plants. The data here considered (Figure 1.1) consist of 132 locations of moss samples collected in 2000, in and around Galicia, a region in the North-Western part of Spain. One of the objectives of this survey was to establish the spatial pattern of lead concentration in Galicia so as to better identify possible sources of contamination; fore more information, see Fernández, Rey, and Carballeira (2000).\nIn this case, geostatistical modelling can be used to predict the lead concentration across Galicia and allows to disentangle variation which is purely random, possibly due to measurement error, and genuine spatial variation, which is our main object of interest.\nThis data-set will be used in this book to show how to carry out the spatial analysis of continuously measured variables using linear geostatistical models.\n\n\n1.4.2 River-blindness in Liberia\n\n\n\n\n\n\nFigure 1.2: River-blindness data from a cross-sectional survey carried out in Liberia.\n\n\n\nIn low-resource settings, where disease registries are typically absent, cross-sectional surveys are an essential monitoring tool that enables the estimation of the disease burden in a population of interest. The data considered in this example (Figure 1.2) have been collected as part of an Africa-wide initiative called the Rapid Epidemiological Mapping of Onchocerchiasis (REMO) carried out in 2011 in 20 African countries (Zouré et al. 2014). The goal of REMO is to identify areas where river-blindness (or onchocerchiasis), a disease transmitted by black flies who breed along fast flowing rivers, is still a public health problem. In this context, it is especially of interest to identify communities with a prevalence above 20% and for treatment is urgently needed.\nIn this book, we will use data collected from Liberia to model nodule prevalence, which is based on a alternative and cheaper diagnostic technique for river-blindness. In the analysis of this data-set, we will illustrate how to formulate and fit Binomial geostatistical models, and how these can be used to predict prevalence within a region of interest.\n\n\n1.4.3 Malaria in the Western Kenyan Highlands\n\n\n\n\n\n\nFigure 1.3: Malaria prevalence data from a cross-sectional survey carried out in Nyanza Province, in the Western Highlands of Kenya.\n\n\n\nMalaria is one of deadliest diseases that affects populations living in tropical and subtropical countries. It is caused by a parasite of the genus Plasmodium which is transmitted through the infectious bite of female Anopheles mosquitoes. In the following chapters, we shall analyse a data-set from a cross-sectional community survey carried out in July 2010 in Nyanza Province, in the Western Highlands of Kenya (Stevenson 2013).\nWhat distinguishes this from the other examples data-sets is that the data contain both individual-level and household-level information. The outcome of interest is the result from a rapid diagnostic test for malaria which. In the book, we will illustrate how to account for the the hierarchical structure of the data and the binary nature of the outcome at each of the stages of the geostatistical analysis.\n\n\n1.4.4 Anopheles gambiae mosquitoes in Southern Cameroon\n\n\n\n\n\n\nFigure 1.4: Map of the collected number of Anopheles gambiae mosquitoes in an area of Southern Cameroon.\n\n\n\nIn studies of vector-borne and zoonotic diseases, understanding of the vector distribution can help to better guide the decision-making process for the implementation, monitoring and evaluation of control programmes. Anopheles gambiae mosquitoes are one of the main vectors for malaria transmission in sub-Saharan Africa. Their distribution over space is affected by several environmental and climatic factors, including temperature, humidity and vegetation.\nThe data-set on mosquitoes (Figure 1.4) that will use in the book consists of a sub-set taken from a large database (Tene Fossog et al. 2015). This was assembled in order to understand how the environment affects the distribution of different species of Anopheles mosquitoes in sub-Saharan Africa. This example data-set will be used to illustrate the application of Poisson geostatistical models for mapping mosquitoes abundance.\n\n\n1.4.5 Simulated-dataset"
  },
  {
    "objectID": "01_intro.html#sec-geostat-models",
    "href": "01_intro.html#sec-geostat-models",
    "title": "1  Introduction",
    "section": "1.5 Geostatistical problems and geostatistical models",
    "text": "1.5 Geostatistical problems and geostatistical models\nWhat the examples of the previous section have in common is that, in each case, the goal of statistical analysis is to draw inferences on an unobserved spatially continuous surface using data collected from a finite set of locations. The lead concentration in Galicia, the prevalence for river-blindness in Liberia and the abundance of A. gambiae mosquitoes in Cameroon can all be represented as spatially continuous processes that originate from the combined effects of environmental factors. We denote this class of inferential problems as geostatistical problems for which a solution can be found through the development and application of suitable geostatistical models, which are the subject of this book.\nAs one can soon realize, geostatistical problems are not unique to global health but arise in many other fields of science, including economics, physics, biology, geology and others. It thus comes to no surprise that geostatistics was initially developed in the South African mining industry in the 1950s (Krige 1951). This was then further developed as a self-contained discipline by Georges Matheron and other researchers at Fontainebleau, in France (Matheron 1963; Chilès and Delfiner 2016). In Watson (1971) and Watson (1972) a first connection is drawn between geostatistics and the prediction of stochastic processes. However, it is only with Ripley (1981) and then Cressie (1991) that geostatistics is explicitly brought into a classical statistical framework for the analysis of spatially referenced data. Diggle, Tawn, and Moyeed (1998) coined the term model-based geostastics and introduced this as belonging to the general class of generalized linear mixed models (Breslow and Clayton 1993), while emphasizing the use of likelihood-based methods of inference. As in Diggle, Tawn, and Moyeed (1998), also in this book, we advocate the application of model-based geostistical models as a class of parametric statistical models on which inference can be carried out using either maximum likelihood estimation or Bayesian methods.\nMore precisely, our attention will be directed at the class of generalized linear geostatistical models, or GLGM. To formally specify this, we first define the random variables \\(S\\), a spatial stochastic process, and the random variable \\(Y= (Y_1, \\ldots, Y_n)\\) which correspond to the outcome observed at a set of locations \\(X = (x_1, \\ldots, x_n)\\). Let us use \\([A]\\) to denote “the distribution of the random variable \\(A\\)”. To formulate a GLGM, we should then specify the joint distribution of \\(S\\) and \\(Y\\), which we write as\n\\[\n[Y, S] = [S] [Y | S].\n\\tag{1.1}\\]\nOn the right-hand side of the equation above, we have factorized the joint distribution of \\(Y\\) and \\(S\\), as the product between the marginal distribution of \\(S\\) and the conditional distribution of \\(Y\\) given \\(S\\). Hence, the formulation of a GLGM can be break down into the tasks of formulating \\([S]\\) and \\([Y | S]\\).\nIn defining \\([S]\\), throughout the book, we shall assume that this is a zero-mean stationary and isotropic Gaussian process. In other words, these assumptions impose that the joint distribution of \\(S(X) = (S(x_1),\\ldots,S(x_n))\\), i.e. the process \\(S\\) at the sampled locations \\(x_1, \\ldots, x_n\\), is invariant with respect to rations and translations of the locations \\(X\\). In practical terms, the main implication of this is that, for any pair of locations \\(x_i\\) and \\(x_j\\) the correlation function \\(\\rho(\\cdot)\\) between \\(S(x_i)\\) and \\(S(x_j)\\) is purely a function of the Euclidean distance, \\(u_{ij}\\), between \\(x_i\\) and \\(x_j\\), i.e. \\[\n{\\rm cov}\\{S(x_i), S(x_j)\\} = \\sigma^2\\rho(u_{ij}),\n\\tag{1.2}\\]\nwhere \\(\\sigma^2\\) is the variance of \\(S(x)\\) for all \\(x\\). Below we provide more details on the type of covoriance functions that we consider in this book. Furthermore, the fact that we assume the process \\(S\\) to have mean zero is because this is process acts as a residual term in our modelling of \\(Y\\). This aspect will be reiterated several times in the following chapters, as it as important implications for the interpretation of the other components of a geostatistical model, as well understanding the results of the analysis.\nFinally, we model \\([Y | S]\\), i.e. the distribution of \\(Y\\) given \\(S\\), is modeled as a set of mutually independent distributions which belong the exponential family, as defined in classical generalized linear modelling framework (Nelder and Wedderburn 1972). It then follows that, we can write \\([Y | S]\\) as\n\\[\n[Y | S] = \\prod_{i=1}^n [Y_i | S(x_i)].\n\\tag{1.3}\\]\nThe final step then consists of specifying a distribution for \\([Y_i | S(x_i)]\\). Table 1.3 gives the range, mean and variance the three specifications for $[Y_i | S(x_i)]$$ which we will consider in this book. In Table 1.3, the canonical function, say \\(g(\\cdot)\\), denotes the natural transformation of the mean component \\(\\mu_i\\) that allows us to introduce both covariates and the spatial process \\(S(x_i)\\) into the model so as to explain the variation in \\(\\mu_i\\) as\n\\[\ng(\\mu_i) = d(x_i)^\\top \\beta + S(x_i).\n\\tag{1.4}\\]\nwhere \\(d(x_i)\\) is a vector of spatially referenced covariates with associated regression coefficients \\(\\beta\\). Finally, the quantity \\(m_i\\), which appears in the formulation of the Binomial and Poisson distributions, is an offset quantity and is used to account for the number of tests or the population size at a given location \\(x_i\\).\n\n\nTable 1.3: Type of outcomes \\(Y_{i}\\) considered in this book.\n\n\n\n\n\n\n\n\n\nDistribution\nRange of \\(Y_i\\)\nMean of \\([Y_i | S(x_i)]\\)\nVariance of \\([Y_i | S(x_i)]\\)\nCanonical link\n\n\n\n\nGaussian\n\\((-\\infty, +\\infty)\\)\n\\(\\mu_i\\)\n\\(\\tau^2\\)\n\\(g(\\mu_i) = \\mu_i\\)\n\n\nBinomial\n\\(1,\\dots,m_i\\)\n\\(m_i\\mu_i\\)\n\\(m_i\\mu_i(1-\\mu_i)\\)\n\\(g(\\mu_i) = \\log\\{ \\mu_i/(1-\\mu_i) \\}\\)\n\n\nPoisson\n\\(1,2,\\ldots,\\infty\\)\n\\(m_i\\mu_i\\)\n\\(m_i\\mu_i\\)\n\\(g(\\mu_i) = \\log\\{ \\mu_i \\}\\)\n\n\n\n\nBased on the formulation in (1.4), we can see that \\(S(x_i)\\) quantifies residual spatial effects on \\(\\mu_i\\) that have not been accounted for by the covariates \\(d(x_i)\\). In an ideal scenario, the covariates \\(d(x_i)\\) should explain all the spatial variation without the need for \\(S(x_i)\\). Although this unrealistic, in practice we may be able to most of the variation in \\(\\mu_i\\) through \\(d(x_i)\\) and, hence, reduce \\(S(x_i)\\) to a negligible component. In Chapter 2, we will show how a thorough exploratory analysis can help to understand whether we have come close to that ideal scenario or, if instead, we need the use of GLGM to model the data.\nThe model described in (1.4) can be seen as the most basic GLGM that can be used for a geostatistical analysis. As we will see in the analysis of some of the examples and, in Chapter 6, for the case studies, extensions of this model will be required to accommodate the intrinsic non-spatial random variation of the data which is not captured by the covariates.\nThe types of problems that statistical models are applied to can be distinguished into three main categories: prediction problems; explanatory problems; problems of hypothesis testing. Most of the times, geostatistical problems tend to fall under the first category, where the goal is make predictive inferences on the process \\(S(x)\\) at location \\(x\\), which is usually outside of the set of sampled locations. However, as will illustrate in the later chapters, geostatistical models play an important also in the other two types of problems. In particular, we will show that spatial correlation can have a substantial impact on the point estimates and standard errors for \\(\\beta\\). Hence, if the goal of the analysis is explain the relationship between a covariate \\(d(x)\\) with the mean component \\(\\mu\\).\n\n1.5.1 The Matern family of correlation functions\nThroughout the book, we shall consider the Matern (2013) family of correlation functions to model the spatial correlation of the Gaussian process \\(S(x)\\). This defined as \\[\n\\rho(u;\\phi,\\kappa) =\\{2^{\\kappa-1} \\Gamma(\\kappa)\\}^{-1} (u/\\phi)^\\kappa K_\\kappa(u/\\phi),\n\\tag{1.5}\\] where \\(\\phi&gt;0\\) and \\(\\kappa&gt;0\\) are parameters and \\(K_\\kappa(\\cdot)\\) is the modified Bessel function of the third kind of order \\(\\kappa\\). The parameters \\(\\phi\\) and \\(\\kappa\\) regulate how fast the spatial correlation decays to zero for increasing distance and the smoothness of the process, respectively. A special case of Matern family of correlation functions, which is obtained when \\(\\kappa=0.5\\), will be of particular relevance to the application considered in this book. This is the expeonential correlation function which we write as \\[\n\\rho(u;\\phi) = \\exp\\{-u/\\phi\\}.\n\\tag{1.6}\\]\nAnother special case, which we dot consider in this book but has often been used in machine learning applications, is the Gaussian correlation function obtain as a limiting case for \\(\\kappa \\to +\\infty\\) the possible smoothest process arising from the Matern family.\nTo better understand how \\(\\phi\\) and \\(\\kappa\\) affect the spatial correlation and the pattern of the spatial of the spatial surface, we now consider some examples.\n\n\n\n\n\nFigure 1.6: Examples of stationary and isotropic Matern correlation functions. Panel (a) shows three different correlations functions that have the same smothness parameter of \\(\\kappa=0.5\\), while varying the scale parameter \\(\\phi\\) over \\({0.05, 0.1, 0.2}\\). In panel (b) the scale of the spatial correlation \\(\\phi\\) is chosen so that each of the three functions reaches 0.05 at distance 0.3 (as also shown by the horizontal and vertical black dashed segments).\n\n\n\n\nFigure 1.6 shows six different Matern correlation functions. In panel (a), we have kept \\(\\kappa\\) fixed to 0.5 and varied \\(\\phi\\) over the values 0.05, 0.1 and 0.2. As expected, for larger values of \\(\\phi\\) the correlation function has a slower decay to zero. Panels (a) to (c), in Figure 1.7, show three realizations of a Gaussian process from each of these correlation functions. The mean of the Gaussian process was set to zero and variance to 1. We can observe that spatial correlations with larger scales are associated with longer spatial trends, whilst smaller scales exhibit a patchier pattern. This is because, as \\(\\phi\\) takes values that are closer to zero, the spatial surface will tend to show a less structured pattern and will revert towards its zero mean more rapidly.\nFinally, let us consider the correlation functions, shown in the panel (b) of Figure 1.6. Here, we have varied \\(\\kappa\\) over the values 0.5, 1.5 and 2.5, whilst \\(\\phi\\) has been fixed in order to force all three correlation functions to reach 0.05 for distance 0.3. In this way, we can better observe the effect of different values \\(\\kappa\\) on the spatial surface for process that have approximately the same range for the spatial correlation. In Figure 1.7, we observe three realizations from these correlation functions. We observe that the differences between the different surface are determined by the small spatial scale behaviour; \\(\\kappa = 0.5\\) correspond to a rougher and less regular spatial pattern, whilst \\(\\kappa=2.5\\) shows a smoothest surface of the three processes considered. These properties of the spatial surface are related to the so called differentiability of the the Guassian process, which determines its local behaviour. If you are interested in delving these theoretical aspects, we suggest reading Chapter 2 of Stein (1999).\n\n\n\n\n\nFigure 1.7: Simulated spatial surface using the three correltion functions shown in Figure 1.6. Panels (a), (b) and (c) correspond to the correlation functions from panel (a) in Figure 1.6 and in order these are: \\(\\phi = 0.05\\) and \\(\\kappa=0.5\\); (b) \\(\\phi=0.1\\) and \\(\\kappa=0.5\\); (c) \\(\\phi=0.2\\) and \\(\\kappa=0.5\\). Panels (c), (d) and (e) correspond to the correlation functions from panel (b) in Figure 1.6 and in order these are: \\(\\phi = 0.1\\) and \\(\\kappa=0.5\\); (b) \\(\\phi=0.063\\) and \\(\\kappa=1.5\\); (c) \\(\\phi=0.051\\) and \\(\\kappa=2.5\\).\n\n\n\n\nThe flexibility provided by the Matern correlation function in capturing different forms of spatial correlations has made one of, if not the most widely used correlation function in model-based geostatistics (Stein 1999). For this reason, in this book we will consider the Matern correlation function. We will consider estimation issues related to the Matern correlation in Chapter 3."
  },
  {
    "objectID": "01_intro.html#workflow-of-a-statistical-analysis-and-structure-of-the-book",
    "href": "01_intro.html#workflow-of-a-statistical-analysis-and-structure-of-the-book",
    "title": "1  Introduction",
    "section": "1.6 Workflow of a statistical analysis and structure of the book",
    "text": "1.6 Workflow of a statistical analysis and structure of the book\n\n\n\nFigure 1.8: Stages of a statistical analysis\n\n\nFigure 1.8 shows the different stages that will follow in carrying the geostatistical analysis of the examples introduced in Section 1.4. The exploratory analysis of the data is an essential first step that is used to understand the empirical associations between risk factors and the the health outcome of interest. In our case, this first stage is also used to justify the use of geostatistical models by questioning the underlying assumptions of standard generalized linear models. Based on the results obtained from the exploration of the data, we then formulate a suitable statistical model and estimate its parameters using likelihood based methods of inference. These also allows us to obtain uncertainty measures about the strength of associations of regression relationships and the other model parameters that define the shape of the spatial correlation in the data. Following the estimation of the model, we then proceed to validate its underlying assumptions using suitable diagnostics that assess whether the model can later be sufficiently trusted to represent the observed variation in the modelled outcome. At this stage, if the diagnostics checks yield results that indicate the incompatibility of the model with the data, we then back to the stage of model formulation and address the issues arisen from the validation stage. If instead, we do not find any evidence against the fitted model we can proceed to carry out sptatial prediction. At this stage, it is important to define suitable predictive targets that can help us to better answer the original research question and better assist the decision making process. The final step of visualization of uncertainty plays an important role in geostatistical analysis in order to convey the main findings of the study in an effective and easy-to-understand way for a wider audience which also consists of non-experts.\nIn the remainder of this book, each chapter focuses on a specific stage as shown in Figure 1.8. We treat visualization of uncertainty together with spatial prediction in Chapter 5.\nChapter 2 will provide an overview of how to handle saptial data in R, in particular raster and shape files. The skills learned in this chapter will be applied throughout the book, and will especially be useful in Chapter 5 and Chapter 6 for generating predictive maps of the modelled outcome.\nChapter 3 focuses on the model building process and estimation of geostatistical models. This chapter will show how to carry out initial exploratory analyses of the data to inform the formulation of suitable geostatistical models and how these can be fitted using maximum likelihood estimation methods.\nChapter 4 illustrated the use of methods that can be used to validate the assumptions and calibration of statistical models.\nChapter 5 shows how geostatistical models can be used to carry out spatial prediction of a health outcome of interest both on a spatially continuous and spatially aggregated scales.\nFinally, Chapter 6 presents the application of all the methods illustrated in the previous chapters to three additional data-sets. This chapter offers a summary of the content of book by putting together all the stages in the geostatistical analyses for each of the three case studies, and illustrates additional functionalities of the RiskMap R package not covered in the previous chapters.\n\n\n\n\nBreslow, N. E., and D. G. Clayton. 1993. “Approximate Inference in Generalized Linear Mixed Models.” Journal of the American Statistical Association 88: 9–25.\n\n\nChilès, J-P, and P. Delfiner. 2016. Geostatistics (Second Edition). Hoboken: Wiley.\n\n\nCressie, N. A. C. 1991. Statistics for Spatial Data. New York: Wiley.\n\n\nDiggle, P. J., J. A. Tawn, and R. A. Moyeed. 1998. “Model-Based Geostatistics.” Journal of the Royal Statistical Society: Series C (Applied Statistics) 47 (3): 299–350. https://doi.org/10.1111/1467-9876.00113.\n\n\nDobson, A. J., and A. Barnett. 2008. An Introduction to Generalized Linear Models. Third. Chapman; Hall/CRC.\n\n\nFernández, J. A, A Rey, and A Carballeira. 2000. “An Extended Study of Heavy Metal Deposition in Galicia (NW Spain) Based on Moss Analysis.” Science of The Total Environment 254 (1): 31–44. https://doi.org/10.1016/S0048-9697(00)00431-9.\n\n\nKrige, D. G. 1951. “A Statistical Approach to Some Basic Mine Valuation Problems on the Witwatersrand.” Journal of the Chemical, Metallurgical and Mining Society of South Africa 52: 119–39.\n\n\nMatern, B. 2013. Spatial Variation. Lecture Notes in Statistics. Springer New York. https://books.google.co.uk/books?id=HrbSBwAAQBAJ.\n\n\nMatheron, G. 1963. “Principles of Geostatistics.” Economic Geology 58: 1246–66.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear Models.” Journal of the Royal Statistical Society A 135: 370–84.\n\n\nPawitan, Yudi. 2001. In All Likelihood : Statistical Modelling and Inference Using Likelihood. Oxford ; New York: Clarendon Press : Oxford University Press.\n\n\nRipley, B. D. 1981. Spatial Statistics. New York: Wiley.\n\n\nRoss, Sheldon. 2013. First Course in Probability, a. 9th ed. Harlow: Pearson Education UK.\n\n\nStein, Michael L. 1999. Interpolation of Spatial Data Some Theory for Kriging. 1st ed. 1999. Springer Series in Statistics. New York, NY: Springer New York : Imprint: Springer.\n\n\nStevenson, Gillian H. AND Gitonga, Jennifer C. AND Stresman. 2013. “Reliability of School Surveys in Estimating Geographic Variation in Malaria Transmission in the Western Kenyan Highlands.” PLOS ONE 8 (10). https://doi.org/10.1371/journal.pone.0077641.\n\n\nTene Fossog, Billy, Diego Ayala, Pelayo Acevedo, Pierre Kengne, Ignacio Ngomo Abeso Mebuy, Boris Makanga, Julie Magnus, et al. 2015. “Habitat Segregation and Ecological Character Displacement in Cryptic African Malaria Mosquitoes.” Evolutionary Applications 8 (4): 326–45. https://doi.org/10.1111/eva.12242.\n\n\nWatson, G. S. 1971. “Trend -Surface Analysis.” Mathematical Geology 3: 215–26.\n\n\n———. 1972. “Trend Surface Analysis and Spatial Correlation.” Geological Society of America Special Paper 146: 39–46.\n\n\nZouré, Honorat GM, Mounkaila Noma, Afework H Tekle, Uche V Amazigo, Peter J Diggle, Emanuele Giorgi, and Jan HF Remme. 2014. “Geographic Distribution of Onchocerciasis in the 20 Participating Countries of the African Programme for Onchocerciasis Control: (2) Pre-Control Endemicity Levels and Estimated Number Infected.” Parasites & Vectors 7 (1): 326–26."
  },
  {
    "objectID": "02_handling-spatial-data.html#importing-and-processing-spatial-data-in-r",
    "href": "02_handling-spatial-data.html#importing-and-processing-spatial-data-in-r",
    "title": "2  Handling of spatial data in R",
    "section": "2.1 Importing and processing spatial data in R",
    "text": "2.1 Importing and processing spatial data in R"
  },
  {
    "objectID": "02_handling-spatial-data.html#visualizing-geostatistical-data",
    "href": "02_handling-spatial-data.html#visualizing-geostatistical-data",
    "title": "2  Handling of spatial data in R",
    "section": "2.2 Visualizing geostatistical data",
    "text": "2.2 Visualizing geostatistical data"
  },
  {
    "objectID": "02_handling-spatial-data.html#section",
    "href": "02_handling-spatial-data.html#section",
    "title": "2  Handling of spatial data in R",
    "section": "2.3 ",
    "text": "2.3"
  },
  {
    "objectID": "04_model-validation.html#how-to-simulate-geostatistical-data-from-a-fitted-model",
    "href": "04_model-validation.html#how-to-simulate-geostatistical-data-from-a-fitted-model",
    "title": "4  Model validation",
    "section": "4.1 How to simulate geostatistical data from a fitted model",
    "text": "4.1 How to simulate geostatistical data from a fitted model"
  },
  {
    "objectID": "04_model-validation.html#validating-the-calibration-of-the-model",
    "href": "04_model-validation.html#validating-the-calibration-of-the-model",
    "title": "4  Model validation",
    "section": "4.2 Validating the calibration of the model",
    "text": "4.2 Validating the calibration of the model"
  },
  {
    "objectID": "04_model-validation.html#validating-the-spatial-correlation-of-the-model",
    "href": "04_model-validation.html#validating-the-spatial-correlation-of-the-model",
    "title": "4  Model validation",
    "section": "4.3 Validating the spatial correlation of the model",
    "text": "4.3 Validating the spatial correlation of the model"
  },
  {
    "objectID": "05_geostatistical-prediction.html#pixel-level-predictive-targets",
    "href": "05_geostatistical-prediction.html#pixel-level-predictive-targets",
    "title": "5  Geostatistical prediction",
    "section": "5.1 Pixel-level predictive targets",
    "text": "5.1 Pixel-level predictive targets"
  },
  {
    "objectID": "05_geostatistical-prediction.html#area-level-predictive-targets",
    "href": "05_geostatistical-prediction.html#area-level-predictive-targets",
    "title": "5  Geostatistical prediction",
    "section": "5.2 Area-level predictive targets",
    "text": "5.2 Area-level predictive targets"
  },
  {
    "objectID": "05_geostatistical-prediction.html#comparing-the-predictive-performance-of-geostatistical-models",
    "href": "05_geostatistical-prediction.html#comparing-the-predictive-performance-of-geostatistical-models",
    "title": "5  Geostatistical prediction",
    "section": "5.3 Comparing the predictive performance of geostatistical models",
    "text": "5.3 Comparing the predictive performance of geostatistical models"
  },
  {
    "objectID": "06_case-studies.html#mapping-stunting-risk-in-ghan",
    "href": "06_case-studies.html#mapping-stunting-risk-in-ghan",
    "title": "6  Case studies",
    "section": "6.1 Mapping stunting risk in Ghan",
    "text": "6.1 Mapping stunting risk in Ghan"
  },
  {
    "objectID": "06_case-studies.html#mapping-river-blindness-in-malawi",
    "href": "06_case-studies.html#mapping-river-blindness-in-malawi",
    "title": "6  Case studies",
    "section": "6.2 Mapping river blindness in Malawi",
    "text": "6.2 Mapping river blindness in Malawi"
  },
  {
    "objectID": "06_case-studies.html#mapping-mosquitoes-abundance-in-cameroon",
    "href": "06_case-studies.html#mapping-mosquitoes-abundance-in-cameroon",
    "title": "6  Case studies",
    "section": "6.3 Mapping mosquitoes abundance in Cameroon",
    "text": "6.3 Mapping mosquitoes abundance in Cameroon"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015.\n“Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical\nSoftware 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBowman, A. W. 1997. Applied Smoothing Techniques for Data Analysis :\nThe Kernel Approach with s-Plus Illustrations. Oxford Statistical\nScience Series ; 18. Oxford : New York: Clarendon Press ; Oxford\nUniversity Press.\n\n\nBreslow, N. E., and D. G. Clayton. 1993. “Approximate Inference in\nGeneralized Linear Mixed Models.” Journal of the American\nStatistical Association 88: 9–25.\n\n\nChilès, J-P, and P. Delfiner. 2016. Geostatistics (Second\nEdition). Hoboken: Wiley.\n\n\nCressie, N. A. C. 1991. Statistics for Spatial Data. New York:\nWiley.\n\n\nDiggle, P. J., J. A. Tawn, and R. A. Moyeed. 1998. “Model-Based\nGeostatistics.” Journal of the Royal Statistical Society:\nSeries C (Applied Statistics) 47 (3): 299–350. https://doi.org/10.1111/1467-9876.00113.\n\n\nDobson, A. J., and A. Barnett. 2008. An Introduction to Generalized\nLinear Models. Third. Chapman; Hall/CRC.\n\n\nFernández, J. A, A Rey, and A Carballeira. 2000. “An Extended\nStudy of Heavy Metal Deposition in Galicia (NW Spain) Based on Moss\nAnalysis.” Science of The Total Environment 254 (1):\n31–44. https://doi.org/10.1016/S0048-9697(00)00431-9.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2001. The\nElements of Statistical Learning. Springer Series in Statistics.\nNew York, NY, USA: Springer New York Inc.\n\n\nKatz, Elizabeth, and Bill & Melinda Gates Foundation. 2020.\n“Gender and Malaria Evidence Reivew.” Bill & Melinda\nGates Foundation. https://www.gatesgenderequalitytoolbox.org/wp-content/uploads/BMGF_Malaria-Review_FC.pdf.\n\n\nKrige, D. G. 1951. “A Statistical Approach to Some Basic Mine\nValuation Problems on the Witwatersrand.” Journal of the\nChemical, Metallurgical and Mining Society of South Africa 52:\n119–39.\n\n\nMatern, B. 2013. Spatial Variation. Lecture Notes in\nStatistics. Springer New York. https://books.google.co.uk/books?id=HrbSBwAAQBAJ.\n\n\nMatheron, G. 1963. “Principles of Geostatistics.”\nEconomic Geology 58: 1246–66.\n\n\nNelder, J. A., and R. W. M. Wedderburn. 1972. “Generalized Linear\nModels.” Journal of the Royal Statistical Society A 135:\n370–84.\n\n\nPawitan, Yudi. 2001. In All Likelihood : Statistical Modelling and\nInference Using Likelihood. Oxford ; New York: Clarendon Press :\nOxford University Press.\n\n\nRipley, B. D. 1981. Spatial Statistics. New York: Wiley.\n\n\nRoss, Sheldon. 2013. First Course in Probability, a. 9th ed.\nHarlow: Pearson Education UK.\n\n\nSmith, David L, Carlos A Guerra, Robert W Snow, and Simon I Hay. 2007.\n“Standardizing Estimates of the Plasmodium Falciparum Parasite\nRate.” Malaria Journal 6 (1): 131–31.\n\n\nStein, Michael L. 1999. Interpolation of Spatial Data Some Theory\nfor Kriging. 1st ed. 1999. Springer Series in Statistics. New York,\nNY: Springer New York : Imprint: Springer.\n\n\nStevenson, Gillian H. AND Gitonga, Jennifer C. AND Stresman. 2013.\n“Reliability of School Surveys in Estimating Geographic Variation\nin Malaria Transmission in the Western Kenyan Highlands.”\nPLOS ONE 8 (10). https://doi.org/10.1371/journal.pone.0077641.\n\n\nTene Fossog, Billy, Diego Ayala, Pelayo Acevedo, Pierre Kengne, Ignacio\nNgomo Abeso Mebuy, Boris Makanga, Julie Magnus, et al. 2015.\n“Habitat Segregation and Ecological Character Displacement in\nCryptic African Malaria Mosquitoes.” Evolutionary\nApplications 8 (4): 326–45. https://doi.org/10.1111/eva.12242.\n\n\nTobler, W. R. 1970. “A Computer Movie Simulating Urban Growth in\nthe Detroit Region.” Economic Geography 46: 234–40.\n\n\nWatson, G. S. 1971. “Trend -Surface Analysis.”\nMathematical Geology 3: 215–26.\n\n\n———. 1972. “Trend Surface Analysis and Spatial\nCorrelation.” Geological Society of America Special\nPaper 146: 39–46.\n\n\nWeisberg, Sanford. 2014. Applied Linear Regression. Fourth.\nHoboken NJ: Wiley. http://z.umn.edu/alr4ed.\n\n\nZhang, Hao. 2004. “Inconsistent Estimation and Asymptotically\nEqual Interpolations in Model-Based Geostatistics.” Journal\nof the American Statistical Association 99 (465): 250–61.\n\n\nZouré, Honorat GM, Mounkaila Noma, Afework H Tekle, Uche V Amazigo,\nPeter J Diggle, Emanuele Giorgi, and Jan HF Remme. 2014.\n“Geographic Distribution of Onchocerciasis in the 20 Participating\nCountries of the African Programme for Onchocerciasis Control: (2)\nPre-Control Endemicity Levels and Estimated Number Infected.”\nParasites & Vectors 7 (1): 326–26."
  }
]