{
  "hash": "56232ddf75ea13bfcb6a4592a29bf1e4",
  "result": {
    "markdown": "---\nfilters:\n  - line-highlight\nexecute: \n  freeze: auto\nfig-width: 5\nfig-asp: 0.75\n---\n\n\n# Geostatistical prediction {#sec-geo-prediction}\n\n## List of the main functions used in the chapter {.unnumbered}\n\n| Function           | R Package | Used for                                                                                                              |\n|-------------------------|------------------|-----------------------------|\n| `pred_over_grid`   | `RiskMap` | Predicting the different component of the linear predictor over a grid: the covariates effects and the random effects |\n| `pred_target_grid` | `RiskMap` | Predictions of target defined at pixel-level                                                                          |\n| `pred_target_shp`  | `RiskMap` | Predictions of targets defined at areal level                                                                         |\n| `assess_pp`        | `RiskMap` | Assessing the predictive performance of geostatistical models                                                         |\n\n## Introduction\n\nGeostatistics was originally developed and applied as a predictive tool for improving mining operations and planning. In the seminal paper on model-based geostatistics (MBG) [@diggle1998], the authors begin by stating:\n\n> \\*\"Conventional geostatistical methodology solves the problem of predicting the realized value of a linear functional of a Gaussian spatial stochastic process\\* $S(x)$ based on observations $Y_i= S(x_i) + Z_i$ at sampling locations $x_i$, where the $Z_i$ are mutually independent, zero-mean Gaussian random variables.\"\n\nIt is no coincidence that, although MBG represents a broad class of statistical models applicable to both estimation and prediction problems, the majority of its applications have focused on prediction. This is evident in many scientific fields where MBG has been used, including global public health, which is the primary application focus in this book. In line with the quote above, for most of the examples considered in this book, the analyses we illustrate aim to use a finite set of locations $x_i$ --- typically corresponding to households or villages --- to make inferences about an observed disease risk surface. Having performed estimation of MBG models in @sec-estimation, we can now carry out spatial prediction to answer critical public health questions, such as identifying hot-spots and cold-spots --- areas of unusually high or low disease risk, respectively --- or determining whether the average prevalence across a region exceeds a predefined policy threshold.\n\nIn this chapter, we will explore these issues, beginning with a formal statistical formulation of the spatial prediction problem and a description of the necessary steps for its implementation. We will then demonstrate how spatial prediction is performed when considering predictive targets at both the *pixel-level* and *areal-level*. Finally, we will conclude the chapter by presenting methods for comparing the predictive performance of different models, with some insights into their relative strengths and weaknesses.\n\n## Spatial prediction using geostatistical models {#sec-spat-pred-target}\n\nLet us consider the class of generalized linear geostatistical models (GLGM) as in @sec-geostat-models. In the formulation of prediction problems, the first step consists of defining our predictive target, which we denote as $T(x)$ where $x$ denotes any locations within the study, which is usually not part of the sampled data. For example in the analysis of the riverblindness in Liberia, our interest lies in identifying areas where disease prevalence exceeds 20$\\%$, a threshold that use been used in the past to identify areas in need of mass drug administration due to the potential for significant disease burden and transmission [@amazigo2008]. Hence, in this case, disease prevalence corresponds to our predictive target $T(x)$. To draw inferences on $T(x)$ and allow us to answer the question of where $T(x)$ exceeds a 20$\\%$ threshold, we first need to obtain the so called *predictive distribution* of $T(x)$. More specifically, the *predictive distribution* of $T(x)$ is the distribution of $T(x)$ conditioned to the data, which we denote by $[T(x) \\: \\mid \\: y]$. More importantly, by incorporating the information provided by the data, the predictive distribution of the target enables us to quantify the uncertainty stemming from the stochastic nature of the process we are modeling. In other words, the predictive distribution reflects the range of possible values that the target $T(x)$ can take and their likelihoods based on the model we have fitted to the data. As we shall see in our examples, we can use the predictive distribution of $T(x)$ both to provide what is our \"best guess\" for $T(x)$ and a summary of uncertainty which quantifies how much concentrated is the predictive distribution around that guess. However, providing a single \"best guess\" for $T(x)$ is not always the answer to our research question. In the example on riverblindness mentioned at the start of the this section, a more natural way to use the predictive distribution would be to compute the likelihood that $T(x)$ exceeds 0.2 at any given location $x$.\n\nIn summary, the first two steps of spatial prediction are:\n\n-   Step 1. define the predictive target $T(x)$;\n\n-   Step 2. obtain the predictive distribution of $T(x)$ and use this to compute summaries that helps to answer your original research question.\n\nThis begs two practical questions: \"How do we obtain the predictive distribution of $T(x)$?\" and \"How do we use the predictive distribution to compute our summaries?\". For a mathematical derivation of the predictive distribution and the form that this takes in the special case of a linear geostatistical model, you can read @sec-theory-prediction. In what follows, we shall assume that we can simulate directly from $[T(x) \\: \\mid \\: y]$ either using direct simulation or Markov Chains Monte Carlo (see @sec-mcmc-mala).\n\nIn our exposition so far, we have also made the implicit assumption that prediction is required for our predictive target at a single, unsampled, location $x$ and for this reason we have denote our predictive target simply as $T(x)$. However, this is almost never the case, since in most cases, our predictive target correspond to either of the following.\n\n-   **Spatially continuous targets** correspond to an unobserved spatially continuous surface (e.g. disease prevalence), formally denoted by $\\mathcal{T} = \\{T(x), x \\in A\\}$, where $A$ is our region of interest.\n\n-   **Areal-level targets**, usually defined as transformation of the spatially continuous surface $\\mathcal{T}$, defined previously, and which we denote as $\\mathcal{T}_{A} = F(\\mathcal{T}, A)$. For example, the average of $T(x)$ over $A$ is an areal-level target, formally defined as $F(\\mathcal{T}, A) = \\int_{A} T(x) \\: dx / |A|$, where $|A|$ is the area of $A$.\n\nBefore we consider these two types of prediction targets more in detail, we first explain the preliminary steps that need to be performed and are common to both. Whether our goal is predict a spatially continuous or areal-level target, the initial task is to draw Monte Carlo samples from the predictive distribution of the spatial process $S(x)$[^05_geostatistical-prediction-1]. Next, we define a set of prediction locations, say $q$ in total, denoted as $\\tilde{X} = {\\tilde{x}_1, \\ldots, \\tilde{x}_q}$. These locations correspond to a regular grid that spans our region of interest, $A$. the predictive distribution of the spatial process over the grid, represented as $[S(\\tilde{X}) \\: |\\: y]$. This distribution allows us to generate Monte Carlo samples for $S(\\tilde{X})$, which will later be used for computing our target.\n\n[^05_geostatistical-prediction-1]: Here, we are overlooking the fact that when $T(x)$ is a linear transformation of $S(x)$ and the the model fitted to the data is a linear geostatistical model, then we can derive any summary of the predictive distribution analytically, without the need of carrying out any simulation. Although this may displease some of our fellow statisticians who care about computational efficiency, for pedagogical reasons, here we have chosen to explain only the Monte Carlo based approach, which works in all scenarios.\n\nAt this stage, we need to consider which of the two following types of predictions from $[S(\\tilde{X}) \\: |\\: y]$ are required.\n\n-   **Marginal predictions** are obtained by simulating independently from the $q$ marginal predictive distributions of $[S(\\tilde{X}) | y]$. In other words, we consider a prediction location $\\tilde{x}_j$, simulate from $[S(\\tilde{x}_j) \\: |\\: y]$ say $B$ samples, and repeat this for $j=1,\\ldots,q$\n\n-   **Joint predictions** are obtained by simulating from the joint distribution of $[S(\\tilde{X}) \\: |\\: y]$. Unlike marginal predictions, joint predictions take into account the correlation between the different components of $[S(\\tilde{X}) \\: |\\: y]$.\n\nTo better understand the technical differences between marginal and joint predictions, we refer you to @sec-pred-distr-theory. However, in practice, the two following facts are important.\n\n-   *Fact 1*. Joint predictions are computationally more intensive than marginal predictions.\n\n-   *Fact 2*. Joint predictions are required when the predictive target is at areal-level.\n\n-   *Fact 3*. For spatially continuous targets, both marginal and joint predictions can be used but, in light of *Fact 1*, we might prefer using the former.\n\n### Generating samples from the predictive distribution of $S(\\tilde{X})$ (continue from @sec-liberia-estim1) {#sec-pred-samples}\n\nWe now show how the ideas and concepts discussed above are put into practice in an geostatistical analysis, using the Liberia data example. Using the fitted model in @sec-liberia-estim1, we use the `predict_over_grid` function in `RiskMap`, to sample from the predictive distribution of the spatial Gaussian process $S(x)$ based on a regular grid spanning the country of Liberia.\n\nThe `pred_over_grid` function enables us to predict the the separate effects of the covariates, given by $d(x)^\\top \\beta$, and the spatial random effects, $S(x)$, at any desired location $x$. Hence, before we can use the `pred_over_grid` function, we need to do 1) obtain a shape file that defines the boundaries of our study area, 2) use that to create a regular grid and 3) extract the covariates values at those locations. The R script below performs all these steps.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1) Obtaining Liberia boundaries \nlibrary(rgeoboundaries)\nliberia_adm0 <- geoboundaries(\"liberia\", adm_lvl = \"adm0\")\nliberia_adm0 <- st_transform(liberia_adm0, crs = 32629)\n\n# 2) Create the grid at 5 km resolution\nliberia_grid <- create_grid(liberia_adm0, spat_res = 5)\n\n# Download elevation data\nlibrary(elevatr)\nliberia_elev <- get_elev_raster(locations = liberia_adm0, \n                                z = 5, clip = \"locations\")\n\n# 3) Extracting elevation at the grid locations\nlb_predictors <- data.frame(elevation = \n                         terra::extract(liberia_elev,\n                                st_coordinates(liberia_grid)))\n```\n:::\n\n\nIn the code above, we generated a prediction grid --- which, we recall, that we denote as $\\tilde{X}$ --- with a spatial resolution of 5 km. Generally, a higher spatial resolution (note that a \"higher spatial resolution\" corresponds to a lower value for `spat_res`) yields more detailed prediction maps but also increases the computational cost. In this case, the choice of a 5 km grid strikes a balance between sufficient visualization clarity and manageable computation time.\n\nAt this point, we have all the necessary inputs to execute the `pred_over_grid` function for both marginal and joint predictions, as demonstrated below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Predicting the spatial process S(x) over the grid, as well as\n# covariates effects\n\n# Marginal predictions\nlb_pred_S_m <- pred_over_grid(fit_liberia, \n                            grid_pred = liberia_grid, \n               predictors = lb_predictors, messages = FALSE,\n               type = \"marginal\")\n\n# Joint predictions\nlb_pred_S_j <- pred_over_grid(fit_liberia, \n                            grid_pred = liberia_grid, \n               predictors = lb_predictors, messages = FALSE,\n               type = \"joint\")\n```\n:::\n\n\nBoth `lb_pred_S_m` and `lb_pred_S_j` contain lists with the same arguments, with one key difference: the samples in the list element named `S_samples` for the former are drawn from the marginal predictive distributions of the components of $S(\\tilde{X})$, while for the latter, they come from the joint predictive distribution of $S(\\tilde{X})$.\n\nAn important note is that in the code above we have used the default settings for the Markov Chain Monte Carlo (MCMC). To change these setting use the function `set_control_sim` whose output should be passed to the argument `contol_sim` of `pred_over_grid`. The diagnostic on the convergence of the MCMC can also be applying the `check_mcmc` function to either `lb_pred_S_m` or `lb_pred_S_j`; see @sec-summary for more details.\n\nWe can inspect the predictive distribution of any component $S(\\tilde{X})$ through the Monte Carlo samples stored in `S_samples`. For example, the script below generates the histogram for the predictive distribution of the first component of $S(\\tilde{X})$ (@fig-hist-first-comp-pd).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(lb_pred_S_m$S_samples[,1], main = \"Predictive distribution\",\n     xlab = expression(S(tilde(x)[1])))\n```\n\n::: {.cell-output-display}\n![Histrogram of the predictive distribution of the first component, $S(\\tilde{x}_1)$ of $S(\\tilde{X})$ for the Liberia data analysis.](05_geostatistical-prediction_files/figure-html/fig-hist-first-comp-pd-1.png){#fig-hist-first-comp-pd}\n:::\n:::\n\n\n## Spatially continuous targets\n\nIn the previous section, we explained and demonstrated how to obtain Monte Carlo samples from the predictive distribution of $S(\\tilde{X})$, which we denoted by $[S(\\tilde{X}) \\: |\\: y]$. Let $S_{(j)}(\\tilde{X})$ and $[S(\\tilde{X}) \\: |\\: y]$ represent the $j$-th Monte Carlo samples for the entire grid and the specific location $x_k$, respectively, for $j=1,\\ldots,B$ and $k=1,\\ldots,q$.\n\nWe define our predictive target, $T(x)$, at any given location $x$, as a transformation of $S(x)$. Thus, we express it as $T(x) = f\\{S(x)\\}$. Note that $f(\\cdot)$ can represent any type of non-linear transformation of $S(x)$ and may include covariate effects and other non-structured random effects. A list of common predictive targets frequently used in geostatistical analysis is provided in @tbl-spat-cont-pred-target.\n\n| Predictive target $T(x)$                                                  | Name of the predictive target | Generalized linear model (GLM) family |\n|----------------------------|----------------------|-----------------------|\n| $d(x)^\\top \\beta + S(x)$                                                  | Linear predictor              | Any GLM                               |\n| $\\frac{\\exp\\{d(x)^\\top \\beta + S(x)\\}}{1+\\exp\\{d(x)^\\top \\beta + S(x)\\}}$ | Prevalence                    | Binomial                              |\n| $\\exp\\{d(x)^\\top \\beta + S(x)\\}$                                          | Mean number of cases          | Poisson                               |\n| $S(x)$                                                                    | Spatial random effects        | Any GLM                               |\n| $d(x)^\\top \\beta$                                                         | Covariates effects            | Any GLM                               |\n\n: List of some of the most common spatially continuous predictive targets in a geostatistical analysis. For each target, we also indicate the generalized linear model (GLM) family for which these can be defined. Note that this list not exhaustive and different predictive other than those listed in the table could also be considered. {#tbl-spat-cont-pred-target}\n\nAn important feature of the predictive targets listed in @tbl-spat-cont-pred-target is that none of them include the nugget effect, denoted by $Z_i$ in our linear predictor definition for generalized linear geostatistical models. The reason for excluding $Z_i$ from the predictive targets is that, in most cases, it lacks a clear, unambiguous scientific interpretation. However, $Z_i$ might be included, for example, in environmental studies focusing on highly localized phenomena where measurement error or small-scale variability is of direct scientific interest; or in epidemiological studies where inferences at the individual level, rather than the population level, are required, and where $Z_i$ is introduced to account for unexplained individual-level variation. It is also worth noticing, the primary impact of including $Z_i$ in $T(x)$ would be an increase in the uncertainty of predictive inferences for $T(x)$, with minimal effect (or none at all, if $T(x)$ corresponds to the linear predictor) on the point predictions.\n\nThe predictive targets in @tbl-spat-cont-pred-target corresponding to $d(x)^\\top \\beta$ or the spatial random effects $S(x)$ might be considered when the goal is to highlight the different contribution to the overall predictive inferences from the measured covariates.\n\n### Example: mapping riverblidness prevalence in Liberia (continuing from @sec-pred-samples) {#sec-liberia-prediction-1}\n\nLet us continue the geostatistical of the riverblindness data in Liberia. The predictive target we consider is prevalence, hence $$T(x) = \\frac{\\exp\\{ \\beta_{0} + \\beta_{1}\\log\\{e(x)\\} + S(x)\\}}{1+\\exp\\{\\beta_{0} + \\beta_{1}\\log\\{e(x)\\} + S(x)\\}},$$ {#eq-rb-prev-target} where $e(x)$ is the elevation in meters at location $x$.\n\nThrough the `pred_target_grid` we can use the output in `lb_pred_S_m` (or `lb_pred_S_j` as well, in this case) to generate predictions of prevalence over the regular grid at 5 km we have created earlier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Prediction of riverblindness prevalence\nlb_prev <- \npred_target_grid(\n  lb_pred_S_m, \n  f_target = list(prev = function(lp) exp(lp)/(1+exp(lp))),\n  pd_summary = list(mean = function(Tx) mean(Tx), \n                    cv = function(Tx) sd(Tx)/ mean(Tx),\n                    exceed20 = function(Tx) mean(Tx > 0.2))\n)\n```\n:::\n\n\nIn the function above the argument `f_target` can take a list of functions, as multiple predictive targets can be defined. Here, we only specified prevalence (`prev`). Note that in defining the predictive target in `f_target`, we need to express that as a function of $d(x)^\\top \\beta + S(x)$. The argument `include_covariates` also allows us to define predictive target that do not use covariates from the model. By default `include_covariates = TRUE`, hence in the code above covariates are part of what is defined as `lp`. To predict the spatial random effects $S(x)$ only, for example, you can set `include_covariates = FALSE` and `f_target = list(Sx = function(lp) lp)`. In `pd_summary`, we define the summaries that we want to visualize from the predictive distribution. Here, we have specified the mean, coefficient of variation and the probability of exceeding a 20$\\%$ prevalence threshold.\n\nWe can then visualize the resulting maps as follows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Displying the results\npar(mfrow = c(1,3))\nplot(lb_prev, which_target = \"prev\", \n     which_summary = \"mean\", main = \"Mean\")\nplot(lb_prev, which_target = \"prev\", \n     which_summary = \"cv\", main = \"Coefficient of variation\")\nplot(lb_prev, which_target = \"prev\", \n     which_summary = \"exceed20\", main = \"Exceedance probability\")\npar(mfrow=c(1,1))\n```\n\n::: {.cell-output-display}\n![Mapping riverblindness in Liberia. Plots of the mean (left panel), coefficient of variation (central panel) and exceedance probability (right panel) for a 20% prevalence threshold.](05_geostatistical-prediction_files/figure-html/fig-lb-maps-1.png){#fig-lb-maps}\n:::\n:::\n\n\nThe maps in @fig-lb-maps indicate a higher prevalence as we move away from the coast and at higher altitudes. A very similar spatial pattern is observed in the likelihood of exceeding 20$\\%$ prevalence. Also, based on the map of the coefficient of variation, the uncertainty around the point predictions of prevalence is higher along the coast and lower in the inland areas of Liberia.\n\n### Example: mapping malaria using age and elevation as predictors {#sec-non-spar-covariates}\n\nWe now consider an example on malaria mapping, where we use two different types of covariates: a spatially referenced covariate corresponding to elevation; the individual age. We use the `malkenya_comm` subset of the `malkenya` data-set (see @sec-indiv-expl-analysis) and, to alleviate the computational burden, we further reduce this data-set by taking the first 1000 rows. Hence, we create our new data-set, `malkenya_comm1000`, using the followind code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmalkenya_comm1000 <- malkenya_comm[1:1000,]\n\nmalkenya_comm1000 <- st_as_sf(malkenya_comm1000, coords = c(\"Long\", \"Lat\"),\n                              crs = 4326)\nmalkenya_comm1000 <- st_transform(malkenya_comm1000, crs = 32736)\n```\n:::\n\n\nBased on the exploratory analysis shown in @sec-indiv-expl-analysis, we consider a geostatistical model fitted to the individual binary outcomes, $Y_{ij}$, which correspond the rapid diagnostic test (RDT) results for the $j$-th individual in the $i$-th community, and take value $Y_{ij}=1$ if the RDT is positive and $Y_{ij}=0$ if the RDT is negative. Using a Binomial geostatistical model, we model the individual probability, $p_{ij}$ of a positive RDT using the following linear predictor. $$\n\\log\\left\\{\\frac{p_{ij}}{1-p_{ij}}\\right\\} = \\beta_{0} + \\beta_{1}a_{ij}+\\beta_{2} \\times\\max\\{a_{ij}-15, 0\\} + \\beta_{3}\\max\\{a_{ij}-40, 0\\} + \\beta_{4} e(x_i) + S(x_i),\n$$ {#eq-lp-malkenya} where $e(x_{i})$ and $a_{ij}$ are the elevation in meter and the age in years for the $j$-th individual residing at the $i$-th household, respectively.\n\nWhen using the model in @eq-lp-malkenya to predict RDT prevalence, a key question arises: how should we handle the age variable, which, unlike elevation, is not available as a geo-referenced covariate at all locations in the study area? The answer depends on the research question being addressed.\n\nFor instance, if the goal is to infer disease risk across different age groups, we can generate different maps for each group of interest. This can be easily achieved by fixing the value of $a_{ij}$ to a specific age for all prediction locations. Alternatively, if the objective is to generate a predictive map for the general population, rather than for a specific age group, we must employ a probabilistic model for age and integrate out its effect.\n\nDeveloping a credible probabilistic model for age is beyond the scope of this section. Instead, we demonstrate a simple yet reasonable solution which uses the empirical age distribution from the data. By random sampling from this distribution, we can then assign age values to prediction locations. The steps are as follows.\n\n1.  Obtain the empirical distribution of age from the data.\n2.  Sample with replacement from such distribution (the `sample` function in R can be used for this purpose) as many times as the number of prediction locations, so that each grid location $\\tilde{x}$ has a value of age assigned.\n3.  Generate a sample from the predictive distribution of the target $T(\\tilde{x})$ at each of the grid locations $\\tilde{x}$.\n4.  Repeat 1 to 3, for as many times as the samples generates from $S(\\tilde{X})$.\n\nThis approach relies on two key assumptions. First, that the age distribution is spatially neutral, i.e., it does not vary across space. Second, that the data are representative of the age distribution in the target population. In the data used for this example, we believe these assumptions are reasonable, given the relatively small study area, where age is unlikely to exhibit significant spatial variation, and the fact that the data are a random sample from the community.\n\nIn the scripts presented in the remainder of this section, we show how to generate a predictive map of prevalence for children age 15 years, and another map that instead uses the approach earlier described to remove the effect of age and generate a map of prevalence for the general population.\n\nFirst, we fit the model. Note that the effect of age is not linear and we use linear splines to account for this, using the results of the exploratory analysis shown in \\@@sec-indiv-expl-analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_malkenya <- glgpm(RDT ~ Age +  pmax(Age-15, 0) + \n                        pmax(Age-40, 0) + elevation +\n                        gp(),\n                      data = malkenya_comm1000,\n                      family = \"binomial\")\n```\n:::\n\n\nAfter fitting the model, we create a prediction grid at a spatial resolution of 500 meters and extract the values of elevation. In this analysis, due to the small size of the study area, we do not use administrative boundaries as they are too large. Instead, we use the convex hull[^05_geostatistical-prediction-2] of the sample locations to define the boundaries of our study area.\n\n[^05_geostatistical-prediction-2]: The convex hull of a set of spatial locations is the smallest possible shape (usually a polygon) that completely encloses all the points, such that if you take any two points inside the shape, the straight line connecting them is also entirely inside the shape. In simple terms, it can be thought of as stretching a rubber band around the outermost points in a set, and the shape the rubber band forms is the convex hull.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create grid from convex hull\nshp_ch <- convex_hull_sf(malkenya_comm1000)\nken_grid <- create_grid(shp_ch, spat_res = 0.5)\n\n# Get elevation raster\nken_elev <- \nget_elev_raster(locations = shp_ch, \n                z = 9, clip = \"locations\")\n```\n:::\n\n\nWe then create the data frame of predictors where age is set to 15.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create the data fr\nken_predictors <- data.frame(elevation = \n                           extract(ken_elev, st_coordinates(ken_grid)),\n                         Age = 15)\n```\n:::\n\n\nWe now have all the ingredients to carry out prediction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_ken_S <- pred_over_grid(fit_malkenya, grid_pred = ken_grid,\n                             predictors = ken_predictors)\n\npred_age15 <- \npred_target_grid(pred_ken_S, \n                 f_target = list(prev = function(lp) exp(lp)/(1+exp(lp))),\n                 pd_summary = list(mean = function(Tx) mean(Tx)))\n```\n:::\n\n\nThe code below shows how to perform the 4 steps described above to generate a predictive map for the general population. Below we use the function `update_predictors` to update the covariates effects that are stored in `mu_pred`, a list element generated as the output of `pred_over_grid`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of Monte Carlo samples\nn_sim <- ncol(pred_age15$lp_samples)\n\n# Number of prediction locations\nn_pred <- nrow(predictors)\n\n# The create object `pred_ken_S_i` with the goal of changing the \n# coviariates effects stored in `mu_pred` according the sample values of age\npred_ken_S_i <- pred_ken_S\npred_ken_S_i$mu_pred <- matrix(NA, nrow = n_pred, ncol = n_sim)\n\nfor(i in 1:n_sim) {\n  # Generate n_pred samples from the empirical distribution of age\n  ken_predictors$Age <- sample(malkenya_comm1000$Age, n_pred, replace = TRUE)\n  # Generate the new covariates effects with `update_predictors` and store them in \n  # `mu_pred`\n  pred_ken_S_i$mu_pred[,i] <- update_predictors(pred_ken_S, ken_predictors)$mu_pred\n}\n\n# Prediction of prevalence\npred_aver_pop <- pred_target_grid(pred_ken_S_i, \n               f_target = list(prev = function(lp) exp(lp)/(1+exp(lp))))\n```\n:::\n\n\nWe can now plot the two maps and compare them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nplot(pred_age15, which_target = \"prev\", which_summary = \"mean\", main = \"Prevalence (15 years)\", range = c(0,0.6))\nplot(pred_aver_pop, which_target = \"prev\", which_summary = \"mean\",  main = \"Prevalence (General population)\", range = c(0,0.6))\npar(mfrow = c(1,1))\n```\n\n::: {.cell-output-display}\n![Maps of the predictive mean of the rapid diagnostic test (RDT) prevalence for children of the age of 15 (left panel) and the general population (right panel).](05_geostatistical-prediction_files/figure-html/fig-malaria-maps-1.png){#fig-malaria-maps}\n:::\n:::\n\n\nThe exploratory analysis of @sec-indiv-expl-analysis had shown that the prevalence around the age of 15 is higher than at other ages. This is also reflected in @fig-malaria-maps, when the left panel, corresponding to the RDT prevalence for children at the age of 15 years, shows a higher level of prevalence than the right panel, which is instead for the general population.\n\n## Areal-level targets\n\nWhen determining areal-level targets, we must first address the following questions:\n\n1.  What spatially continuous target, $T(x)$, are we seeking to aggregate?\n2.  What are the spatial units, denoted as $A_i$ for $i = 1, \\dots, N$, over which the aggregation is required?\n3.  What aggregating function should be applied?\n4.  Should spatial weights be incorporated in the aggregation, and if so, what weights are appropriate?\n\nThe first question was discussed in earlier sections, where examples of predictive targets that define $T(x)$ can be found in @tbl-spat-cont-pred-target. The answers to the remaining questions are context-dependent and closely linked to the primary research objectives. In public health settings, the areas $A_i$ often correspond to administrative units or regions where decisions are made and resources are allocated.\n\nA common aggregating function for $T(x)$ is the mean, which is formally defined as:\n\n$$\n\\mathcal{M}_i = \\frac{1}{|A_i|}\\int_{A_i} T(x) \\: dx.\n$$ {#eq-mean-alt}\n\nIn addition to the mean, several other aggregating functions can be applied depending on the context of the analysis. We also note that computing the integral in @eq-mean-alt requires some approximations. Our approach utilizes the regular grid $\\tilde{X}$, previously defined when dealing with spatially continuous targets. This grid covers the area $A_i$, allowing us to approximate $M_i$ as: $$\\mathcal{M}_i \\approx \\frac{1}{\\#\\{j : \\tilde{x}_j \\in A_i\\}} \\sum_{\\tilde{x}_j \\in A_i} T(\\tilde{x}_j), $$ {#eq-mean-alt-approx} where $\\#\\{j : \\tilde{x}_j \\in A_i\\}$ denotes the number of grid locations $\\tilde{x}_j$ that fall within $A_i$. The same approximation will be applied to other areal-level targets discussed in this section. However, for simplicity, we will omit the detailed explanation of this step and instead express the predictive target as an integral.\n\nFor instance, in the study of Anopheles mosquitoes in Cameroon, we may be interested in estimating the total number of mosquitoes trapped within the study area ($A_i$ represents a single region in this case). If $T(x)$ denotes the number of mosquitoes trapped at location $x$, the areal-level target can be expressed as: $$\n  \\mathcal{S}_i = \\int_{A_i} T(x) \\: dx.\n  $$\n\nAdditionally, to capture the heterogeneity of $T(x)$ within an area, variance-based measures can be used. The variance of $T(x)$ in $A_i$ is given by: $$\n   \\mathcal{V}_i = \\frac{1}{|A_i|}\\int_{A_i} \\left(T(x) - \\mathcal{M}_i\\right)^2 \\: dx,\n   $$ where $\\mathcal{M}_i$ is the mean of $T(x)$ in $A_i$.\n\nThe formulation of the areal-level targets given so far assumes equal weighting for all locations within the area. Alternatively, if the we consider for example the areal level target in @eq-mean-alt, one could use spatial weights, $w(x) > 0$, and redefine $\\mathcal{M}_i$ as:\n\n$$\n\\mathcal{M}_i = \\frac{\\int_{A_i} w(x) T(x) \\: dx}{\\int_{A_i} w(x) \\: dx}.\n$$\n\nSelecting appropriate spatial weights is crucial, as it reflects the significance of different locations within the area. We can define three types of weighting: population-density, risk-based, and distance-based. We point out that this distinction is not always clear cut (population could indeed be a risk factor in our analysis) but this classification is made only for the sake of making the explanation clearer.\n\nFor example, if the goal is to prioritize areas with higher populations, weights $w(x)$ could be proportional to population density at location $x$. This approach gives greater importance to locations where more people reside, which can be particularly relevant when aggregating disease prevalence data for resource allocation.\n\nIn cases where certain sub-regions within $A_i$ are at higher risk for disease (e.g., proximity to environmental hazards or areas with lower access to healthcare), risk-based weights could be applied. Here, $w(x)$ would be higher in regions identified as higher risk, providing a more targeted aggregation of disease prevalence. For example, populations in rural areas may face higher exposure to infectious diseases than those in urban areas, making it important to assign greater weight to these higher-risk regions in the aggregation process.\n\nIf the objective is to account for proximity effects (such as the spread of an infectious disease or contamination from a known source), distance-based weights could be used. Locations closer to a known disease outbreak area or source of exposure could be given more weight to reflect the spatial dynamics of disease transmission.\n\nUsing spatial weights in these ways ensures that the aggregation of disease prevalence $T(x)$ reflects not only the distribution of the disease but also the underlying population, risk, or spatial characteristics relevant to the public health problem under investigation.\n\n### Example: predicting the average riverblindness prevalence at admin level 1 in Liberia (continuing from @sec-liberia-prediction-1) {#sec-liberia-prediction-2}\n\nWe continue our analysis of the Liberia data and consider areal-level targets. More specifically, we aim to predict the average prevalence over the regions which give the administrative level 1 of Liberia. By denoting with $A_i$ the i-th out of the 15 regions in Liberia. By denoting with $T(x)$ reiverblindness prevalence as defined in @eq-rb-prev-target, we define our predictive target as $$\nT_{i} = \\frac{1}{|A_i|} \\int_{A_i} T(x) \\: dx.\n$$ The R code below shows how predictions for $T_i$ can be performed using the `pred_target_shp` function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Admin level 1 boundaries\nlibrary(rgeoboundaries)\nlb_adm1 <- geoboundaries(country = \"Liberia\", adm_lvl = \"adm1\")\n\n# Prediction of prevalence \npred_shp <- pred_target_shp(lb_pred_S_j, shp = lb_adm1,\n                            shp_target = function(Tx) mean(Tx),\n                            f_target = list(prev = \n                                       function(lp) exp(lp)/(1+exp(lp))),\n                            pd_summary = list(mean = mean),\n                            col_names = \"shapeName\")\n\n```\n:::\n\n\nThe argument `shp_target` is used define the type of aggregation function over the region $A_i$, in this case the mean of the target $T(x)$. Also note that, like for spatially continuous targets, we can define any summary of the predictive distribution of $\\mathcal{M}_i$ through the argument `pd_summary`. Here, we chose to compute only the mean of the predictive distribution but other summaries, such as exceedance probabilities, standard error, quantiles and so on, can be specified if needed.\n\nBefore we plot the results on a map, let us also consider the population-density weighted target, defined as $$\nT_{i}^* = \\frac{ \\int_{A_i} w(x) T(x) \\: dx}{\\int_{A_i} w(x) \\: dx}.\n$$ To aid the explanation of the R code script, we first rewrite the above areal-level target as $$\nT_{i}^* = \\int_{A_i} \\tilde{w_i}(x) T(x) \\: dx.\n$$ where $\\tilde{w}_{i}(x)$ are the standardized weights that integrate to 1 in $A_i$ (i.e. $\\int_{A_i} \\tilde{w}_i(x) \\: dx = 1$).\n\nWe first retrieve the population density data from the World Pop database (see @sec-pop-data) and use these to derive the un-standardized weights $w(x)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n\n# Obtaining population density\nlibrary(wpgpDownloadR)\nlbr_url <- wpgpGetCountryDataset(ISO3 = \"LBR\", covariate = \"ppp_2014\")\nlibrary(terra)\nlbr_pop <- rast(lbr_url)\nlbr_pop <- project(lbr_pop, \"EPSG:32629\")\n\n# Extra pop density weights at the prediction grid\nweights_pred <- extract(lbr_pop, st_coordinates(liberia_grid))$lbr_ppp_2014\n```\n:::\n\n\nIn the code above the population density weights are extracted at the locations of the prediction grid. In the `pred_target_shp` function, in order to use the weights for our predictive target, we need to specify two additional arguments: `weights` to which we pass the population density values (in our case these are stored in `weights_pred`); `standardize_weights`, which is a logical argument taking value `TRUE` if the weights to be standardized so that the sum over the grid covering a region $A_i$ adds up to one.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_shp_w <- pred_target_shp(lb_pred_S_j, shp = lb_adm1,\n                            shp_target = function(Tx) sum(Tx),\n                            f_target = list(prev = \n                                       function(lp) exp(lp)/(1+exp(lp))),\n                            pd_summary = list(mean = mean),\n                            weights = weights_pred,\n                            standardize_weights = TRUE,\n                            col_names = \"shapeName\")\n```\n:::\n\n\nWe can then finally plot the results from the prediction at admin level 1, without using any weights and by weighing according to population density.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_1 <- \nplot(pred_shp, which_target = \"prev\", which_summary = \"mean\",\n     palette = \"RdYlGn\",\n    limits = c(0.1, 0.30),\n    breaks = seq(0.1,0.30, by = 0.05)) + \n  guides(fill=guide_legend(title=\"Prevalence\")) +\n  ggtitle(\"Average prevalence \\n (no weights)\") + \n   theme(plot.title = element_text(size = 15))\n\nplot_2  <- \n  plot(pred_shp_w, which_target = \"prev\", which_summary = \"mean\",\n     palette = \"RdYlGn\",\n    limits = c(0.1, 0.30),\n    breaks = seq(0.1,0.30, by = 0.05)) + \n  guides(fill=guide_legend(title=\"Prevalence\")) +\n  ggtitle(\"Average prevalence \\n (population weighted)\") + \n  theme(plot.title = element_text(size = 15))\n\nlibrary(gridExtra)\ngrid.arrange(plot_1, plot_2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![Maps of the predictive mean for the admin level 1 average prevalence in Liberia. The left panel uses a standard average of locations within an admin level 1 region, whilst in the right panel each location is weighted according to population density.](05_geostatistical-prediction_files/figure-html/fig-average-prev-Liberia-1.png){#fig-average-prev-Liberia}\n:::\n:::\n\n\nIn @fig-average-prev-Liberia, we observe that the differences between the two predictive targets, with and without weights, are not substantial. However, we observe that the predicted average prevalence shows slightly greater variation in coastal regions, particularly in Montserrado County, where the majority of the population residing in the capital of Monrovia. This concentration of population likely accounts for the more noticeable differences in this area.\n\n### Example: predicting the total number of Anopheles gambiae mosquitoes (continuing from @sec-anopheles-fit) {#sec-anopheles-pred}\n\nFor the analysis on Anopheles gambiae mosquitoes, we consider the prediction of the total number of mosquitoes within the study area. In this case, because there is not a natural definition of the study area borders as in the previous analysis, we consider the convex hull as representing those borders. To pursue this let us first, visualize the predictive map of the number of mosquitoes on a regular grid cover the study area. In other words, we first consider the spatially continuous target $$\nT(x) = \\exp\\{\\beta_0 + \\beta_1 e(x) + S(x)\\},\n$$ where, we recall, $e(x)$ is the elevation in meters at location $x$. In the code below in addition to generate prediction for $T(x)$, we also create a binary indicator defined as $$\nw(x) = \n\\begin{cases}\n1 & \\text{if  } 390 < e(x) < 837 \\\\\n0 & \\text{otherwise.}\n\\end{cases}\n$$ The values of 390 and 837 meters correspond to the minimum and maximum values of elevation observed in the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create grid from convex hull\nshp_ch <- convex_hull_sf(an_fit$data_sf)\nan_grid <- create_grid(shp_ch, spat_res = 2)\n\n\nan_elev <- get_elev_raster(locations = shp_ch, \n                                z = 9, clip = \"locations\")\n\npredictors <- data.frame(elevation= terra::extract(an_elev,\n                                                   st_coordinates(an_grid)))\n\npred_an_S <- pred_over_grid(an_fit, grid = an_grid, \n               predictors = predictors,\n               type = \"joint\")\n\npred_n_mosq_grid <- \npred_target_grid(pred_an_S, \n                 f_target = list(n_mosq = function(lp) exp(lp)),\n                 pd_summary = list(mean = function(Tx) mean(Tx)))\n\nan_weights <- 1*(predictors$elevation > 390 & predictors$elevation < 837)\n```\n:::\n\n\nWe now define our predictive target corresponding to the total number of mosquitoes within the study area $A$, given by the convex hull of the observed locations, as $$\nT = \\int_{A} w(x) T(x) \\: dx\n$$ {#eq-tot-mosq} The rationale is to estimate the total number of mosquitoes in the study area while restricting predictions to locations within the observed range of elevation. This approach helps avoid the risk of predicting in ecological areas that may not be suitable for mosquito presence. It is also important to note that in @eq-tot-mosq, we do not standardize the weights in this case.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(1,2))\nplot(pred_n_mosq_grid, which_target = \"n_mosq\", which_summary = \"mean\",\n     main = \"Number of mosquitoes\")\nplot(an_grid, pch = 20, cex = 1, col = an_weights+1, main = \"Weights\")\npar(mfrow = c(1,1))\n```\n\n::: {.cell-output-display}\n![Maps of the predicted averae number of mosqutoes (left panel). The right panel shows prediction locations in red if they have an elevation between 390 and 837 meters, or black otherwise.  ](05_geostatistical-prediction_files/figure-html/fig-mosq-map-1.png){#fig-mosq-map}\n:::\n:::\n\n\n@fig-mosq-map shows the prediction map for the number of mosquitoes at each prediction location, indicating a relatively high spatial heterogeneity as was also indicated by the low value for the estimate of the scale of the spatial correction (parameter $\\phi$). The right panel, we have an image of the weights $w(x)$, with locations that will not contribute to the prediction of the total number of mosquitoes denoted in black.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_n_mosq_shp <- \npred_target_shp(pred_an_S, shp = shp_ch, \n                 weights = an_weights,\n                 shp_target = sum,\n                 f_target = list(n_mosq = function(lp) exp(lp)),\n                 pd_summary = list(mean = function(Tx) mean(Tx),\n                                   q025 = function(Tx) quantile(Tx, 0.025),\n                                   q075 = function(Tx) quantile(Tx, 0.975)))\n\npred_n_mosq_shp$target$reg1$n_mosq\n## $mean\n## [1] 17707.52\n## \n## $q025\n##     2.5% \n## 15020.76 \n## \n## $q075\n##    97.5% \n## 21008.32\n```\n:::\n\n\nSince we are not standardizing the weights, there is no need to specify `standardize_weights`, as it is set to `FALSE` by default. In the code above, we printed the mean of the predictive distribution of $T$ along with the 95% prediction interval. The point estimate of approximately 17,719 mosquitoes is likely an underestimate of the total number of Anopheles gambiae in the area, as the trap used to count the mosquitoes may not capture all of them, particularly those outside the immediate trapping zone or those active at different times.\n\n## Simulation-based assessment of predictive performance\n\n### How to simulate from a geostatistical model\n\nBefore we discuss more in detail how to perform a simulation study, it is important to understand how data can be simulated from a geostatistical model.\n\nThe way we have formulated geostatistical models in @sec-geostat-models provides a hint as to what step should be followed when simulating from a geostatistical model. Using the same notation of @sec-geostat-models, these are the steps.\n\n1.  For a given set of predefined locations $X$ compute the covariance matrix $\\Sigma$ with $(i,j)-$th entry $\\sigma^2 \\rho(u_{ij})$.\n\n2.  Compute the covariates effects $d(x)^\\top$ for all the locations in $X$.\n\n3.  Simulate $B$-time from a multivariate Gaussian distribution with mean vector 0 and covariance $\\Sigma$ as previously defined. We denote the output form this step as $S_{(j)}(x)$, for $j=1,\\ldots,B$.\n\n4.  Compute the linear predictor at each locations of $X$, by adding the covariates effects in 3 to the simulated random effects in 4, hence $g\\{\\mu_{(j)}(x)\\} = d(x)^\\top + S_{(j)}(x)$.\n\nNote that before performing the steps above, the values of the model parameters must be fixed. For example, if simulating from a model fitted to the data, the values of the model parameters should be the maximum likelihood estimates. At this point, if the goal is to simulate actual observations for the outcome $Y$ at the locations $X$. Then we need to add the following two steps. 5A. If the nugget term is included in the model then, draw samples $Z_{(j)}$ for each location in $X$ from a Gaussian distribution with mean 0 and variance $\\tau^2$. Finally add the simulated values $Z_{(j)}$ to $g\\{\\mu_{(j)}(x)\\}$. 6A. Compute $\\mu_{(j)}(x)$ by applying the inverse link function $g^{-1}(\\cdot)$ to the samples of the linear predictor previously computed. 7A. Simulate from the distribution of $Y$ given $\\mu_{(j)}(x)$, according the chosen model, for $j=1,\\ldots,B$.\n\nThese steps, from 1 to 7A, have been implemented in the `glgpm_sim` function. For example, using the fitted Binomial geostatistical model from the example on riverblindness in Liberia, we simulate a single data-set as follows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nliberia_sim <- glgpm_sim(n_sim = 1, model_fit = fit_liberia)\n```\n:::\n\n\nThe output `liberia_sim` is a list with as many components as indicated by `n_sim`. In this this case, we can access the simulated data-set as `liberia_sim[[1]]`. For example, we could refit the same model to the simulated data as\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_liberia_sim <- \nglgpm(formula = npos ~ log(elevation) + gp(long, lat, nugget = NULL), \n    data = liberia_sim[[1]], family = \"binomial\", den = ntest, \n    crs = 32629)\n```\n:::\n\n\nNote that if the data's CRS has been converted, the simulated dataset will inherit the new CRS rather than the original. This means that any transformation applied to the CRS before simulation (e.g., from geographic to projected coordinates) will affect the spatial scale and interpretation of the simulated data.\n\nIn other cases, as in @sec-sim-assessment, the locations $X$ correspond to a regular grid and the steps that we want to carry out on completion of step 4 above depend on the objective of the simulation. We explain this more in detail in the next section.\n\n### Setting up and carrying out a simulation study\n\n### Assessessment of threshold-based classification of spatial units using geostatistical models {#sec-sim-assessment}\n\n## Comparing the predictive performance of geostatistical models {#sec-compare-pp}\n\nIn this section, we address the problem of identifying the geostatistical model that offers the best predictive performance among a set of candidates. However, we first need a clear definition of predictive performance which can be used to select suitable statistical tools that can be to used to evaluate it. Broadly speaking, predictive performance is defined by how well a model's predictive distribution aligns with observed data. Evaluating predictive performance requires examining two key characteristics of the predictive distribution: *sharpness* and *calibration*.\n\n\n### How to split geostatistical data for model performance comparisons\n\nTo carry out the assessment and comparison of the predictive performance of geostatistical models using the methods illustrated in the following sections, the first crucial step to decide which approach to use to use split the data-set into a *training set* and *test set*. In the context of geostatistical analysis, the *training set* is the subset of the original data-set that is used to estimate the model parameters and then predict at the locations of the *test set*, which is used to assess how well the model can predict unseen data.\n\nWhen splitting geostatistical data-sets for model performance evaluation, it is essential to consider the objective of the spatial prediction. Here, we consider two main prediction objectives: 1) predicting in areas disjoint from the study area where there are no data; 2) inferring the spatial surface of disease risk within a given study area. Let us give an example to better explain the difference between these two objectives. Under objective 1), if survey data exist for a specific country, say Kenya, but not for a neighboring country, say Somalia, a model trained on Kenyan data might then be used to predict disease prevalence in Somalia. In objective 2), instead, high-resolution risk maps of a health outcome might be required within a single country. The splitting of geostatistical data for predictive performance assessment should align with the intended application of the model and reflect whether the goal is to predict in entirely disjoint areas with no data (objective 1) ) or to infer the spatial surface of interest within the same study area (objective 2) ). Below, we elaborate on suitable methods for each of these two.\n\nIn the first scenario (objective 1) ), splitting the data using k-fold cross-validation with spatially coherent folds is more appropriate. These folds can be created using clustering methods such as k-means or hierarchical clustering (see @yin2024 for review of the main clustering methods). These methods group spatial locations based on their coordinates or covariates, ensuring each fold represents a distinct spatial region. This approach mimics the challenge of predicting in disjoint areas by withholding entire spatial clusters from the training data. For instance, k-means minimizes within-cluster variance to produce compact, non-overlapping groups, while hierarchical clustering organizes data into nested clusters based on a linkage criterion like geographic distance. In the code below we show an example of the use of these methods for data splitting, using the `spatial_clusterin_cv` function from the `spatialsample` package [@mahoney2023].\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spatialsample)\nset.seed(123)\nliberia_sf <- liberia %>%\n              st_as_sf(., coords = c(\"long\", \"lat\"),\n                       crs = 4326) %>%\n              st_transform(.,crs = 32629)\n\n\n# K-means\nkmeans_liberia <- spatial_clustering_cv(liberia_sf,\n                      v = 10, cluster_function = \"kmeans\")\nkmeans_plot <- autoplot(kmeans_liberia) + ggtitle(\"K-means\")\n\n# Hierarchical clustering\nhclust_liberia <- spatial_clustering_cv(liberia_sf,\n                      v = 10, cluster_function = \"hclust\") \n                   \n\nhclust_plot <- autoplot(hclust_liberia) +\n  ggtitle(\"Hierarchical clustering\")\n\nlibrary(gridExtra)\ngrid.arrange(kmeans_plot, hclust_plot, ncol = 2)\n```\n\n::: {.cell-output-display}\n![Examples un the use of spatial clustering methods for the splitting of the Liberia data on riverblindness.](05_geostatistical-prediction_files/figure-html/fig-clus-loc-1.png){#fig-clus-loc}\n:::\n:::\n\nThe results of the splitting are shown in @fig-clus-loc, where we have generated 10 folds using the k-means and hierarchical clustering methods with the default options. We can observe that the test sets are very similar, with fold 3 from the k-means and fold 7 from hierarchical clustering being identical. If using only the distance between locations to cluster the data into different folds, the choice of clustering method should have much impact on the model assessment as long as we repeat the split and assessment of the methods multiple times as we will show later in the next sections.\n\nBefore proceeding further, it is important to highlight that when the objective is predicting an outcome in entirely disjoint areas, the risks and limitations of this assessment should be carefully considered. In such cases, predictions are predominantly driven by covariates because the spatial Gaussian  process $S(x)$ cannot extrapolate spatial patterns across regions with no connecting data. When the disjoint region requiring predictions is far from the sampled data, the Gaussian process contributes primarily to inflating prediction uncertainty and has no tangible impact on the point predictions, reflecting the lack of ground data in the area. Consequently, this validation approach primarily assesses the predictive power of the covariates rather than the geostatistical model as a whole. This limitation may lead to overestimating the model’s generalization capabilities, especially if the model is being used to predict areas that are far away from the study area. Hence, the results of this validation should be interpreted cautiously, emphasizing the critical role of covariates in driving predictions under such conditions.\n\nWe now consider the second prediction objective of inferring the spatial surface within the study area. In this case, a random sampling approach can be employed. However, to preserve a good spatial coverage of the study area, a minimum distance can be imposed between selected locations. For example, methods such as spatial thinning or stratified sampling can be adapted to enforce a minimum spatial separation. This can be done in R using the `subsample.distance` function from the `spatialEco` package [@evans2021].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spatialEco)\nset.seed(123)\n\n# Regulirized spatial sampling: 30km\ndist30_liberia <- subsample.distance(liberia_sf,\n                      size = 20, d = 30000)\ndist30_plot <- ggplot(dist30_liberia) + geom_sf() +\n               theme_minimal() + \n               ggtitle(\"Minimum distance: 30km\")\n\n# Regulirized spatial sampling: 40km\ndist40_liberia <- subsample.distance(liberia_sf,\n                      size = 20, d = 40000)\ndist40_plot <- ggplot(dist40_liberia) + geom_sf() +\n               theme_minimal() + \n               ggtitle(\"Minimum distance: 40km\")\n\ngrid.arrange(dist30_plot, dist40_plot, ncol = 2)\n```\n\n::: {.cell-output-display}\n![Examples un the use of spatialky regularized thinning for the splitting of the Liberia data on riverblindness into training and test set. The plots show the resulting test sets by randomly selecting 20 locations from the original data-set and imposing a minimum distance of 30km (left panel) and 40km (right panel).](05_geostatistical-prediction_files/figure-html/fig-reg-loc-1.png){#fig-reg-loc}\n:::\n:::\n\nIn the code above, we randomly selected 20 locations from the original data-set, enforcing minimum distances of 30 km and 40 km between points. The results are presented in @fig-reg-loc. The choice of the minimum distance for the test sample should ideally consider the spatial correlation scale (previously denoted by $\\phi$) of the spatial Gaussian process $S(X)$. To minimize correlation within the test set, the minimum distance could be set to a value greater than $\\phi$. However, increasing the minimum distance makes it more challenging to obtain a test set of the desired sample size. Reducing correlation within the test set is important for ensuring that the assessment of predictive performance is as robust as possible.\n\nA key challenge in both approaches is the potential lack of independence between the training and test data-sets due to spatial correlation. This can lead to an overly optimistic evaluation of model performance, as the training set may already contain information that is correlated with the test set. However, if we assume that all models benefit equally from this correlation during performance evaluation, the methods we illustrate next can still provide a reliable comparison of model performance in relative terms, even if absolute performance metrics may be biased.\n\n### Assessing calibration\n\nA model is considered *well-calibrated* if its predictions adequately reflect the true uncertainty of the data. Assessment of a model's calibration can be carried out in several ways. We can examine the agreement between the point predictions, say $\\hat{y}\\_i$, and the observed outcomes \\$y_i \\$, commonly referred to as *accuracy*. The mean squared error (MSE), defined as $\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$, is an example of a commonly used metric to evaluate the accuracy of a model. To characterize calibration more fully, it is also essential to quantify the spread of the predictive distributions, which also defines the *precision* of predictions. For example, prediction intervals generated from the predictive distribution for $y_i$ can help assess the precision of a geostatistical model. In summary, a well-calibrated model is both accurate and precise.\n\nAssessment of the calibration of the model should precede the assessment of sharpness, since, as we shall see in the next section, this relies on the model being well-calibrated. The probability integral transform (PIT) was originally proposed by @dawid1984 as a way to assess the calibration of a model. The PIT is based on the simple observation that if we consider a variable $Y$ and apply the transformation $Y^* = F_{Y}(Y)$, where $F_{Y}(\\cdot)$ is the cumulative density function (or cumulative distribution if $Y$ is discrete) of $Y$, it then follows that $Y^*$ follows a uniform distribution in the unit interval. The fundamental problem and the reason why statistical modelling exists is that we do not $F_{Y}$ but we would like to propose a model $M$ that we believe adequately approximates $F_{Y}$ with $F_{M}$. The main advantage of using the PIT is that assessment of whether $M$ is well calibrated reduces to assessing whether the transform set of data $F_{M}(y_1), \\ldots, F_{M}(y_n)$ follows a uniform distribution. To illustrate this, let us a consider a simple simulated example.\n\nWe first create a function that can compute the PIT.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a function for the PIT\n# F_M is the cumulative density function generated by the adopted model\npit_transform <- function(data, F_M) {\n  # Apply the model CDF to the data\n  pit_values <- F_M(data)\n  return(pit_values)\n}\n```\n:::\n\n\nWe then apply this function to show how the assessment of calibration through the PIT is carried out under two scenarios: 1) $F_{Y}$ is a Gamma distribution with shape 2 and scale parameter 1; 2) $F_{Y}$ is Student's T distribution with 3 degrees of freedom. In each of the two scenarios, we show how the PIT behaves when our model coincides with the the true model (i.e. $F_{M} = F_{Y}$) and when instead our model $M$ is a Gaussian distribution, with mean and variance estimated from the data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate data from a skewed (gamma) distribution\nn <- 1000\nshape <- 2\nrate <- 1\ndata_gamma <- rgamma(n, shape = shape, rate = rate)\n\n# Define correct and incorrect model CDFs for gamma data\n# Correct CDF\nmodel_cdf_correct_gamma <- function(x) pgamma(x, shape = shape, rate = rate)\n# Incorrect CDF: Assume data is normal (wrong model)\nmodel_cdf_incorrect_gamma <- function(x) pnorm(x, mean = mean(data_gamma), sd = sd(data_gamma))\n\n# Compute PIT values for gamma data under both models\npit_correct_gamma <- pit_transform(data_gamma, model_cdf_correct_gamma)\npit_incorrect_gamma <- pit_transform(data_gamma, model_cdf_incorrect_gamma)\n\n# Generate data from a heavy-tailed (t) distribution\ndf <- 3\ndata_t <- rt(n, df = df)\n\n# Define correct and incorrect model CDFs for t data\n# Correct CDF\nmodel_cdf_correct_t <- function(x) pt(x, df = df)\n# Incorrect CDF: Assume data is normal (wrong model)\nmodel_cdf_incorrect_t <- function(x) pnorm(x, mean = mean(data_t), sd = sd(data_t))\n\n# Compute PIT values for t data under both models\npit_correct_t <- pit_transform(data_t, model_cdf_correct_t)\npit_incorrect_t <- pit_transform(data_t, model_cdf_incorrect_t)\n\n# Combine results into a data frame for plotting\ndf_plot <- data.frame(\n  PIT = c(pit_correct_gamma, pit_incorrect_gamma, pit_correct_t, pit_incorrect_t),\n  Model = rep(c(\"Correct Gamma Model\", \"Incorrect Normal Model for Gamma\", \n                \"Correct t Model\", \"Incorrect Normal Model for t\"), each = n)\n)\n\n# Plot PIT distributions\nhist_plot <- ggplot(df_plot, aes(x = PIT)) +\n  geom_histogram(aes(y = ..density..), bins = 20, fill = \"skyblue\", color = \"black\", alpha = 0.7) +\n  geom_hline(yintercept = 1, linetype = \"dashed\", color = \"red\") +\n  facet_wrap(~ Model, scales = \"free\") +\n  labs(title = \"Probability Integral Transform (PIT) Distributions\",\n       x = \"PIT Values\",\n       y = \"Density\") +\n  theme_minimal()\n\n# Display the plot\nprint(hist_plot)\n```\n\n::: {.cell-output-display}\n![Histograms of the probability integral transform applied to simulated data. The top panels show the histograma of the PIT when the fitted model coincides with the true data generating model. The lower panels show how the PIT histrograms deviate from a uniform distribution. For more details, we refer to main text.](05_geostatistical-prediction_files/figure-html/fig-hist-pit-1.png){#fig-hist-pit}\n:::\n:::\n\n\nIn @fig-hist-pit, we assess whether the PIT transformed data follow a uniform distribution if the bars of the histogram are all approximately at the same height as indicated by the dashed line. Our preference to this diagnostic plot, is to use a qq-plot for a uniform distribution as shown below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# QQ plot to check uniformity of PIT values\nqq_plot <- ggplot(df_plot, aes(sample = PIT)) +\n  stat_qq(distribution = qunif) +\n  stat_qq_line(distribution = qunif, color = \"red\") +\n  facet_wrap(~ Model, scales = \"free\") +\n  labs(title = \"QQ Plot of PIT Values against Uniform Distribution\",\n       x = \"Theoretical Quantiles (Uniform)\",\n       y = \"Sample Quantiles (PIT Values)\") +\n  theme_minimal()\n\n# Display both plot\nprint(qq_plot)\n```\n\n::: {.cell-output-display}\n![QQ-plots of the probability integral transform applied to simulated data. The top panels show the histograma of the PIT when the fitted model coincides with the true data generating model. The lower panels show how the PIT qq-plots deviate from a uniform distribution. The red line is the identity line For more details, we refer to main text.](05_geostatistical-prediction_files/figure-html/fig-qqplot-pit-1.png){#fig-qqplot-pit}\n:::\n:::\n\n\nIn the results of @fig-qqplot-pit, we consider a model well-calibrated if the qq-plot is as close as possible to the identity line. It is clear that in the first case the use of Gaussian distribution is violated by the skewness of the data generated from a Gamma distribution, and in the second scenario by the heavier tail of the Student's T distribution.\n\nIn the simulated example, the PIT might seem unnecessary because we can directly assess if the data follow the assumed distribution (Gamma, Gaussian, or Student's T) by comparing empirical and theoretical distributions. In these simple models, including linear Gaussian geostatistical models, the marginal distribution of the data is analytically available, so assessing goodness-of-fit is straightforward.\n\nHowever, in the case of Binomial and Poisson geostatistical models, the marginal distribution of the data $Y$ -- unconditioned on the spatial random effects $S(x)$ -- cannot be obtained analytically. Specifically, geostatistical models for count data assume that $Y$, conditioned on $S(x)$, follows either a Binomial or Poisson distribution. But once we integrate out $S(x)$ to obtain the marginal (i.e., unconditioned) distribution of $Y$, this distribution is no longer Poisson or Binomial. Instead, it becomes an overdispersed count distribution that lacks a closed-form expression.\n\nIn such cases, the PIT becomes valuable for evaluating model adequacy, as it allows for assessing the uniformity of PIT values against the assumed model distribution, even when the marginal distribution is intractable. However, direct application of the PIT raises the issue that the the transformation $F_{Y}(Y)$ does not follow a uniform distribution, because of the discrete nature of the random variable $Y$. Hence, for a given discrete-outcome model $M$, we use a modified version of the PIT, referred to as nonrandomized PIT (henceforth, nPIT), originally proposed by @czado2009, taking the form $$\n\\text{nPIT}(u,y) = \n\\begin{cases} \n0, & u \\leq F_{M}(y-1) \\\\ \n\\frac{u - F_{M}(y-1)}{P_x - F_{M}(y-1)}, & F_{M}(y-1) \\leq u \\leq F_{M}(y) \\\\ \n1, & u \\geq F_{M}(y)\n\\end{cases}.\n$$ We then take the average nPIT over the observed outcomes $y_{1}, \\ldots, y_{n}$, hence $$\n\\text{AnPIT}(u) = \\frac{1}{n} \\sum_{i=1}^n \\text{nPIT}(u, y_{i}). \n$$ Assessment of calibration is then carried out by checking wether the AnPIT is as close as possible to the identity function, i.e. $\\text{AnPIT}(u) = u$. Hence, by plotting $\\text{AnPIT}(u)$ against $u$, we can consider any deviations from the identity line as evidence that the model is not well-calibrated.\n\nWe now illustrate the use of the AnPIT through a simulated example. We simulate data from a negative Binomial distribution, with mean $\\lambda = 5$ and dispersion parameter $\\alpha = 1/2$, hence $E[Y] = \\lambda$ and $\\text{Var}[Y] = \\lambda \\times (1 + \\alpha \\lambda)$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parameters for the Negative Binomial distribution\nset.seed(123)\nn <- 200                # number of simulations\nlambda <- 5             # mean\ndispersion <- 0.5       # dispersion parameter\nsize <- 1 / dispersion  # size parameter for negative binomial\n\n# Simulate data from the Negative Binomial distribution\nsim_data_nb <- rnbinom(n, size = size, mu = lambda)\n\n# Define the nonrandomized PIT function\ncompute_npit <- function(y, u, cdf_func) {\n  # Calculate cumulative probabilities F_M(y-1) and F_M(y) using the provided CDF function\n  f_y_minus_1 <- cdf_func(y - 1)\n  f_y <- cdf_func(y)\n  \n  # Apply the piecewise formula for nPIT\n  if (u <= f_y_minus_1) {\n    return(0)\n  } else if (u <= f_y) {\n    return((u - f_y_minus_1) / (f_y - f_y_minus_1))\n  } else {\n    return(1)\n  }\n}\n\n# Generate a sequence of u values from 0 to 1\nu_values <- seq(0, 1, length.out = 100)\n\n# Calculate the average nPIT for each u value for both models\nAnPIT_nb <- sapply(u_values, function(u) {\n  mean(sapply(sim_data_nb, compute_npit, u = u, cdf_func = function(k) pnbinom(k, size = size, mu = lambda)))\n})\n\nAnPIT_poisson <- sapply(u_values, function(u) {\n  mean(sapply(sim_data_nb, compute_npit, u = u, cdf_func = function(k) ppois(k, lambda = lambda)))\n})\n\n# Combine the results into a data frame for plotting\nAnPIT_data <- data.frame(\n  u = rep(u_values, times = 2),\n  AnPIT = c(AnPIT_nb, AnPIT_poisson),\n  Model = rep(c(\"Negative Binomial (correct model)\", \"Poisson (wrong model)\"), each = length(u_values))\n)\n\n# Plotting the average nPIT as a function of u for both models\nggplot(AnPIT_data, aes(x = u, y = AnPIT, color = Model)) +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\", color = \"black\") +  # Identity line\n  facet_wrap(~ Model) +\n  coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +  \n  xlab(\"u\") +\n  ylab(\"AnPIT\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")  \n\n```\n\n::: {.cell-output-display}\n![Average nonrandomized Probability Integral Transform (AnPIT) applied to simulated data from a Negative Binomial distribution. The left plot shows the AnPIT when the model is correctly specified. The right panel shows the AnPIT when a Poisson model is instead used. For more details  we refer to the main text.](05_geostatistical-prediction_files/figure-html/fig-anpit-1.png){#fig-anpit}\n:::\n:::\n\n@fig-anpit shows clear evidence that the Poisson model is not adequately accounting for the overdispersion of the simulated data from the Negative Binomial distribution.\n\nHow can we adapt this approach to generalized linear geostatistical models (GLGMs)? The concept is straightforward, though the implementation is less so (but fortunately, we have already done this in `RiskMap`). Essentially, in all relevant equations above, we need to replace $M$ with the predictive distribution of the fitted geostatistical model.\n\nTo explain further, suppose we have fitted our model using data $Y^{(1)} = \\left( y^{(1)}_{1}, \\ldots, y^{(1)}_{n} \\right)$, and we want to use another set of data $Y^{(2)} = \\left( y^{(2)}_{1}, \\ldots, y^{(2)}_{m} \\right)$ to assess calibration. Our model $M$in the equations above corresponds to the distribution of $\\left[ Y^{(1)} \\mid Y^{(2)} \\right]$, derived from our geostatistical model. Since its derivation  cannot be done analytically, we use Monte Carlo methods to approximate $\\left[ Y^{(1)} \\mid Y^{(2)} \\right]$. We discuss this in more detail in @sec-theory-prediction. However, the key point here is to understand why we perform this assessment and how to interpret the results.\n\n\n### Assessing sharpness\n\nSharpness, the other aspect of predictive performance mentioned above, refers to the concentration or narrowness of the predictive distribution around $y_i$. A sharp predictive distribution has a narrow spread, indicating that the model is highly confident in its predictions. Importantly, sharpness does not depend on how close predictions are to the true values (i.e., it is independent of accuracy). A model with high sharpness will produce predictions with low variance, but this sharpness is only meaningful if the model is well-calibrated, ensuring that the high confidence aligns with observed values. For example, a geostatistical model for disease prevalence mapping that predicts a very tight range of possible prevalence values for a location is producing sharp predictions. However, if the actual prevalence often falls outside this range, then the predictions may be sharp but poorly calibrated, thus limiting the value of quantifying sharpness alone. It is important to distinguish between sharpness and precision, as they are similar but subtly different concepts. When considering precision, we are interested in the reliability of the indicators of uncertainty generated from the predictive distribution, such as ensuring that the nominal coverage of prediction intervals matches the actual coverage. Sharpness, instead, refers strictly to the spread of the predictive distribution without regard to accuracy. In other words, while precision generally describes the reproducibility of model predictions across multiple outcomes $y_i$, sharpness purely reflects the model's spread around its point predictions.\n\n### Summary\n\n| Diagnostic/Metric                          | Performance characteristic | Descritpion |\n|----------------------------------|----------------------|----------------|\n| Probability integral transform (PIT)       | Calibration                |             |\n| Nonrandomized PIT                          | Calibration                |             |\n| Mean square error                          | Calibration                |             |\n| Continuous ranked probability score (CRPS) | Calibration and sharpness  |             |\n| Scaled CRPS                                | Calibration and sharpness  |             |\n\n: Summary of the different diagnostic tools and metrics, discussed in this chapter, that are used to assess the predictive performance of a geostatistical model. {#tbl-summary-pp-diag}\n\n## Theory {#sec-theory-prediction}\n\n### Derivation of the predictive distribution {#sec-pred-distr-theory}\n\n### Scoring rules\n\n## FAQs\n\n-   *What should be the spatial resolution of the grid used for prediction?*\n\n-   *I have covariates with different spatial resolutions. Which spatial resolution should I use?*\n\n-   *My prediction map looks like noise. What could be the error?*\n\n-   *The main goal of my analysis is to carry out predictions of the main outcome. However, can I also use the model to draw inferences on the association between the outcome and the covariates that I have included in the model?*\n\n-   *In my model I have some covariates that are attached to the individual (e.g. age, gender, occupation, etc.). How do I specify them when doing prediction?* <br> Read @sec-non-spar-covariates.\n\n## Review questions\n\n-   What are the steps that needs to taken when performing spatial prediction?\n-   Define the predictive distribution of a predictive target and explain how this is used to answer a research question.\n-   Distinguish between spatially continuous and areal-level targets, giving at least two examples for each.\n-   Explain the difference between marginal and joint predictions, and for which targets each can be used.\n-   Define predictive performance in terms of calibration and sharpness. \n- Explain how calibration can be assessed using graphical tools, by distinguishing between continuous and count outcomes.\n-   Give examples of metrics that can be used to evaluate calibration and sharpness.\n\n## Exercises\n\n1.  *Parametric bootstrap.* Using the model fitted to the Anopheles gambiae mosquitoes in @sec-anopheles-fit, simulate 1000 data-sets using the `glgpm_sim` function and estimate the model to each of the simulated data-set. Use the resulting 1000 estimates to compute 95$\\%$ confidence intervals for each of the model parameters and compare this with the confidence intervals obtained from the model summary.\n\n2. Using the code from @sec-compare-pp, redraw the plot of @fig-anpit by choosing different values for the dispersion parameter of the Negative Binomial distribution. What happens when $\\alpha$ is close to 0 and why?\n\n",
    "supporting": [
      "05_geostatistical-prediction_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}