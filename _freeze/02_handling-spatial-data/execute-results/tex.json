{
  "hash": "0e5c39e95e0b7d2d79bd616bb6ebf05d",
  "result": {
    "engine": "knitr",
    "markdown": "---\nfilters:\n  - line-highlight\nexecute: \n  freeze: auto\nfig-width: 5\nfig-asp: 0.75\n---\n\n\n\n\n\n\n# Handling of spatial data in R {#sec-handling-data}\n\n## Introduction\n\nGeostatistical analysis primarily deals with **point-referenced data**. However, geostatistical modeling often requires more than just point data---**raster** and **areal (polygon)** data are frequently used to build essential covariates, enhance spatial understanding, or provide environmental context. For instance, population density, climate data, land use, and elevation, often key covariates in many spatial analyses, usually come in raster or polygon formats. Efficient handling of these different data types is critical for successful model-building. This chapter provides a comprehensive guide to the various stages of managing spatial data in R, using the `sf` and `terra` packages, to support a complete geostatistical workflow.\n\n## Spatial data handling in geostatistical analysis\n\nThroughout a geostatistical analysis, handling spatial data takes place at multiple key stages:\n\n1.  **Retrieving External Spatial Data Sources**: Before starting the modeling process, one often needs to acquire spatial datasets from external sources to improve model accuracy. These could include satellite-derived environmental variables, population data from [WorldPop](https://www.worldpop.org/), or climate data from [WorldClim](https://www.worldclim.org/). Acquiring these datasets and ensuring they are in compatible formats and resolutions is an essential early step in spatial analysis.\n\n2.  **Importing and Standardizing Spatial Data**: Once external data is obtained, it must be imported into R. This involves handling various file formats such as shapefiles or geopackages for vector data and GeoTIFFs for raster data. Moreover, different datasets might use different **Coordinate Reference Systems (CRS)**, requiring CRS standardization to ensure that all datasets align correctly. Failure to standardize CRS can result in spatial misalignments and incorrect results.\n\n3.  **Extracting Covariate Information for Modeling**: Covariate extraction is an essential step in geostatistical modeling. After importing spatial data, it is necessary to extract relevant covariate information both for the **sampled locations** (where we have observed data) and the **prediction locations** (where we wish to make predictions). This step involves linking raster or polygonal covariates---such as climate, population, or land cover data---to the geostatistical data points.\n\n4.  **Prediction and Creation of a Spatial Grid**: For predictive geostatistical models, a regular grid of points is often created over the study region. Covariates must then be assigned to each grid point to make predictions across the entire region of interest. Creating predictive grids and linking them to the necessary covariates is key to generating continuous spatial predictions from geostatistical models.\n\n5.  **Visualizing Spatial Data**: Visualization is crucial for exploring spatial data, interpreting model results, and communicating findings. Whether working with point-referenced data, polygons, or rasters, clear and effective visualization helps reveal patterns that inform the modeling process. Effective visualization can also help highlight covariate trends, spatial clusters, and uncertainty in predictions.\n\n## Chapter overview\n\nThis chapter introduces methods for handling spatial data at each stage of the geostatistical analysis workflow. You'll learn how to:\n\n-   Retrieve covariates from external spatial data sources;\n\n-   Import and standardize spatial data;\n\n-   Extract covariates for geostatistical analysis;\n\n-   Create predictive grids and link them with covariates;\n\n-   Visualize spatial data effectively to aid in analysis and presentation.\n\nBy mastering these steps, you'll be equipped to handle the complexity of spatial data in geostatistical modeling, enhancing your analyses and improving your predictions.\n\n## Accessing covariates for disease mapping\n\nCovariates can play a crucial role in understanding and modeling the spatial variation in disease risk. In geostatistical analysis, incorporating environmental, demographic, and climatic variables might improve the predictive power of models or at least reduce the level of uncertainty in the predictions. These covariates can influence factors such as the spread of infectious diseases, the distribution of disease vectors, and the socio-economic conditions that impact health outcomes.\n\nA wide range of open-access spatial datasets provide covariates for disease mapping. These include population density, climate, land cover, and human infrastructure data, which often come in raster or polygon formats. Alongside these environmental covariates, administrative boundaries are also crucial for public health analysis, as they help organize and aggregate data at different levels (e.g., country, region, or district). For example, aggregating health outcomes or covariate data by administrative units allows researchers to identify geographic disparities and allocate resources accordingly. Fortunately, open-source platforms such as [geoBoundaries](https://www.geoboundaries.org/) provide easily accessible administrative boundary data for many countries at various levels of granularity. This data is available in formats compatible with geospatial analysis tools in R, making it easy to integrate into geostatistical workflows.\n\n@tbl-data-sources below summarizes key sources of covariates useful in disease mapping. Each dataset offers specific types of data, from satellite-derived environmental variables to gridded population estimates and administrative boundaries, which can be accessed through various R packages or APIs.\n\nMENTION RSPATIAL DATA WEBSITE?? https://rspatialdata.github.io/\n\n| **Source**              | **Data Type**                   | **Description**                              | **R Package**        |\n|------------------|------------------|-------------------|------------------|\n| **WorldClim**           | Climate (temperature, rainfall) | Global climate data                          | `geodata`            |\n| **MODIS**               | Remote Sensing                  | Satellite imagery (e.g., vegetation indices) | `MODIStsp`           |\n| **OpenStreetMap (OSM)** | Human settlements, roads        | Global geographic features                   | `osmdata`            |\n| **Google Earth Engine** | Satellite imagery               | Large-scale environmental data analysis      | `rgee`               |\n| **WorldPop**            | Population data                 | Gridded population density estimates         | `worldpop` (via API) |\n\n: Some key sources of covariates useful in disease mapping and R packages to retrieve them. {#tbl-data-sources}\n\n### Example: Downloading administrative boundaries\n\nAdministrative boundaries provide an essential spatial structure for many types of geostatistical analyses, particularly in disease mapping where data is often aggregated by administrative units such as regions, districsts or provinces. The [geoBoundaries](https://www.geoboundaries.org/) datasets, which are accessible via the `rgeoboundaries` package, provides openly available administrative boundary data for nearly every country, allowing researchers to integrate these boundaries into their analyses seamlessly.\n\n\n\n\n\n\n::: {#fig-liberia-admin .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Load the rgeoboundaries package\nlibrary(rgeoboundaries)\n\n# Download administrative boundaries for Liberia (level 0: country)\nliberia_admin0 <- gb_adm0(\"Liberia\")\n\n# Do the same for level 1: regions\nliberia_admin1 <- gb_adm1(\"Liberia\")\n\n# Plot the administrative boundaries\nplot(liberia_admin0$geometry)\nplot(liberia_admin1$geometry)\n```\n\n::: {.cell-output-display}\n![Adimin level 0](02_handling-spatial-data_files/figure-pdf/fig-liberia-admin-1.pdf){#fig-liberia-admin-1}\n:::\n\n::: {.cell-output-display}\n![Admin level 1](02_handling-spatial-data_files/figure-pdf/fig-liberia-admin-2.pdf){#fig-liberia-admin-2}\n:::\n\nAdministrative boundaries for Liberia retrieved using the `rgeoboundaries` package.\n\n:::\n\n\n\n\n\n\n### Example: Download population data {#sec-pop-data}\n\nPopulation density is a key covariate in geostatistical models for public health research. In this section, we demonstrate how to retrieve high-resolution population data for Liberia from [WorldPop](https://www.worldpop.org/) using the `wpgpDownloadR` package. This package provides easy access to the WorldPop datasets, which offers gridded population estimates at various spatial resolutions.\n\nBefore downloading the data, we will search for available datasets for Liberia. The function `wpgpListCountryDatasets()` helps in retrieving a list of all available datasets for a specified country.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(wpgpDownloadR)\n\n# Search for datasets available for Liberia \n# usign the ISO3 country code\nlbr_datasets <- wpgpListCountryDatasets(ISO3 = \"LBR\")\n\nif(knitr::is_html_output()){\n  lbr_datasets\n} else {\n  head(lbr_datasets)\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     ISO ISO3 Country Covariate\n125  430  LBR Liberia  ppp_2000\n374  430  LBR Liberia  ppp_2001\n623  430  LBR Liberia  ppp_2002\n872  430  LBR Liberia  ppp_2003\n1121 430  LBR Liberia  ppp_2004\n1370 430  LBR Liberia  ppp_2005\n                                                                                                                                                                 Description\n125  Estimated total number of people per grid-cell 2000 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n374  Estimated total number of people per grid-cell 2001 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n623  Estimated total number of people per grid-cell 2002 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n872  Estimated total number of people per grid-cell 2003 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n1121 Estimated total number of people per grid-cell 2004 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n1370 Estimated total number of people per grid-cell 2005 The dataset is available to download in Geotiff format at a resolution of 3 arc (approximately 100m at the equator)\n```\n\n\n:::\n:::\n\n\n\n\n\n\nWe can check the description column to see what datasets are available. Let's download the population data for Liberia for the year 2014 at a 100m resolution. The `wpgpGetCountryDataset` function will then download a raster dataset based on ISO3 code and covariate name.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlbr_pop_url <- wpgpGetCountryDataset(ISO3 = \"LBR\", covariate = \"ppp_2014\") \n```\n:::\n\n\n\n\n\n\nThis will download a raster file locally in a temporary directory. The path to the downloaded file is contained in the `lbr_pop_url` variable and when we introduce the `terra` package in the next sections we will show how to upoload the population raster into R. It is also possible to specify the directory where we want the raster to be downloaded using the `destDir` argument.\n\n## Importing and standardizing spatial data\n\nIn geostatistical analysis, importing and standardizing spatial data is a critical step to ensure that data from different sources align and can be used effectively. Spatial data, whether it's vector data (points, lines, polygons) or raster data (grids), can come in various formats and may use different **Coordinate Reference Systems (CRS)**. To perform accurate spatial analyses, it's essential to import data correctly and ensure consistency in terms of projection and format. This section will cover how to import vector and raster data into R, explain the concept of CRS and sensure that different datasets align properly for subsequent geostatistical analysis.\n\n### Importing vector data\n\nVector data are typically stored in formats such as **shapefiles** (`.shp`) or **geopackages** (`.gpkg`). The `sf` (simple features) package in R is the most common tool for handling vector data.\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the sf package\nlibrary(sf)\n\n# Import a shapefile (e.g., administrative boundaries)\nadmin <- st_read(\"path_to_your_shapefile/Admin_Boundaries.shp\")\n\n# Inspect the data\nprint(admin)\nplot(admin)  # Basic plot of the shapefile\n```\n:::\n\n\n\n\n\n\nThe `st_read()` function reads various spatial data formats, automatically recognizing file types.\n\n### Importing raster data\n\nRaster data consists of a grid of cells, where each cell holds a value representing a spatial attribute such as elevation or temperature. The `terra` package in R is designed to work with raster data and has superseded the older `raster` package due to better performance and greater functionality. Let's see now how to import a GeoTIFF file using `terra`. We can upload the population raster for Liberia that we have downloaded in @sec-pop-data.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the terra package\nlibrary(terra)\n\n# Import a raster file \nlbr_pop_100 <- rast(lbr_pop_url)\n\n# Inspect the raster data\nprint(lbr_pop_100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nclass       : SpatRaster \ndimensions  : 5040, 4945, 1  (nrow, ncol, nlyr)\nresolution  : 0.0008333333, 0.0008333333  (x, y)\nextent      : -11.48625, -7.365417, 4.352084, 8.552084  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : lbr_ppp_2014.tif \nname        : lbr_ppp_2014 \nmin value   :  0.006426936 \nmax value   : 92.716874581 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Basic plot of the raster\nplot(lbr_pop_100)\n```\n\n::: {.cell-output-display}\n![Liberia population count for 2014 at 100m resolution](02_handling-spatial-data_files/figure-pdf/fig-lbr-pop-1.pdf){#fig-lbr-pop fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nThe output of `print()` provides a detailed summary of the raster's properties. The `rast()` function reads raster files in formats like GeoTIFF, ASCII grid, or other common raster formats and then we used `plot()` as a quick way to visualize raster data.\n\n### Understanding coordinate reference systems (CRS)\n\nWhen working with spatial data, especially from different sources, one of the most critical tasks is ensuring that all datasets share the same **Coordinate Reference System (CRS)**. A CRS defines how the two-dimensional map data corresponds to locations on the three-dimensional Earth. If different layers (e.g., raster and vector data) have different CRSs, they may not align correctly when plotted or analyzed together, leading to inaccurate analyses or visualizations.\n\nCRSs can either be:\n\n-   **Geographic**: These use latitude and longitude coordinates to represent locations on the Earth's surface. The most common example is **WGS84** (EPSG:4326), the default CRS used by GPS and global datasets.\n\n-   **Projected**: These convert the Earth's curved surface to a flat map and preserve certain properties like area, distance, or direction. Examples include **Universal Transverse Mercator (UTM)** or **Albers Equal Area** projections.\n\nIn many cases, using a projected rather than a geographic CRS s preferred, especially when any summary statistic or parameter is distance-related. In a geographic CRS, distances between two points are calculated using angular coordinates (degrees), which do not translate easily into linear units like meters or kilometers. This makes interpreting distances challenging, as the length of a degree of latitude differs from the length of a degree of longitude. In contrast, a projected CRS uses a linear coordinate system (usually meters), ensuring that distances are accurately represented on a flat surface. This is important when computing spatial variograms, covariance functions, or some parameters in geostatistical models, where distance between sampling locations is a key factor. In this context, we assume that the default distance metric is Euclidean distance. However, an important exception to our recommendation arises when analyzing data across large, global-scale regions. In such cases, it is more appropriate to use a geographic CRS along with spherical distances, as these better reflect the curved nature of the Earth's surface.\n\n### EPSG codes\n\nAn **EPSG code** is a unique identifier that defines a CRS. These codes, managed by the **European Petroleum Survey Group (EPSG)**, are widely used in geographic information systems (GIS) to simplify the use of specific projections, ensuring that spatial data is correctly aligned and interpreted. Each EPSG code corresponds to a unique CRS or map projection, making it easier to standardize and manage spatial data from different sources.\n\nSome key and often used EPSG codes are:\n\n-   **EPSG:4326**: This code represents **WGS84**, the most commonly used geographic CRS, which uses latitude and longitude to describe locations on the Earth's surface. It is the default CRS for global datasets and GPS systems.\n\n-   **EPSG:326XX**: These codes represent the **UTM (Universal Transverse Mercator)** projection, which divides the world into zones. Each zone is optimized to preserve local distances and areas. For example (e.g. EPSG:32629: UTM Zone 29N, covering parts of Western Africa, including Liberia)\n\n-   **EPSG:3857**: This code is for the **Web Mercator projection**, which is widely used for web mapping services, including **Google Maps**, **OpenStreetMap**, and **Bing Maps**. This projected CRS uses meters as the unit of distance and is optimized for visualizing maps on a 2D plane, though it distorts area and distance, especially at high latitudes. It is well-suited for interactive online mapping but not ideal for precise distance-based geostatistical analyses.\n\n### Convert a data frame to an `sf` object\n\nIn geospatial analysis, data is often provided in tabular formats like CSV files that contain spatial coordinates (e.g., latitude and longitude). To use these data effectively in R, it is necessary to convert the data frame into an `sf` object, which is the standard format for working with spatial data in R. Here we show how to achieve this, we can use the Liberia data available in the `RiskMap` package as it is a data frame.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the RiskMap package\nlibrary(RiskMap)\n\n# Load the Liberia data set\ndata(\"liberia\")\n\n# Convert the data frame to an sf object\nliberia_sf <- st_as_sf(liberia, \n                       coords = c(\"long\", \"lat\"), \n                       crs = 4326)\n\n# Inspect the new sf object\nliberia_sf\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 90 features and 4 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -11.40335 ymin: 4.42552 xmax: -7.4928 ymax: 8.46911\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   ntest npos elevation log_elevation                   geometry\n1     50   14  17.82621      2.880670    POINT (-10.4671 6.2878)\n2     46   10 104.85070      4.652537   POINT (-10.45111 6.5725)\n3     43   11 119.09543      4.779925  POINT (-10.02615 6.56667)\n4     50   10 144.10921      4.970571  POINT (-10.28775 6.73333)\n5     48    9  19.03508      2.946283 POINT (-10.03595 6.016667)\n6     50   13  16.99954      2.833186   POINT (-10.335 6.266666)\n7     43    9 137.75360      4.925467   POINT (-9.63333 6.13541)\n8     50   11 101.75000      4.622519   POINT (-10.06875 6.2375)\n9     47   11 147.12867      4.991308    POINT (-9.73333 6.3869)\n10    43    0  26.61054      3.281308     POINT (-9.7856 5.7354)\n```\n\n\n:::\n\n```{.r .cell-code}\nplot(liberia_sf)\n```\n\n::: {.cell-output-display}\n![](02_handling-spatial-data_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nThe `st_as_sf()` function converts the data frame into an `sf` object. The `coords` argument specifies which columns contain the spatial coordinates and the `crs` argument assigns the CRS that in this case we know being WGS84 (EPSG:4326). The `sf` object can now be used for operations such as spatial joins, distance calculations, and mapping with other spatial layers. Note that the columns containing the spatial coordinates have been replaced by a `geometry` column, which now stores this information. If you would like to retain the original coordinate columns in the output, you can set the `remove` argument to `FALSE` when converting the data frame to an `sf` object.\n\n### Working with CRSs in R\n\nWhen working with data from multiple sources, such as environmental layers, population data, or administrative boundaries, ensuring that all datasets share the same CRS is essential for accurate spatial analysis. This section covers the core tasks involved in managing CRSs in R: checking the CRS of spatial data to ensure datasets are compatible and Reprojecting spatial data into a common CRS when necessary.\n\n#### Checking the CRS of Spatial Data\n\nBefore performing any spatial operation, it's crucial to check the CRS of your spatial datasets. Knowing whether your data uses geographic coordinates (e.g., WGS84) or a projected coordinate system (e.g., UTM) helps ensure that they are aligned and ready for analysis. Both `sf` and `terra` provide functions to retrieve and inspect the CRS, ensuring datasets are spatially aligned before analysis. The `st_crs()` function retrieves the CRS information for vector data.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check the CRS of the vector data\nst_crs(liberia_sf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThe same can be achieved for raster data with the `crs()` function from the `terra` package.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check the CRS of the raster data\ncrs(lbr_pop_100, proj = TRUE, describe = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    name authority code area         extent                                proj\n1 WGS 84      EPSG 4326 <NA> NA, NA, NA, NA +proj=longlat +datum=WGS84 +no_defs\n```\n\n\n:::\n:::\n\n\n\n\n\n\n#### Reprojecting spatial data to a common CRS\n\nIf your datasets have different CRSs or if you want to change CRS (e.g. from geographical to projected) you will need to reproject one or more datasets so they can be spatially aligned. This ensures that they can be overlaid and analyzed together. For vector data, `st_transform()` reprojects the data into a specified CRS. This example transforms the liberia point data from WGS84 into UTM. To know what's the correct UTM zone and hence EPSG code for Liberia we can use the `get_epsg_utm` function from the `RiskMap` package.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MOVE THIS INTO RISKMAP\n\nget_epsg_utm <- function(sf_object) {\n  # Ensure the input is an sf object\n  if (!inherits(sf_object, \"sf\")) {\n    stop(\"Input must be an sf object.\")\n  }\n  \n  # Check if the CRS is WGS 84 (EPSG: 4326)\n  crs <- st_crs(sf_object)\n  if (is.null(crs) || crs$epsg != 4326) {\n    stop(\"Input sf object must have a CRS of WGS84 (EPSG: 4326).\")\n  }\n  \n  # Get the coordinates of the centroid of the sf object\n  centroid <- st_centroid(st_union(sf_object)) \n  coords <- st_coordinates(centroid)  \n  \n  # Function to calculate UTM zone based on longitude\n  get_utm_zone <- function(lon) {\n    return((floor((lon + 180) / 6) %% 60) + 1)\n  }\n  \n  # Calculate UTM zone from longitude\n  utm_zone <- get_utm_zone(coords[1])\n  \n  # Determine the EPSG code based on the latitude (north/south)\n  epsg_code <- if (coords[2] >= 0) {\n    32600 + utm_zone   # Northern Hemisphere\n  } else {\n    32700 + utm_zone   # Southern Hemisphere\n  }\n  \n  # Return the calculated EPSG code\n  return(epsg_code)\n}\n\n# Obtain EPSG code for UTM for Liberia\nget_epsg_utm(liberia_sf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 32629\n```\n\n\n:::\n\n```{.r .cell-code}\n# Reproject the vector data \nliberia_sf_utm <- st_transform(liberia_sf, crs = get_epsg_utm(liberia_sf))\n```\n:::\n\n\n\n\n\n\nReprojecting raster data is more complex than reprojecting vector data due to the continuous nature of raster grids. The process involves recalculating cell values to fit a new grid based on the new CRS, which can lead to challenges like resampling, distortion, and data loss. When reprojecting a raster, the grid must adjust to the new CRS, often requiring resampling of cell values. The method you choose depends on the data type: **nearest neighbor** is best for categorical data like land use while **bilinear or cubic interpolation** is good for continuous data like temperature, where smooth transitions are needed.\n\nThe function `project()` from the `terra` package can be used to reproject a raster.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlbr_pop_100_utm <- project(lbr_pop_100, \n                           crs(liberia_sf_utm),\n                           method = \"bilinear\")\n```\n:::\n\n\n\n\n\n\nReprojecting a raster may alter its resolution. For example, reprojecting from geographic (degrees) to projected (meters) CRS can result in a mismatch between the original and new cell sizes. Moreover, distortion can occur when converting between projections, especially at high latitudes. Some cells may be stretched or compressed, leading to potential loss of information or edge artifacts. These distortions arise because the Earth is not flat, and projecting the curved surface of the Earth onto a flat plane (or vice versa) leads to trade-offs. For example, the Mercator projection preserves angles and shapes but distorts area, particularly near the poles.\n\nFor these reasons, it's often better to reproject vectors rather than rasters when both data types are used together and avoid as much as possible to change the CRS of raster. One way to achieve this is to work with WGS84 when performing all spatial operations like extraction of covariates from rasters and then transform only the point data to a projected CRS before fitting the model.\n\n### Common Issues and Considerations\n\n#NOT SURE ABOUT THIS, MAYBE EXPAND OR REMOVE\n\n-   **CRS Mismatch**: One of the most common issues in spatial analysis is the mismatch of CRSs between datasets. Always check and reproject datasets if necessary before performing spatial operations like overlays or extractions.\n\n-   **Datum Transformations**: In some cases, especially when working with datasets from different regions, a simple projection transformation may not be enough. You might need to handle datum shifts (e.g., from NAD83 to WGS84). R can handle these transformations, but it's important to be aware of them.\n\n-   **Choosing the Right CRS**: The choice of CRS depends on your study area and the type of analysis. For global studies, WGS84 is often used. For regional studies, **UTM** or other local projections may be more appropriate to preserve accuracy in distances and areas.\n\n## Extracting covariate data\n\nIn geostatistical models, the inclusion of relevant covariates (environmental, demographic, or climatic) can potentially enhances predictive accuracy. Covariate data often come from raster or polygon sources, and extracting these values for point locations is essential to link spatial context to point-referenced data. These covariates could include variables like temperature, elevation, land cover, or population density, which influence the spatial distribution of diseases. In this section, we will cover how to extract covariates at point locations from both polygon layers raster layers.\n\n#### Extracting covariates from polygon layers\n\nPolygon layers contain discrete spatial entities, such as administrative boundaries or land use areas, with associated attributes. Extracting covariates from polygon layers involves associating point data with the attributes of the polygon in which they fall. Here is an example with.....NEED SOME DATA FOR THIS\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform a spatial join to transfer polygon attributes to the points\npoints_with_admin <- st_join(liberia_sf, liberia_admin1[\"shapeName\"])\n\n# View the results, points now include covariates from the polygon layer\nhead(points_with_admin)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -10.4671 ymin: 6.016667 xmax: -10.02615 ymax: 6.73333\nGeodetic CRS:  WGS 84\n  ntest npos elevation log_elevation   shapeName                   geometry\n1    50   14  17.82621      2.880670     Margibi    POINT (-10.4671 6.2878)\n2    46   10 104.85070      4.652537 Montserrado   POINT (-10.45111 6.5725)\n3    43   11 119.09543      4.779925     Margibi  POINT (-10.02615 6.56667)\n4    50   10 144.10921      4.970571     Margibi  POINT (-10.28775 6.73333)\n5    48    9  19.03508      2.946283 Grand Bassa POINT (-10.03595 6.016667)\n6    50   13  16.99954      2.833186 Grand Bassa   POINT (-10.335 6.266666)\n```\n\n\n:::\n:::\n\n\n\n\n\n\nThis spatial join operation adds attributes (in this case the admin1 names) from the polygons to the points. These attributes can now be used in geostatistical models to explain spatial variation in disease risk based on the regions they fall into.\n\n### Extracting covariates from raster layers\n\nRaster data provides continuous spatial information, such as elevation, climate data, or population density. Covariate values from raster layers can be extracted for specific points using the **`extract()`** function from the `terra` package. Each point will receive the value of the raster cell it overlaps.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Extract raster values at the point locations\ncovariate_values <- extract(lbr_pop_100, liberia_sf)\n\n# Combine the extracted values with the point data\nliberia_sf$pop_total <- covariate_values[, 2]\n\n# View the updated dataset\nhead(liberia_sf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 6 features and 5 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -10.4671 ymin: 6.016667 xmax: -10.02615 ymax: 6.73333\nGeodetic CRS:  WGS 84\n  ntest npos elevation log_elevation                   geometry pop_total\n1    50   14  17.82621      2.880670    POINT (-10.4671 6.2878) 0.2914019\n2    46   10 104.85070      4.652537   POINT (-10.45111 6.5725) 0.6087928\n3    43   11 119.09543      4.779925  POINT (-10.02615 6.56667) 0.2044306\n4    50   10 144.10921      4.970571  POINT (-10.28775 6.73333) 0.5078438\n5    48    9  19.03508      2.946283 POINT (-10.03595 6.016667) 0.1767686\n6    50   13  16.99954      2.833186   POINT (-10.335 6.266666) 1.6693381\n```\n\n\n:::\n:::\n\n\n\n\n\n\nIn this example, the `extract()` function assigns the raster value from the population density layer to each point in the dataset. This allows the point data to include population density as a covariate in the analysis.\n\nInstead of extracting values for exact point locations, it can sometimes be useful to aggregate covariate values within a defined area around each point. This is often done by creating a buffer around each point and calculating summary statistics (e.g., mean, sum) of the raster values within that buffer. For instance, you might want to calculate the average population density or temperature within a 2 km radius around each point to smooth out fine-scale variation.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create buffers around each point (e.g., 2 km radius)\nbuffered_points <- st_buffer(liberia_sf_utm, dist = 1000)  \n\n# Plot the buffers for visualization\nplot(st_geometry(buffered_points), col = \"blue\", border = \"black\")\n```\n\n::: {.cell-output-display}\n![](02_handling-spatial-data_files/figure-pdf/unnamed-chunk-14-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\n# Extract raster values within the buffer areas and calculate the \n# mean or sum. Note that since we used the utm data to work on the\n# meter scale we need to convert them back to WGS84\nmean_pop_density <- extract(lbr_pop_100, \n                            st_transform(buffered_points, crs = 4326),\n                            fun = mean, na.rm = TRUE)\n\n# Add the averaged values to the points dataset\nliberia_sf$pop_mean2km <- mean_pop_density[,2]\n\n# View the updated dataset\nhead(liberia_sf)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSimple feature collection with 6 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -10.4671 ymin: 6.016667 xmax: -10.02615 ymax: 6.73333\nGeodetic CRS:  WGS 84\n  ntest npos elevation log_elevation                   geometry pop_total\n1    50   14  17.82621      2.880670    POINT (-10.4671 6.2878) 0.2914019\n2    46   10 104.85070      4.652537   POINT (-10.45111 6.5725) 0.6087928\n3    43   11 119.09543      4.779925  POINT (-10.02615 6.56667) 0.2044306\n4    50   10 144.10921      4.970571  POINT (-10.28775 6.73333) 0.5078438\n5    48    9  19.03508      2.946283 POINT (-10.03595 6.016667) 0.1767686\n6    50   13  16.99954      2.833186   POINT (-10.335 6.266666) 1.6693381\n  pop_mean2km\n1   0.3239253\n2   0.5534695\n3   0.2254926\n4   0.4708411\n5   0.2419380\n6   3.7534250\n```\n\n\n:::\n:::\n\n\n\n\n\n\nYou can modify the `fun` argument to calculate other summary statistics, such as the sum, min or max of the raster values within the buffer. This approach is particularly useful when the phenomenon being modeled (e.g., disease transmission) is influenced by broader spatial factors around the observation point, rather than just the value at the exact point location.\n\n## Creating a predictive grid\n\nA predictive grid is a regularly spaced set of points or cells that spans the study region. This grid serves as the basis for predictions made by your model. The density of the grid (i.e., the distance between grid points) affects both the resolution of the prediction and the computational cost. For point-based predictions, we can generate a grid of points over a polygon (e.g., administrative boundary) using the `sf` package and the `st_make_grid` function.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First we convert the Liberia boundaries to the UTM CRS\n# because we want our grid in meters\nliberia_admin0_utm <- liberia_admin0 |> \n  st_transform(crs = get_epsg_utm(liberia_sf))\n\n# Generate prediction grid at 5km resolution\npred_locations <- st_make_grid(liberia_admin0_utm, \n                               cellsize = 5000, \n                               what = \"centers\")\n\n# Exclude locations that fall outside the study area\npred_locations <- st_intersection(pred_locations, liberia_admin0_utm)\n\n# Visualize the result\nplot(liberia_admin0_utm$geometry, col = \"white\")\nplot(pred_locations, cex = .01, col = \"red\", pch = 19, add = T)\n```\n\n::: {.cell-output-display}\n![](02_handling-spatial-data_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\n## Visualizing spatial data\n\nVisualization is a key part of spatial data analysis, as it allows you to explore and communicate spatial patterns and relationships effectively. R provides a lot of functionalities to visualize spatial data and create very beautiful maps. Until now we have used basic plotting functions. Here we introduce the `ggplot2` package that allows to combine different types of geographic data in a map. The `ggplot2` package in R provides a flexible and powerful framework for creating both simple and complex visualizations, including maps of point data, polygons, and rasters. With the help of extensions like `geom_sf()` and `geom_raster()`, `ggplot2` makes it easy to visualize spatial data, whether you're working with point locations, polygons, or continuous raster data.\n\n### Visualizing point data\n\nPoint data often represents the locations of observations (e.g., disease cases, sampling sites). `ggplot2` allows you to plot these points and optionally color them by a covariate (e.g., disease prevalence or population density). Here is an example that uses the Liberia data.\n\n\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(ggplot2)\n\n# Create a new variable with prevalence in the dataset\nliberia_sf$prevalence <- liberia$npos / liberia$ntest\n\n# Plot only the locations\nggplot(data = liberia_sf) +\n  geom_sf(col = \"black\") +  \n  theme_minimal() +\n  labs(title = \"Survey locations\")\n```\n\n::: {.cell-output-display}\n![](02_handling-spatial-data_files/figure-pdf/unnamed-chunk-16-1.pdf)\n:::\n\n```{.r .cell-code}\n# Color the points according to prevalence\nggplot(data = liberia_sf) +\n  geom_sf(aes(color = prevalence)) +  \n  scale_color_viridis_c(labels = scales::label_percent()) +  \n  theme_minimal() +\n  labs(title = \"Onchocerciasis in Liberia\",\n       color = \"Prevalence (%)\")\n```\n\n::: {.cell-output-display}\n![](02_handling-spatial-data_files/figure-pdf/unnamed-chunk-16-2.pdf)\n:::\n:::\n\n\n\n\n\n\nHere `geom_sf()` is used to plot the spatial points. `aes(color = prevalence)` specifies that the points should be colored based on the `prevalence` covariate, providing a visual representation of spatial variation in disease risk. The `scale_color_viridis_c()` function applies the Viridis color scale, which is well-suited for continuous data and is friendly for those with color blindness. The `labels = scales::label_percent()` argument ensures that the color scale's labels are displayed as percentages (e.g., 5, 10%) rather than raw decimal values. To make the plot visually clean and minimal, `theme_minimal()` is applied, stripping away unnecessary background elements and keeping the focus on the data. Finally, the `labs()` defines the plot title and the color legend label.\n\n### Visualizing polygon data\n\nPolygon data typically represents administrative boundaries, land use, or other regional divisions. We can still use `geom_sf()` to create maps of polygons, optionally filling them by a covariate.\n\n\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\n# Plot Liberia admin 1 level boundaries \nggplot(data = liberia_admin1) +\n  geom_sf() + \n  theme_minimal() +\n  labs()\n```\n\n::: {.cell-output-display}\n![](02_handling-spatial-data_files/figure-pdf/unnamed-chunk-17-1.pdf)\n:::\n\n```{.r .cell-code}\n# We compute the area of each polygon\nliberia_admin1$area <- as.numeric(st_area(liberia_admin1) / 1000 ^ 2)\n\n# Color the polygons according tis new variable\nggplot(data = liberia_admin1) +\n  geom_sf(aes(fill = area), color = \"black\") + \n  scale_fill_distiller(direction = -1) +\n  theme_minimal() +\n  labs(fill = \"Area km^2\")\n```\n\n::: {.cell-output-display}\n![](02_handling-spatial-data_files/figure-pdf/unnamed-chunk-17-2.pdf)\n:::\n:::\n\n\n\n\n\n\nIn this code, `aes(fill = area)` is used to fill each polygon with colors corresponding to its area. The `color = \"black\"` argument outlines the polygons in black, and you could set `fill = NA` to make the polygons transparent while still displaying the borders. The `scale_fill_distiller(direction = -1)` function applies a color gradient from ColorBrewer, with the `direction = -1` argument reversing the gradient (e.g., darker colors for larger areas).\n\n### Visualizing raster data\n\nIn `ggplot2`, you can visualize raster data by converting it into a data frame of coordinates and values. You can convert raster data into a format that `ggplot2` can handle by using the `as.data.frame()` function from `terra`.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert the raster to a data frame for ggplot2\nraster_df <- as.data.frame(lbr_pop_100, xy = TRUE)\n\n# Plot raster using ggplot2\nggplot(data = raster_df) +\n  geom_raster(aes(x = x, y = y, fill = lbr_ppp_2014)) + \n  scale_fill_viridis_c() +\n  coord_cartesian() +\n  theme_minimal() +\n  labs(title = \"Population Density\",\n       fill = \"Density\")\n```\n\n::: {.cell-output-display}\n![](02_handling-spatial-data_files/figure-pdf/unnamed-chunk-18-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nIn this example `as.data.frame()` converts the raster into a data frame with x and y coordinates and their corresponding raster values and `geom_raster()` is used to plot the raster cells, coloring them based on the population density.\n\n### Combining Multiple Spatial Data Types\n\nIn many cases, it's useful to combine different spatial data types (points, polygons, and rasters) in a single visualization. `ggplot2` allows you to overlay these layers, providing a more comprehensive view of your spatial data.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Combine points, polygons, and raster data in one plot\ncombined_map <- ggplot() +\n  geom_raster(data = raster_df, aes(x = x, y = y, fill = lbr_ppp_2014)) + \n  geom_sf(data = liberia_admin1, fill = NA, color = \"grey\") +    \n  geom_sf(data = liberia_sf, shape = 21, col = \"black\", fill = \"white\") +\n  scale_fill_viridis_c() +  \n  theme_minimal() +\n  labs(title = \"Combined Spatial Data: Points, Polygons, and Raster\",\n       fill = \"Population Density\",\n       x = \"\", y = \"\")\n\ncombined_map\n```\n\n::: {.cell-output-display}\n![](02_handling-spatial-data_files/figure-pdf/unnamed-chunk-19-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nTo enhance spatial visualizations in `ggplot2`, adding a scale bar and a north arrow improves map readability and professionalism. The `ggspatial` package offers tools to easily integrate these elements into your maps. Below is an example that demonstrates how to use `ggspatial` to add a scale bar and north arrow to a map that includes raster, polygon, and point data.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary libraries\nlibrary(ggspatial)\n\n# Add scale bar and north arrow\ncombined_map +\n  annotation_scale(location = \"bl\", width_hint = 0.5) +  \n  annotation_north_arrow(location = \"tr\", which_north = \"true\") \n```\n\n::: {.cell-output-display}\n![](02_handling-spatial-data_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n:::\n",
    "supporting": [
      "02_handling-spatial-data_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}